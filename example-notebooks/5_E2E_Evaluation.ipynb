{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddca0e9-8b4b-4ac5-b7fc-ea0998a835c3",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook builds of all the previous notebooks. We validated our chunking strategy and embeddings, rerank, LLM-As-A-Judge-Prompt and finally our prompt itself. The last step is to put it all together to validate the entire system. \n",
    "\n",
    "Remember back to our first two notebooks, we don't expect to get the top result all the time. However we want to see how our overall system is performing on our validation dataset. To do this we'll build a local RAG system based. \n",
    "\n",
    "#### What Metrics Should I Care About.\n",
    "For an E2E system, we care about a couple metrics. These metrics have been given a new name with the introduction of RAG but they build off a lot of existing metrics used in the data science space. \n",
    "\n",
    "\n",
    "# What Will We Do?\n",
    "* Curate a dataset of questions and ground truth answers (we've created one already)\n",
    "* Modify our grading rubric to include the ground truth answer.\n",
    "* Setup a retrieval task\n",
    "* Inject the context from our retrieval task into our LLM and validate the results with our updated rubric.\n",
    "* Compare our custom setup to RAGAS (an open source evaluation tool) to see how it compares.\n",
    "\n",
    "At this end of this notebook, we should have a working example that demonstrates E2E how well our retrieval system is working\n",
    "\n",
    "**Note** For E2E evaluation, RAGAS is a great tool. It however doesn't work very well in a production system where we don't know the answers ahead of time. Because of this, the LLM-As-A-Judge prompt we used previously is better suited to validate your production system. It's not uncommon to either use this grading rubric as a safeguard before returning answers to users, or running it asyncronously on a subset of responses to gather metrics for more advanced observability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9a112-1ca1-4835-80d7-8a544f9c57dd",
   "metadata": {},
   "source": [
    "# Import Validation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b554e-09bf-4f19-92ad-cafb78c17aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('../data/eval-datasets/5_e2e_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a0ecc-9b0d-457c-b027-bcf48c6179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import boto3\n",
    "\n",
    "# Initialize Chroma client from persistant disk. We'll use the same collection from our first notebook\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma\")\n",
    "\n",
    "# Also initialize the bedrock client so we can call some embedding models!\n",
    "session = boto3.Session(profile_name='default')\n",
    "bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fc6c8-e728-4159-bf92-3c778197f1ae",
   "metadata": {},
   "source": [
    "# Setup Embeddings / Chunk Retrieval Task\n",
    "Included in this repo is a data/chroma section that contains a prebuilt chroma collection using the chunking strategy and embedding model we selected as the best in our first notebook. Instead of recreating that, we can modify the ChromaHelper from the first notebook and load it from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942b5f0-9fe4-4108-bad4-039f78c0468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    id: str\n",
    "    document: str\n",
    "    embedding: List[float]\n",
    "    distance: float\n",
    "    metadata: Dict = {}\n",
    "\n",
    "class ChromaDBRetrievalTask:\n",
    "    def __init__(self, chroma_client, collection_name: str, embedding_function):\n",
    "        self.client = chroma_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        # Get the existing collection\n",
    "        self.collection = self._get_collection()\n",
    "\n",
    "    def _get_collection(self):\n",
    "        return self.client.get_collection(\n",
    "            name=self.collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query_text: str, n_results: int = 5) -> List[RetrievalResult]:\n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=['embeddings', 'documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        # Transform the results into RetrievalResult objects\n",
    "        retrieval_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                id=results['ids'][0][i],\n",
    "                document=results['documents'][0][i],\n",
    "                embedding=results['embeddings'][0][i],\n",
    "                distance=results['distances'][0][i],\n",
    "                metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "            ))\n",
    "        return retrieval_results\n",
    "\n",
    "# Define some experiment variables\n",
    "EMBEDDING_MODEL: str = \"amazon.titan-embed-text-v2:0\"\n",
    "COLLECTION_NAME: str = 'experiment_3_collection'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "# Create our retrieval task for Chroma.\n",
    "chroma_retrieval_task: ChromaDBRetrievalTask = ChromaDBRetrievalTask(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = COLLECTION_NAME,\n",
    "    embedding_function = embedding_function\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58aa6e-dca5-4ea3-99a6-26db11d05e87",
   "metadata": {},
   "source": [
    "# Setup ReRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378edd7-a419-454f-b64a-0a3f085285fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder as SentenceTransformerCrossEncoder\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Passage(BaseModel):\n",
    "    chunk: str\n",
    "    file_name: str\n",
    "    score: float = 0.0\n",
    "\n",
    "class CrossEncoderReRankTask:\n",
    "    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-12-v2', score_threshold: float = -0.999, max_length: int = 512):\n",
    "        self.cross_encoder = SentenceTransformerCrossEncoder(model_name)\n",
    "        self.score_threshold = score_threshold\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def chunk_text(self, text: str, max_length: int) -> List[str]:\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for word in words:\n",
    "            if current_length + len(word) + 1 > max_length:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "                current_length = len(word)\n",
    "            else:\n",
    "                current_chunk.append(word)\n",
    "                current_length += len(word) + 1\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def rerank(self, query: str, passages: List[Passage]) -> List[Passage]:\n",
    "        all_input_pairs = []\n",
    "        chunk_map = {}\n",
    "\n",
    "        for i, passage in enumerate(passages):\n",
    "            chunks = self.chunk_text(passage.chunk, self.max_length)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                all_input_pairs.append([query, chunk])\n",
    "                chunk_map[(i, j)] = chunk\n",
    "\n",
    "        # Get scores from the cross-encoder\n",
    "        scores = self.cross_encoder.predict(all_input_pairs)\n",
    "\n",
    "        # Aggregate scores for each original passage\n",
    "        passage_scores = {}\n",
    "        for (i, j), score in zip(chunk_map.keys(), scores):\n",
    "            if i not in passage_scores:\n",
    "                passage_scores[i] = []\n",
    "            passage_scores[i].append(score)\n",
    "\n",
    "        # Calculate final score for each passage (e.g., using max score)\n",
    "        final_scores = {i: max(scores) for i, scores in passage_scores.items()}\n",
    "\n",
    "        # Sort passages based on their scores in descending order\n",
    "        sorted_passages = sorted([(score, passages[i]) for i, score in final_scores.items()], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Update passage scores and return\n",
    "        result = []\n",
    "        for score, passage in sorted_passages:\n",
    "            passage.score = float(score)\n",
    "            result.append(passage)\n",
    "\n",
    "        # Lets only return the top 2.\n",
    "        return result[:2]\n",
    "\n",
    "# Define the ReRank task. By default we use ms-mini-marco from the the HuggingFace Sentence Transformer Library\n",
    "reranker: CrossEncoderReRankTask =  CrossEncoderReRankTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad33c39-a733-4ffc-b4b7-70baed300f22",
   "metadata": {},
   "source": [
    "# Setup Full Retrieval Task\n",
    "In this step we'll create a class that combines our chromaDB collection & ReRank to return the most relevant passages we can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5b267-8f99-4737-b0af-4e16320c47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalTask:\n",
    "    def __init__(self, chroma_retriever: ChromaDBRetrievalTask, reranker: CrossEncoderReRankTask):\n",
    "        self.chroma_retriever = chroma_retriever\n",
    "        self.reranker = reranker\n",
    "\n",
    "    # Retrieve our results, Rerank, and return the top 2 results\n",
    "    def retrieve(self, query, n_results=5) -> List[Passage]:\n",
    "        initial_results: RetrievalResult  = self.chroma_retriever.retrieve(query, n_results)\n",
    "        passages = [Passage(chunk=r.document, file_name = r.metadata['relative_path']) for r in initial_results]\n",
    "        return self.reranker.rerank(query, passages)\n",
    "\n",
    "retrieval_task: RetrievalTask = RetrievalTask(\n",
    "    chroma_retriever=chroma_retrieval_task,\n",
    "    reranker=reranker\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650f4a1-2ddf-4976-8167-adcd8c9cd5ee",
   "metadata": {},
   "source": [
    "# Setup RAG With Bedrock\n",
    "This is mostly reused from the previous task with one exception. In the RAGClient, we're making a Retrieval call to populate the context. We store it in context and context_chunks. Context is useful for the LLM-As-A-Judge Evaluation while Context Chunks is a specific requirement for a tool we'll use to compare our custom evaluator to (RAGAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b369b2-48ec-4f30-a305-6ba658c4e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "class BaseBedrockClient:\n",
    "    def __init__(self, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        self.client = boto3.client('bedrock-runtime')\n",
    "        self.user_prompt = user_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model_id = model_id\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "    def create_chat_payload(self, inputs: dict) -> list[dict]:\n",
    "        prompt = self.user_prompt.format(**inputs)\n",
    "        return [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    def call(self, messages: list[dict]) -> str:\n",
    "        response = self.client.converse(\n",
    "            modelId=self.model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=self.hyper_params,\n",
    "            system=[{\"text\": self.system_prompt}]\n",
    "        )\n",
    "        return response['output']['message']['content'][0]['text']\n",
    "\n",
    "    def call_threaded(self, message_lists: List[List[Dict[str, Any]]]) -> List[str]:\n",
    "        future_to_position = {}\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for i, request in enumerate(message_lists):\n",
    "                future = executor.submit(self.call, request)\n",
    "                future_to_position[future] = i\n",
    "            \n",
    "            responses = [None] * len(message_lists)\n",
    "            for future in as_completed(future_to_position):\n",
    "                position = future_to_position[future]\n",
    "                try:\n",
    "                    response: str = future.result()\n",
    "                    responses[position] = response\n",
    "                except Exception as exc:\n",
    "                    print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                    responses[position] = None\n",
    "        return responses\n",
    "\n",
    "class RAGClient(BaseBedrockClient):\n",
    "    def __init__(self, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict, retrieval_task: RetrievalTask):\n",
    "        super().__init__(user_prompt, system_prompt, model_id, hyper_params)\n",
    "        self.retrieval_task = retrieval_task\n",
    "\n",
    "    def extract_response(self, llm_output: str) -> str:\n",
    "        response_match = re.search(r'<response>(.*?)</response>', llm_output, re.DOTALL)\n",
    "        return response_match.group(1).strip() if response_match else \"No response found\"\n",
    "\n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "\n",
    "        message_lists = []\n",
    "        contexts = []  # Store context as it's passed into the prompt\n",
    "        context_lists = [] # Store context for RAGAS evaluation\n",
    "        for _, row in df.iterrows():\n",
    "            # Get passages for context\n",
    "            passages: List[Passages] = self.retrieval_task.retrieve(row[\"query_text\"])\n",
    "            # Combine into single context\n",
    "            context = \"\\n\\n\".join(f\"###File name:\\n{p.file_name}\\n###Passage:\\n{p.chunk}\" for p in passages)\n",
    "\n",
    "            # Store contexts for downstream dependencies\n",
    "            contexts.append(context)\n",
    "            context_lists.append(json.dumps([p.chunk for p in passages]))\n",
    "            \n",
    "            # Construct message list using the query text and relevant passages retrieved.\n",
    "            message_lists.append(self.create_chat_payload({\n",
    "                \"query_text\": row[\"query_text\"],\n",
    "                \"context\": context\n",
    "            }))\n",
    "        \n",
    "        responses = self.call_threaded(message_lists)\n",
    "\n",
    "        df['context'] = contexts\n",
    "        df['context_chunks'] = context_lists\n",
    "        df['llm_response'] = [self.extract_response(r) for r in responses]\n",
    "        return df\n",
    "\n",
    "class EvaluationClient(BaseBedrockClient):\n",
    "    def __init__(self, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        super().__init__(user_prompt, system_prompt, model_id, hyper_params)\n",
    "\n",
    "    def extract_score_and_thinking(self, llm_output: str) -> tuple:\n",
    "        thinking_match = re.search(r'<thinking>(.*?)</thinking>', llm_output, re.DOTALL)\n",
    "        score_match = re.search(r'<score>(.*?)</score>', llm_output, re.DOTALL)\n",
    "\n",
    "        thinking = thinking_match.group(1).strip() if thinking_match else \"No thinking found\"\n",
    "        score = float(score_match.group(1)) if score_match else None\n",
    "        \n",
    "        return score, thinking\n",
    "\n",
    "    def evaluate(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        message_lists = [self.create_chat_payload({\n",
    "            \"query_text\": row[\"query_text\"],\n",
    "            \"context\": row[\"context\"],\n",
    "            \"llm_response\": row[\"llm_response\"],\n",
    "            \"ground_truth\": row[\"ground_truth\"]\n",
    "        }) for _, row in df.iterrows()]\n",
    "        \n",
    "        responses = self.call_threaded(message_lists)\n",
    "\n",
    "        llm_scores = []\n",
    "        llm_thinking = []\n",
    "\n",
    "        for response in responses:\n",
    "            if response is not None:\n",
    "                score, thinking = self.extract_score_and_thinking(response)\n",
    "                llm_scores.append(score)\n",
    "                llm_thinking.append(thinking)\n",
    "            else:\n",
    "                llm_scores.append(None)\n",
    "                llm_thinking.append(\"Error occurred during processing\")\n",
    "\n",
    "        df['grade'] = llm_scores\n",
    "        df['reasoning'] = llm_thinking\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccfc7a-f13f-4feb-b4f6-58b777952eda",
   "metadata": {},
   "source": [
    "# Define RAG Prompts\n",
    "This is the same prompt we used in the previous task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ff584-ca63-4684-99bd-4b372a9be0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an advanced AI assistant specialized in Retrieval Augmented Generation (RAG).\n",
    "Your primary function is to provide accurate, concise, and relevant answers based solely on the given context.\n",
    "Follow these guidelines strictly:\n",
    "\n",
    "1. Use only information from the provided context. Do not introduce external knowledge or make assumptions.\n",
    "2. Ensure your answers are complete, addressing all aspects of the question using available information.\n",
    "3. Be extremely concise. Use as few words as possible while maintaining clarity and completeness.\n",
    "4. Maintain 100% accuracy based on the given context. If the context doesn't contain enough information to answer fully, state this clearly.\n",
    "5. Structure your responses for maximum clarity. Use bullet points or numbered lists when appropriate.\n",
    "6. If the context contains technical information, explain it in simple terms as if speaking to a non-technical person.\n",
    "7. Do not apologize or use phrases like \"Based on the context provided\" or \"According to the information given\".\n",
    "8. If asked about something not in the context, simply state \"The provided context does not contain information about [topic].\"\n",
    "\n",
    "Your goal is to achieve the highest possible score on context utilization, completeness, conciseness, accuracy, and clarity.\"\"\"\n",
    "\n",
    "# User Prompt\n",
    "RAG_USER_PROMPT = \"\"\"Answer the following question using only the provided context:\n",
    "\n",
    "<query>\n",
    "{query_text}\n",
    "</query>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Instructions:\n",
    "1. Read the question and context carefully.\n",
    "2. Formulate a concise and accurate answer based solely on the given context.\n",
    "3. Ensure your response is clear and easily understandable to a non-technical person.\n",
    "4. Do not include any information not present in the context.\n",
    "5. If the context doesn't contain relevant information, state this clearly and concisely.\n",
    "6. Place your response in <response></response> tags.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c229ad-0d6c-42b7-9bab-5d2cc40d4cde",
   "metadata": {},
   "source": [
    "# Reuse Rubric \n",
    "We'll reuse our Rubric from the previous task but we'll add the \"ground truth\" section to the results and slightly tweak our evaluation criteria to account for context relevancy. Because we want the total score to be 5 (which is arbitrary), we'll remove one of the previous evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a4787-cf6b-4c86-8a5f-ec325b4d9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "RUBRIC_SYSTEM_PROMPT = \"\"\"You are an expert judge evaluating Retrieval Augmented Generation (RAG) applications.\n",
    "Your task is to evaluate given answers based on context and questions using the criteria provided.\n",
    "Evaluation Criteria (Score either 0 or 1 for each, total score is the sum):\n",
    "1. Context Utilization: Does the answer use only information provided in the context, without introducing external or fabricated details?\n",
    "2. Completeness: Does the answer thoroughly address all key elements of the question based on the available context, without significant omissions?\n",
    "3. Conciseness: Does the answer efficiently use words to address the question and avoid unnecessary redundancy?\n",
    "4. Context Relevancy: Is the context returned sufficient to provide an answer like the gold standard answer.\n",
    "5. Clarity: Is the answer easy to understand and follow?\n",
    "Your role is to provide a fair and thorough evaluation for each criterion, explaining your reasoning clearly.\"\"\"\n",
    "\n",
    "# User Prompt\n",
    "RUBRIC_USER_PROMPT = \"\"\"Please evaluate the following RAG response:\n",
    "\n",
    "Question:\n",
    "<query_text>\n",
    "{query_text}\n",
    "</query_text>\n",
    "\n",
    "Ground Truth Answer\n",
    "<llm_response>\n",
    "{ground_truth}\n",
    "</llm_response>\n",
    "\n",
    "\n",
    "Generated answer:\n",
    "<llm_response>\n",
    "{llm_response}\n",
    "</llm_response>\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Evaluation Steps:\n",
    "1. Carefully read the provided context, question, and answer.\n",
    "2. For each evaluation criterion, assign a score of either 0 or 1:\n",
    "   - Context Utilization\n",
    "   - Completeness\n",
    "   - Conciseness\n",
    "   - Accuracy\n",
    "   - Clarity\n",
    "3. Provide a clear explanation for each score, referencing specific aspects of the response.\n",
    "4. Calculate the total score by adding up the points awarded (minimum 0, maximum 5).\n",
    "5. Present your evaluation inside <thinking></thinking> tags.\n",
    "6. Include individual criterion scores (0 or 1) in the thinking tags and the total score inside <score></score> tags.\n",
    "7. Ensure your response is valid XML and provides a comprehensive evaluation.\n",
    "8. Use the ground truth to evaluate whether the information returned was not relevant to answer the question fully. If not, \n",
    "\n",
    "Example Output Format:\n",
    "<thinking>\n",
    "Context Utilization: 1 - The answer strictly uses information from the context without introducing external details.\n",
    "Completeness: 1 - The response covers all key elements of the question based on the available context.\n",
    "Conciseness: 1 - The answer is helpful and doesn't repeat the same information more than once.\n",
    "Context Relevancy: 0 - The context was not relevant to the question.\n",
    "Clarity: 1 - The response is clear and easy to follow.\n",
    "</thinking>\n",
    "<score>4</score>\n",
    "\n",
    "Please provide your detailed evaluation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c7c93-7018-475a-bf19-fcab82075c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different models we can use. \n",
    "SONNET_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "HAIKU_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Initialize RAG Client\n",
    "rag_client: RAGClient = RAGClient(\n",
    "    RAG_USER_PROMPT, \n",
    "    RAG_SYSTEM_PROMPT, \n",
    "    HAIKU_ID,\n",
    "    {\"temperature\": 0.5, \"maxTokens\": 4096},\n",
    "    retrieval_task\n",
    ")\n",
    "\n",
    "# Initialize Eval Client\n",
    "eval_client = EvaluationClient(\n",
    "    RUBRIC_USER_PROMPT, \n",
    "    RUBRIC_SYSTEM_PROMPT, \n",
    "    HAIKU_ID, \n",
    "    {\"temperature\": 0.7, \"maxTokens\": 4096}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9579e4-30d0-4d64-82c9-1fa8ea7668c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RAG responses\n",
    "rag_df = rag_client.process(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b585697-8532-492b-975b-68b445d82ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RAG Responses\n",
    "llm_as_a_judge_results_df = eval_client.evaluate(rag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58060f-512f-4059-ba08-dfd88c5de85e",
   "metadata": {},
   "source": [
    "# Create Summary View of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed29036-f31a-4428-a6c6-942fd7597550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textwrap import fill\n",
    "\n",
    "class E2EEvaluator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.grades = df['grade'].astype(float)\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        return {\n",
    "            'Mean': np.mean(self.grades),\n",
    "            'Median': np.median(self.grades),\n",
    "            'Standard Deviation': np.std(self.grades),\n",
    "            'Minimum Grade': np.min(self.grades),\n",
    "            'Maximum Grade': np.max(self.grades)\n",
    "        }\n",
    "    \n",
    "    def generate_report(self):\n",
    "        metrics = self.calculate_metrics()\n",
    "        report = \"E2E Validation Result\\n\"\n",
    "        report += \"========================\\n\\n\"\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            report += f\"{metric}: {value:.2f}\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def analyze_grade_distribution(self):\n",
    "        return self.df['grade'].value_counts().sort_index()\n",
    "\n",
    "    def pretty_print_lowest_results(self, n=3, width=80):\n",
    "        lowest_results = self.df.nsmallest(n, 'grade')\n",
    "        for index, row in lowest_results.iterrows():\n",
    "            print(f\"{'='*width}\\n\")\n",
    "            print(f\"Grade: {row['grade']}\\n\")\n",
    "            print(\"Query Text:\")\n",
    "            print(fill(row['query_text'], width=width))\n",
    "            print(\"\\nLLM Response:\")\n",
    "            print(fill(row['llm_response'], width=width))\n",
    "            print(\"\\nReasoning:\")\n",
    "            print(fill(row['reasoning'], width=width))\n",
    "            print(f\"\\n{'='*width}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5267560-6420-47b3-9bd2-297acdfae695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named 'df'\n",
    "evaluator = E2EEvaluator(llm_as_a_judge_results_df)\n",
    "\n",
    "# Generate and print the report\n",
    "print(evaluator.generate_report())\n",
    "\n",
    "# Analyze grade distribution\n",
    "print(evaluator.analyze_grade_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84643a95-9470-4aee-8647-0204e2741046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the results and spot check them\n",
    "llm_as_a_judge_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a274ca-ff96-491a-91a9-529f288bb726",
   "metadata": {},
   "source": [
    "# E2E Test Results\n",
    "The E2E test results are pretty good! However, it doesn't account for scenarios where you simply don't have the correct context. This is where RAGAS is really useful. It will provide some more granular metrics for us. However, a word of caution, it is very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea29a9-60f3-4e78-a8ff-b86bb46025f1",
   "metadata": {},
   "source": [
    "# Bonus - Use RAGAS\n",
    "In this last section, we'll use a library called Ragas as an alternative to building this all manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fccbe-18fe-44ba-b56c-46a705b3c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness\n",
    ")\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "class RAGASEvaluator:\n",
    "    def __init__(self, embedding_model, model_id, bedrock_client):\n",
    "        self.embedding_model_id = embedding_model\n",
    "        self.model_id = model_id\n",
    "        self.bedrock_client = bedrock_client\n",
    "        \n",
    "        # Define embedding model for RAGAS\n",
    "        self.ragas_embedding_model = BedrockEmbeddings(model_id=self.embedding_model_id, client=self.bedrock_client)\n",
    "        \n",
    "        # Define the llm to use for RAGAS evaluation\n",
    "        self.ragas_llm = ChatBedrock(\n",
    "            model_id=self.model_id,\n",
    "            model_kwargs = {\"temperature\": 0.5, \"max_tokens\": 2048}\n",
    "        )\n",
    "        \n",
    "        # Define metrics we care about\n",
    "        self.METRICS = [\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness\n",
    "        ]\n",
    "\n",
    "    def notebook_to_hf_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        # Initialize the new data structure\n",
    "        data_samples = {\n",
    "            'question': [],\n",
    "            'answer': [],\n",
    "            'contexts': [],\n",
    "            'ground_truth': []\n",
    "        }\n",
    "        \n",
    "        # Iterate through each row in the dataframe\n",
    "        for _, row in df.iterrows():\n",
    "            # Add the question (query_text)\n",
    "            data_samples['question'].append(row['query_text'])\n",
    "            \n",
    "            # Add the answer (llm_response)\n",
    "            data_samples['answer'].append(row['llm_response'])\n",
    "            \n",
    "            # Parse the context_chunks JSON string and add to contexts\n",
    "            context_chunks = json.loads(row['context_chunks'])\n",
    "            data_samples['contexts'].append(context_chunks)\n",
    "            \n",
    "            # Add the ground truth\n",
    "            data_samples['ground_truth'].append(row['ground_truth'])\n",
    "        \n",
    "        # Create and return the Dataset object\n",
    "        return Dataset.from_dict(data_samples)\n",
    "\n",
    "    def evaluate_rag(self, rag_df: pd.DataFrame):\n",
    "        # Convert dataframe to HuggingFace dataset\n",
    "        ragas_dataset = self.notebook_to_hf_dataset(rag_df)\n",
    "        \n",
    "        # Perform evaluation\n",
    "        result = evaluate(\n",
    "            ragas_dataset,\n",
    "            metrics=self.METRICS,\n",
    "            llm=self.ragas_llm,\n",
    "            embeddings=self.ragas_embedding_model,\n",
    "            run_config=RunConfig(max_workers=1)\n",
    "        )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fce97e-22ba-4ad1-9c32-45485f1514c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Ragas Evaluation\n",
    "HAIKU_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "ragas_evaluator = RAGASEvaluator(EMBEDDING_MODEL, HAIKU_ID, bedrock)\n",
    "ragas_results = ragas_evaluator.evaluate_rag(rag_df)\n",
    "print(json.dumps(ragas_results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865ed11-b9df-49ad-8969-6a7a31c52bd5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook we combined our embeddings, ReRank, and prompt together to run E2E tests on our entire system. We explored doing this all manually and also explored using a tool like RAGAS. Based on our findings, document level chunking worked very well for this use case. \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"context_precision\": 0.9583333332833334,\n",
    "    \"faithfulness\": 0.8954968944099378,\n",
    "    \"answer_relevancy\": 0.7086921115930934,\n",
    "    \"context_recall\": 1.0,\n",
    "    \"answer_correctness\": 0.6089388020774851\n",
    "}\n",
    "```\n",
    "\n",
    "## Takeaways\n",
    "While tools like RAGAS are great for quick experiments and evaluating an end to end systems, it often times isn't as flexible as building something custom. It also doesn't provide the level of granularity you want when building an IR system in general. By adding validation datasets at each touchpoint in your LLM augmented system, we can get a much more comprehensive view of what's happening and where the bottlenecks to better performance are. \n",
    "\n",
    "Another important takeaway is that how you chunk your data matters (aguably) more than what model you choose to vend the RAG results. We explored only basic chunking strategies. The pros to them are that they're easy, fast, and don't cost a lot of credits to index your data. The downside to them is that they're basic and more advanced chunking strategies could unlock greater performance. \n",
    "\n",
    "No dataset or IR problem is exactly the same. It's important to evaluate it to understand how your system is performing and update your chunking and validation sets over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
