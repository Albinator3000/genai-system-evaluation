{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52226f6b-2660-4851-818e-efecea8766c9",
   "metadata": {},
   "source": [
    "# Human Evaluation\n",
    "\n",
    "## What Will We Do? \n",
    "* Create an eval prompt template (notebook 4a) \n",
    "* Create two different RAG prompt templates and compare results (notebook 4a) \n",
    "* Run LLM-As-A-Judge against an eval dataset and grade RAG system response (notebook 4a) \n",
    "* **Run Human-Eval to ensure LLM-As-A-Judge is aligned with human preferences (notebook 4b)** \n",
    "* **Run Human-Eval and compare GroundTruth with RAG system response (notebook 4b)**\n",
    "* Run through evaluation with different LLMs (notebook 4c) \n",
    "\n",
    "So let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65b599-7663-4785-bd75-0f139216fa17",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "7c04eab6-7444-45ae-8ce8-f731398c51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda create -y --name llm-system-eval python=3.11.7\n",
    "# !conda init && activate llm-system-eval\n",
    "# !conda install -n llm-system-eval ipykernel --update-deps --force-reinstall -y\n",
    "# OR\n",
    "# !pyenv virtualenv 3.11.7 llm-system-eval\n",
    "# !pyenv activate llm-system-eval\n",
    "\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c38224",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b58a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:lambda:us-east-1:432418664414:function:PRE-PassThrough\n",
      "arn:aws:lambda:us-east-1:432418664414:function:ACS-PassThrough\n"
     ]
    }
   ],
   "source": [
    "# set variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'llm-system-eval.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "REGION = os.environ['REGION']\n",
    "os.environ['WORKTEAM_ARN'] = os.getenv('WORKTEAM_ARN')\n",
    "WORKTEAM_ARN = os.environ['WORKTEAM_ARN']\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "os.environ['SAGEMAKER_ROLE_ARN'] = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "SAGEMAKER_ROLE_ARN = os.environ['SAGEMAKER_ROLE_ARN'] # OR sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "\n",
    "# Also initialize the bedrock client so we can call some embedding models!\n",
    "session = boto3.Session(profile_name='default')\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "role_name = SAGEMAKER_ROLE_ARN.split(\"/\")[-1]\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify ARNs for resources needed to run an text classification job.\n",
    "ac_arn_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\"\n",
    "}\n",
    "# PreHumanTaskLambdaArn for text classification(single)\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-PassThrough\".format(\n",
    "    REGION, ac_arn_map[REGION]\n",
    ")\n",
    "\n",
    "# AnnotationConsolidationConfig for text classification(single)\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-PassThrough\".format(REGION, ac_arn_map[REGION])\n",
    "\n",
    "print(prehuman_arn)\n",
    "print(acs_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64bd19",
   "metadata": {},
   "source": [
    "# Human Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503cb53",
   "metadata": {},
   "source": [
    "## Human Eval1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b646f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. The file ../data/eval-datasets/4_review_judge.json is ready.\n",
      "File uploaded to s3://sagemaker-us-east-1-026459568683/genai/4_review_judge.json\n"
     ]
    }
   ],
   "source": [
    "# Human Eval1: review LLM-as-a-Judge evaluation results with human evaluation results to ensure they align.\n",
    "\n",
    "# Step 1: Load jsonl file, e.g. 4_answer_validation_opensearch_graded1.jsonl\n",
    "\n",
    "human_eval_df = pd.read_json('../data/eval-datasets/4_answer_validation_opensearch_graded1.jsonl', lines=True)\n",
    "\n",
    "eval_criteria = '''Evaluation Criteria (Additive Score, 0-3):\n",
    "                    1. Context: Award 1 point if the answer uses only information provided in the context, without introducing external or fabricated details.\n",
    "                    2. Completeness: Add 1 point if the answer addresses all key elements of the question based on the available context, without omissions.\n",
    "                    3. Conciseness: Add a final point if the answer uses the fewest words possible to address the question and avoids redundancy.\n",
    "                    '''\n",
    "# Step 2: Convert to the desired format for GroundTruth\n",
    "formatted_data = \"\"\n",
    "for index, row in human_eval_df.iterrows():\n",
    "    entry = {\n",
    "        \"source\": row[\"query_text\"],\n",
    "        \"response\": row[\"generated_answer\"],\n",
    "        \"eval_criteria\": eval_criteria,\n",
    "        \"reasoning\": row[\"reasoning\"],\n",
    "        \"score\": row[\"score\"],\n",
    "        \"retrieved_chunks\": str(row[\"retrieved_chunks\"]),\n",
    "    }\n",
    "    formatted_data += json.dumps(entry) + \"\\n\"\n",
    "\n",
    "# Step 3: Save the output as a new JSON file\n",
    "local_path = '../data/eval-datasets/4_review_judge.json'\n",
    "with open(local_path, 'w') as f:\n",
    "    f.write(formatted_data)\n",
    "\n",
    "print(f\"Conversion complete. The file {local_path} is ready.\")\n",
    "\n",
    "# Step 4: Upload the manifest to S3\n",
    "bucket_name = bucket\n",
    "prefix = 'genai'\n",
    "# Upload the file to S3\n",
    "manifest_s3_uri = sagemaker_session.upload_data(path=local_path, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(f\"File uploaded to {manifest_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23393f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template file uploaded to s3://sagemaker-us-east-1-026459568683/genai/4_instructions_judge_review.template\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create template for judge eval review\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def make_template(save_fname=\"4_instructions_judge_review.template\"):\n",
    "    template = \"\"\"\n",
    "      <html>\n",
    "      <head>\n",
    "        <meta charset=\"UTF-8\" />\n",
    "        <script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "        <script src=\"https://assets.crowd.aws/gen-ai-components.js\"></script>\n",
    "        <script>\n",
    "          document.getElementById('submitButton').onclick = function() {\n",
    "              document.querySelector('crowd-form').submit();\n",
    "          };\n",
    "        </script>\n",
    "        <style>\n",
    "          body {\n",
    "            padding-left: 20px;\n",
    "            margin-bottom: 20px;\n",
    "          }\n",
    "          .highlight {\n",
    "            background-color: yellow;\n",
    "          }\n",
    "          .container {\n",
    "            margin-bottom: 20px;\n",
    "          }\n",
    "          .outer-container {\n",
    "            display: flex;\n",
    "            justify-content: space-between;\n",
    "            width: 100%;\n",
    "            max-width: 1400px;\n",
    "          }\n",
    "          .left-container, .right-container {\n",
    "            padding: 20px;\n",
    "            min-width: 75%;\n",
    "          }\n",
    "          .right-container {\n",
    "            border-left: 2px solid #d5dbdb;\n",
    "          }\n",
    "        </style>\n",
    "      </head>\n",
    "    <body>\n",
    "    <crowd-form>\n",
    "      <!-- Prevent crowd-form from creating its own button -->\n",
    "      <crowd-button form-action=\"submit\" style=\"display: none;\"></crowd-button> \n",
    "      <div class=\"container\">\n",
    "        <h1>Instructions</h1>\n",
    "        Review the prompt, RAG System response, RAG Context, and evaluation critera. Then complete the tasks on the right panel using your best judgment. \n",
    "      </div>\n",
    "      <br>\n",
    "      <div class=\"outer-container\">\n",
    "        <div class=\"left-container\">\n",
    "          <h2>Evaluation Criteria</h2>\n",
    "          <div>{{ task.input.eval_criteria }}</div>\n",
    "          <br>\n",
    "          <h2>Original Prompt</h2>\n",
    "          <div>{{ task.input.source }}</div>\n",
    "          <br>\n",
    "          <h2>RAG System Response</h2>\n",
    "          <div>{{ task.input.response }}</div>\n",
    "          <br>\n",
    "          <h2>RAG Context</h2>\n",
    "          <div>{{ task.input.retrieved_chunks }}</div>\n",
    "          <br> \n",
    "          <h2>LLM-as-a-Judge Score</h2>\n",
    "          <div>{{ task.input.score }}</div>\n",
    "          <br>\n",
    "          <h2>LLM-as-a-Judge Reasoning</h2>\n",
    "          <div>{{ task.input.reasoning }}</div>\n",
    "          \n",
    "                \n",
    "        </div>\n",
    "        <div class=\"right-container\">\n",
    "          <h2>Evaluation</h2>\n",
    "          <div>\n",
    "            <label for=\"context\">Context:</label>\n",
    "            <div>\n",
    "              <input type=\"radio\" id=\"context1\" name=\"context\" value=\"1\" required>\n",
    "              <label for=\"context1\">1 Point</label><br>\n",
    "              <input type=\"radio\" id=\"context2\" name=\"context\" value=\"0\">\n",
    "              <label for=\"context2\">0 Point</label><br>\n",
    "            </div>\n",
    "          </div>\n",
    "          <br>\n",
    "          <div>\n",
    "            <label for=\"completeness\">Completeness:</label>\n",
    "            <div>\n",
    "              <input type=\"radio\" id=\"completeness1\" name=\"completeness\" value=\"1\" required>\n",
    "              <label for=\"completeness1\">1 Point</label><br>\n",
    "              <input type=\"radio\" id=\"completeness2\" name=\"completeness\" value=\"0\">\n",
    "              <label for=\"completeness2\">0 Point</label><br>\n",
    "            </div>\n",
    "          </div>\n",
    "          <br>\n",
    "          <div>\n",
    "            <label for=\"conciseness\">Conciseness:</label>\n",
    "            <div>\n",
    "              <input type=\"radio\" id=\"conciseness1\" name=\"conciseness\" value=\"1\" required>\n",
    "              <label for=\"conciseness1\">1 Point</label><br>\n",
    "              <input type=\"radio\" id=\"conciseness2\" name=\"conciseness\" value=\"0\">\n",
    "              <label for=\"conciseness2\">0 Point</label><br>\n",
    "            </div>\n",
    "          </div>\n",
    "          <br>\n",
    "          <div>\n",
    "            <label for=\"issues\">Select any issues observed:</label>\n",
    "            <div>\n",
    "              <input type=\"checkbox\" id=\"grammar\" name=\"issues\" value=\"grammar\">\n",
    "              <label for=\"grammar\">Grammar issues</label><br>\n",
    "              <input type=\"checkbox\" id=\"factual\" name=\"issues\" value=\"factual\">\n",
    "              <label for=\"factual\">Factual inaccuracies</label><br>\n",
    "              <input type=\"checkbox\" id=\"missing\" name=\"issues\" value=\"missing\">\n",
    "              <label for=\"missing\">Missing information</label><br>\n",
    "              <input type=\"checkbox\" id=\"other\" name=\"issues\" value=\"other\">\n",
    "              <label for=\"other\">Other</label>\n",
    "            </div>\n",
    "          </div>\n",
    "          <br>\n",
    "          <div>\n",
    "           <crowd-button id=\"submitButton\">Submit</crowd-button>\n",
    "          </div\n",
    "        </div>\n",
    "      </div>\n",
    "    </crowd-form>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    with open(save_fname, \"w\") as f:\n",
    "        f.write(template)\n",
    "\n",
    "# Create the template file locally\n",
    "local_path ='../data/eval-datasets/4_instructions_judge_review.template'\n",
    "make_template(save_fname=local_path)\n",
    "\n",
    "# Upload the file to S3\n",
    "template_s3_uri = sagemaker_session.upload_data(path=local_path, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(f\"Template file uploaded to {template_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ddbdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LabelingJobArn': 'arn:aws:sagemaker:us-east-1:026459568683:labeling-job/rag-review-llm-judge-20240827-233822',\n",
       " 'ResponseMetadata': {'RequestId': '700b4ee3-ad60-4d9c-aee4-786aaa0cf8f1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '700b4ee3-ad60-4d9c-aee4-786aaa0cf8f1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '111',\n",
       "   'date': 'Wed, 28 Aug 2024 03:38:22 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Create Human Eval job with GroundTruth: review LLM as a Judge evaluation results with human evaluation results\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "labeling_job_name = \"rag-review-llm-judge-\" + timestamp_str\n",
    "\n",
    "sagemaker_client.create_labeling_job(\n",
    "    LabelingJobName=labeling_job_name,\n",
    "    LabelAttributeName='label',\n",
    "    InputConfig={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'ManifestS3Uri': manifest_s3_uri #Enter S3 URI of Input Data Json\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputPath': f's3://{bucket}/{prefix}/output/' #Enter S3 URI of Output folder\n",
    "    },\n",
    "    RoleArn=SAGEMAKER_ROLE_ARN, #Enter IAM Sagemaker Execution Role here,\n",
    "    HumanTaskConfig={\n",
    "        'WorkteamArn': WORKTEAM_ARN, #Enter Workteam ARN\n",
    "        'UiConfig': {\n",
    "            'UiTemplateS3Uri': template_s3_uri #Enter S3 URI of UI template\n",
    "        },\n",
    "        'PreHumanTaskLambdaArn': prehuman_arn,\n",
    "        'TaskKeywords': [\n",
    "            'QnA',\n",
    "        ],\n",
    "        'TaskTitle': labeling_job_name,\n",
    "        'TaskDescription': \"Review the LLM-as-a-Judge results using the dimensions on the side\",\n",
    "        'NumberOfHumanWorkersPerDataObject': 1,\n",
    "        'TaskTimeLimitInSeconds': 60*30,\n",
    "        'TaskAvailabilityLifetimeInSeconds': 60*60*24*10,\n",
    "        'MaxConcurrentTaskCount': 100,\n",
    "        'AnnotationConsolidationConfig': {\n",
    "            'AnnotationConsolidationLambdaArn': acs_arn\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f681dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<body>\n",
       "<h4>Review LLM-as-a-Judge results</h4>\n",
       "<p>Please complete the human evaluation tasks available in the labeling portal.</p>\n",
       "<p><a href=\"https://f649f6hr50.labeling.us-east-1.sagemaker.aws\">https://f649f6hr50.labeling.us-east-1.sagemaker.aws</a>\n",
       "<p><b>Ensure all tasks are completed before proceeding to the next steps in this notebook.<b></p>\n",
       "<body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 7: Do Human evaluation\n",
    "workforce = sagemaker_client.describe_workforce(WorkforceName=\"default\")\n",
    "worker_portal_url = 'https://' + workforce[\"Workforce\"][\"SubDomain\"]\n",
    "\n",
    "\n",
    "# Display the URL and instructions\n",
    "display(HTML(f\"\"\"\n",
    "<body>\n",
    "<h4>Review LLM-as-a-Judge results</h4>\n",
    "<p>Please complete the human evaluation tasks available in the labeling portal.</p>\n",
    "<p><a href=\"{worker_portal_url}\">{worker_portal_url}</a>\n",
    "<p><b>Ensure all tasks are completed before proceeding to the next steps in this notebook.<b></p>\n",
    "<body>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086aa948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LabelingJobStatus': 'Completed',\n",
       " 'LabelCounters': {'TotalLabeled': 24,\n",
       "  'HumanLabeled': 24,\n",
       "  'MachineLabeled': 0,\n",
       "  'FailedNonRetryableError': 0,\n",
       "  'Unlabeled': 0},\n",
       " 'CreationTime': datetime.datetime(2024, 8, 27, 23, 38, 23, 55000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 8, 28, 11, 2, 40, 498000, tzinfo=tzlocal()),\n",
       " 'JobReferenceCode': '026459568683QNSoxT1GelBEmC4k',\n",
       " 'LabelingJobName': 'rag-review-llm-judge-20240827-233822',\n",
       " 'LabelingJobArn': 'arn:aws:sagemaker:us-east-1:026459568683:labeling-job/rag-review-llm-judge-20240827-233822',\n",
       " 'LabelAttributeName': 'label',\n",
       " 'InputConfig': {'DataSource': {'S3DataSource': {'ManifestS3Uri': 's3://sagemaker-us-east-1-026459568683/genai/4_review_judge.json'}}},\n",
       " 'OutputConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-026459568683/genai/output/',\n",
       "  'KmsKeyId': ''},\n",
       " 'RoleArn': 'arn:aws:iam::026459568683:role/demo-SagemakerNotebookIAMRole-026459568683',\n",
       " 'StoppingConditions': {'MaxPercentageOfInputDatasetLabeled': 100},\n",
       " 'HumanTaskConfig': {'WorkteamArn': 'arn:aws:sagemaker:us-east-1:026459568683:workteam/private-crowd/evalteam',\n",
       "  'UiConfig': {'UiTemplateS3Uri': 's3://sagemaker-us-east-1-026459568683/genai/4_instructions_judge_review.template'},\n",
       "  'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-PassThrough',\n",
       "  'TaskKeywords': ['QnA'],\n",
       "  'TaskTitle': 'rag-review-llm-judge-20240827-233822',\n",
       "  'TaskDescription': 'Review the LLM-as-a-Judge results using the dimensions on the side',\n",
       "  'NumberOfHumanWorkersPerDataObject': 1,\n",
       "  'TaskTimeLimitInSeconds': 1800,\n",
       "  'TaskAvailabilityLifetimeInSeconds': 864000,\n",
       "  'MaxConcurrentTaskCount': 100,\n",
       "  'AnnotationConsolidationConfig': {'AnnotationConsolidationLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:ACS-PassThrough'}},\n",
       " 'LabelingJobOutput': {'OutputDatasetS3Uri': 's3://sagemaker-us-east-1-026459568683/genai/output/rag-review-llm-judge-20240827-233822/manifests/output/output.manifest',\n",
       "  'FinalActiveLearningModelArn': ''},\n",
       " 'ResponseMetadata': {'RequestId': '6c92d34d-03c9-43d2-a482-5cb3c70d400e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6c92d34d-03c9-43d2-a482-5cb3c70d400e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1788',\n",
       "   'date': 'Wed, 28 Aug 2024 15:05:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: monitor the job's progress\n",
    "sagemaker_client.describe_labeling_job(LabelingJobName=labeling_job_name)\n",
    "# stop labeling job\n",
    "# sagemaker_client.stop_labeling_job(LabelingJobName=labeling_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a35e5732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>source</th>\n",
       "      <th>response</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>score</th>\n",
       "      <th>completeness</th>\n",
       "      <th>conciseness</th>\n",
       "      <th>context</th>\n",
       "      <th>human_eval_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working?</td>\n",
       "      <td>There is not enough information available to answer this question. The context does not mention why zstd compression is not working for your specific case with OpenSearch version 2.1. The context only provides information about the zstd compression codec being introduced in OpenSearch 2.9 and later versions.</td>\n",
       "      <td>Context:\\n1. The context does not provide any information about why zstd compression is not working for OpenSearch version 2.1. It only mentions that zstd compression was introduced in OpenSearch 2.9 and later versions.\\n\\nCompleteness:\\n1. The answer correctly states that there is not enough information in the context to explain why zstd compression is not working for OpenSearch 2.1.\\n2. The answer addresses the key elements of the question by acknowledging that the context does not cover version 2.1 and only provides information about compression codecs available in 2.9 and later versions.\\n\\nConciseness:\\n1. The answer is concise and does not contain any redundant information. It directly states that the context lacks the necessary information to answer the question.</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I'm trying to set up this new aggregate view thing for saved objects in OpenSearch Dashboards, but I'm worried about messing up our existing multi-tenancy setup. The docs mention something about tenant indexes and a kibana_server role. How do I make sure I don't break anything when I turn this feature on? And what's the deal with not being able to turn it off once it's enabled</td>\n",
       "      <td>The docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. As for the kibana_server role, it is required for OpenSearch Dashboards to perform necessary operations and has permissions defined for various indexes and actions. There is not enough information provided about how to ensure the existing multi-tenancy setup is not broken when enabling the aggregate view feature.</td>\n",
       "      <td>Context:\\n1. The context mentions that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag. It also states that once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The context does not provide any information on how to ensure the existing multi-tenancy setup is not broken when enabling this feature.\\n\\nCompleteness:\\n1. The answer addresses the key elements of the question by mentioning that the aggregate view feature is experimental, must be enabled using a feature flag, and cannot be disabled once enabled without potential issues. However, it does not provide any information on how to ensure the existing multi-tenancy setup is not broken, as mentioned in the question.\\n\\nConciseness:\\n1. The answer is concise and does not contain any redundant information.</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What software license does Opensearch use?</td>\n",
       "      <td>There is not enough information available to answer this question</td>\n",
       "      <td>Context:\\n1. The context does not provide any information about the software license used by OpenSearch. Therefore, I cannot award a point for this criterion.\\n\\nCompleteness:\\n2. The generated answer correctly states that there is not enough information available to answer the question based on the provided context. Therefore, I will award 1 point for completeness.\\n\\nConciseness:\\n3. The generated answer is concise and uses the fewest words possible to address the question without redundancy. I will award 1 point for conciseness.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Does GPU accelerated nodes support Pytorch?</td>\n",
       "      <td>Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report.</td>\n",
       "      <td>Context:\\n1. The context clearly states that \"GPU-accelerated ML nodes require [PyTorch](https://pytorch.org/docs/stable/index.html) 1.12.1 work with ML models.\" Therefore, the answer uses information provided in the context and does not introduce external or fabricated details. I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The question asks if GPU accelerated nodes support PyTorch, and the answer confirms that they support PyTorch 1.12.1 according to the report. This addresses all key elements of the question based on the available context, so I will award 1 point for completeness.\\n\\nConciseness:\\n3. The answer is concise and uses the fewest words possible to address the question without redundancy. I will award 1 point for conciseness.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Does opensearch support hugging face models? If so which ones?</td>\n",
       "      <td>Yes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context.</td>\n",
       "      <td>Context:\\n1. The context provides a detailed list of Hugging Face models supported by OpenSearch for text embedding, sparse encoding, and cross-encoding tasks. It covers the model names, versions, vector dimensions, auto-truncation support, and artifact links for downloading the models.\\n\\nCompleteness:\\n2. The generated answer directly addresses the original question by confirming that OpenSearch supports Hugging Face models and specifying which ones are supported under the \"Sentence transformers\" and \"Cross-encoder models\" sections. The answer covers all key elements of the question based on the provided context.\\n\\nConciseness:\\n3. The answer is concise and uses the fewest words possible to address the question without redundancy.\\n\\nScoring:\\nContext: 1 point - The answer uses only information provided in the context without introducing external details.\\nCompleteness: 1 point - The answer addresses all key elements of the question based on the available context.\\nConciseness: 1 point - The answer is concise and avoids redundancy.\\n\\nTotal score: 3/3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I have a custom model, can I run it in Opensearch?</td>\n",
       "      <td>Yes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search.</td>\n",
       "      <td>1. Context: The context provides detailed information about registering and deploying models in OpenSearch, including custom models, pretrained models, and models hosted on third-party platforms. It covers the required parameters and provides examples for each scenario.\\n\\n2. Completeness: The generated answer fully addresses the original question of whether custom models can be run in OpenSearch. It confirms that custom models can be registered and deployed in OpenSearch, and provides the necessary steps and parameters to do so.\\n\\n3. Conciseness: The generated answer is concise and directly addresses the question without redundancy.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I have a model and some ML nodes, how do I boost it's performance?</td>\n",
       "      <td>To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.</td>\n",
       "      <td>Context:\\n- The context provides information on how to boost the performance of an ML model by using GPU acceleration on ML nodes in an OpenSearch cluster.\\n- It mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia.\\n- It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.\\n\\nQuestion:\\n\"I have a model and some ML nodes, how do I boost it's performance?\"\\n\\nGenerated answer:\\n\"To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.\"\\n\\nEvaluation based on criteria:\\n\\n1. Context: The answer uses only information provided in the context, without introducing external or fabricated details. Award 1 point.\\n\\n2. Completeness: The answer addresses all key elements of the question based on the available context, without omissions. It explains how to boost performance by using GPU acceleration and mentions the supported GPU instances. Award 1 point.\\n\\n3. Conciseness: The answer is concise and uses the fewest words possible to address the question without redundancy. Award 1 point.\\n\\nTotal score: 3/3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you show me an example of how to use lat/long coordinates?</td>\n",
       "      <td>The report provides several examples of how to use latitude and longitude coordinates to index a geopoint field in OpenSearch:\\n\\n- As an object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\\n- As a string with latitude and longitude separated by a comma: \"40.71,74.00\"\\n- As a geohash string: \"txhxegj0uyp3\"\\n- As an array with longitude first, then latitude: [74.00, 40.71]\\n- As a Well-Known Text string: \"POINT (74.00 40.71)\"\\n- As a GeoJSON object: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }</td>\n",
       "      <td>Context:\\n1. The answer uses only information provided in the context, without introducing any external or fabricated details. It directly quotes examples from the context, so I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The answer addresses all key elements of the question by providing multiple examples of how to use latitude and longitude coordinates to index a geopoint field. It covers the various formats mentioned in the context, so I will award 1 point for completeness.\\n\\nConciseness:\\n3. The answer includes all the relevant examples from the context without any unnecessary redundancy. It uses a concise format to present the examples, so I will award 1 point for conciseness as well.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I use vector search?</td>\n",
       "      <td>There is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search.</td>\n",
       "      <td>Context:\\n- The context provides an overview of vector search in OpenSearch, including how to create vector indexes, ingest vector data, and perform vector searches using different methods (approximate k-NN, script score k-NN, and Painless extensions).\\n- It covers the different engines (nmslib, Faiss, Lucene) and algorithms (HNSW, IVF) supported for approximate k-NN search, as well as how to choose the right method based on requirements like query latency, query quality, memory limits, and indexing latency.\\n- The context also explains how to generate vector embeddings within OpenSearch using the Neural Search plugin.\\n\\nCompleteness:\\n- The context does not provide specific instructions on how to use vector search, but rather gives an overview of the different vector search capabilities in OpenSearch.\\n- It does not cover the actual steps or API requests needed to perform a vector search query.\\n\\nConciseness:\\n- The context provides a comprehensive overview of vector search in OpenSearch without unnecessary details or redundancy.\\n\\nBased on the evaluation criteria, I would award the following scores:\\n\\nContext: 1 point - The answer uses only information provided in the context.\\nCompleteness: 0 points - The answer does not address how to use vector search, which is the key element of the question.\\nConciseness: 1 point - The answer is concise and does not contain redundant information.\\n\\nTotal score: 2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I understand the memory requirements for using hnsw?</td>\n",
       "      <td>The memory requirements for using HNSW with vector quantization depend on the quantization method used:\\n\\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\\n\\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes/vector.\\n\\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension.</td>\n",
       "      <td>Context:\\n1. The answer uses information directly from the provided context, without introducing any external or fabricated details. It provides the memory estimation formulas for HNSW and IVF with different quantization methods like Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. So I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The answer addresses all key elements of the question by providing detailed memory estimation formulas and examples for different quantization methods used with HNSW and IVF. It covers the memory requirements for using HNSW with vector quantization comprehensively based on the given context. So I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The answer is quite detailed and comprehensive in covering the different quantization methods and their memory estimation formulas. While it could potentially be more concise, the level of detail provided is necessary to fully address the question based on the given context. Therefore, I will award 1 point for this criterion.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you show me some different examples of using different quantization methods for vectors?</td>\n",
       "      <td>There is not enough information available to answer this question. The given context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.</td>\n",
       "      <td>Context:\\n1. The context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.\\n\\nCompleteness:\\n1. The generated answer correctly states that the context does not provide enough information to answer the question about showing examples of using different quantization methods for vectors. It addresses the key elements of the question based on the available context.\\nScore: 1\\n\\nConciseness:\\n1. The generated answer is concise and uses the fewest words possible to address the lack of information in the context to answer the question.\\nScore: 1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Would you recommend I use ReRank? If so, what type of model would you recommend for ReRank?</td>\n",
       "      <td>Based on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model.</td>\n",
       "      <td>Context:\\n1. The answer uses information directly from the provided context, without introducing any external or fabricated details. It accurately summarizes the key points about using ReRank with a cross-encoder model for improving search relevance. Therefore, I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The answer addresses all key elements of the question. It recommends using ReRank, suggests using a cross-encoder model, and mentions the options of using an OpenSearch-provided model or a custom local model, as stated in the context. Therefore, I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The answer is concise and uses the fewest words necessary to address the question without redundancy. It does not include any unnecessary or extraneous information. Therefore, I will award 1 point for this criterion.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?</td>\n",
       "      <td>To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.</td>\n",
       "      <td>Context:\\n1. The generated answer provides details on how to set up a tiered cache with an on-heap and disk tier for the index request cache, which directly addresses the question. It also covers how to configure the cache to prevent excessive resource consumption. Therefore, I will award 1 point for using information from the context.\\n\\nCompleteness:\\n2. The generated answer covers all the key elements of the question, including setting up the tiered cache, configuring the on-heap and disk tiers, and tweaking settings to manage resource usage. It does not omit any crucial aspects based on the available context. Therefore, I will award 1 point for completeness.\\n\\nConciseness:\\n3. The generated answer is concise and does not contain any redundant information. It provides the necessary steps and configuration details in a clear and succinct manner. Therefore, I will award 1 point for conciseness.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I need to combine relevance scores from different types of searches.</td>\n",
       "      <td>A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.</td>\n",
       "      <td>Context:\\n1. The answer uses information directly from the context, without introducing any external or fabricated details. It accurately describes what a hybrid query is and how it combines relevance scores from multiple queries. Therefore, I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The answer addresses the key elements of the question by explaining how a hybrid query allows combining relevance scores from different types of searches. It provides details on how a hybrid query contains an array of query clauses and combines their relevance scores using a search pipeline. The answer covers the main aspects needed to answer the original question, so I will award 1 point for completeness.\\n\\nConciseness:\\n3. The answer is concise and uses a reasonable number of words to convey the necessary information without redundancy. It could potentially be more concise by removing the last sentence about disabling hybrid queries, as that is not directly relevant to the question. However, the answer is still relatively concise overall, so I will award 1 point for this criterion.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What metrics does query metrics give me out of the box?</td>\n",
       "      <td>According to the report, query metrics provide the following out-of-the-box metrics:\\n\\n- Number of queries per query type (e.g. match, regex)\\n- Number of queries per aggregation type (e.g. terms)\\n- Number of queries per sort order (e.g. ascending, descending)\\n- Histograms of latency for each query type, aggregation type, and sort order\\n- Histograms of CPU usage for each query type, aggregation type, and sort order\\n- Histograms of memory usage for each query type, aggregation type, and sort order</td>\n",
       "      <td>Context:\\n1. The answer directly quotes the relevant information from the context, without introducing any external or fabricated details. So I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The answer covers all the key metrics mentioned in the context that query metrics provides out-of-the-box, without any omissions. So I will award 1 point for this criterion as well.\\n\\nConciseness: \\n3. The answer is a direct quote from the context and uses the fewest words possible to address the question, without any redundancy. So I will award 1 point for this criterion too.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Does opensearch work with open telemetry?</td>\n",
       "      <td>There is not enough information available to answer this question</td>\n",
       "      <td>Context:\\nThe context provides information about OpenSearch's Metric Analytics feature, including how to ingest and visualize metrics from OpenTelemetry and Prometheus. However, it does not explicitly mention whether OpenSearch works with OpenTelemetry or not.\\n\\nCompleteness:\\nThe answer \"There is not enough information available to answer this question\" is complete, as the context does not provide enough details to determine if OpenSearch works with OpenTelemetry. The answer addresses the key elements of the question based on the available context without omissions. Award 1 point.\\n\\nContext:\\nThe answer uses only the information provided in the context and does not introduce any external or fabricated details. Award 1 point.\\n\\nConciseness:\\nThe answer is concise and uses the fewest words possible to address the question without redundancy. Award 1 point.</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?</td>\n",
       "      <td>To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT _plugins/_notifications/configs/<config_id>\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/sample-url\"\\n    }\\n  }\\n}</td>\n",
       "      <td>Context:\\n1. The answer uses information directly from the context, specifically the \"Update channel configuration\" section, which provides the exact steps and example to update an existing Slack channel configuration using the Notifications API. No external or fabricated details are introduced, so 1 point is awarded for this criterion.\\n\\nCompleteness:\\n2. The answer addresses all key elements of the question by providing the HTTP method (PUT), the endpoint (/_plugins/_notifications/configs/<config_id>), and the required request body with the updated webhook URL and description. It also includes a clear example demonstrating the request. Therefore, 1 point is awarded for completeness.\\n\\nConciseness:\\n3. The answer is concise and uses the fewest words necessary to address the question without redundancy. It does not include any extraneous or unnecessary information, so 1 point is awarded for conciseness.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?</td>\n",
       "      <td>The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.</td>\n",
       "      <td>Context:\\n1. The answer uses information directly from the context, specifically the list of permissions required for installing the anomaly detection plugin. It does not introduce any external or fabricated details, so I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The answer addresses all key elements of the question by providing the specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin, as well as the exact object this permission applies to. It does not omit any important information from the question, so I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The answer is concise and uses the fewest words possible to address the question without redundancy. It directly quotes the relevant permissions from the context, so I will award 1 point for this criterion.\\n\\nTotal score: 3 (1 + 1 + 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What environment variable and YAML structure must be used to customize the admin password in a helm chart?</td>\n",
       "      <td>To customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\\n\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password></td>\n",
       "      <td>Context:\\n1. The answer uses information directly from the context, specifically the section \"For OpenSearch 2.12 or greater, customize the admin password in `values.yaml` under `extraEnvs`...\" and the provided example. Therefore, it meets the context criterion and should be awarded 1 point.\\n\\nCompleteness:\\n2. The answer addresses all key elements of the question by specifying the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD and the YAML structure extraEnvs in the values.yaml file. It provides a complete example as well. Therefore, it meets the completeness criterion and should be awarded 1 point.\\n\\nConciseness:\\n3. The answer is concise and uses the fewest words necessary to address the question without redundancy. Therefore, it meets the conciseness criterion and should be awarded 1 point.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?</td>\n",
       "      <td>The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.</td>\n",
       "      <td>Context:\\n- The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent document.\\n- It does not provide information on how to group data by a nested field while also calculating metrics on a parent-level field in the same query.\\n\\nCompleteness:\\n- The generated answer correctly states that the context does not provide enough information to answer the original question. It acknowledges that the reverse_nested aggregation is explained, but does not cover grouping by nested fields and calculating parent-level metrics together. Therefore, I will award 1 point for completeness.\\n\\nContext:\\n- The generated answer does not introduce any external or fabricated details. It only refers to the information provided in the context. Therefore, I will award 1 point for using only the context.\\n\\nConciseness:\\n- The generated answer is concise and uses the fewest words possible to address the question without redundancy. Therefore, I will award 1 point for conciseness.</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Make it simpler like what happens when I have missing values in an agg?</td>\n",
       "      <td>When there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N/A\"` will place missing values in a bucket named \"N/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0.</td>\n",
       "      <td>Context:\\n1. The answer uses information directly from the provided context, without introducing any external or fabricated details. It explains how the `missing` parameter works in aggregations and provides examples from the context. Therefore, I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The answer addresses all key elements of the question, which asks about what happens when there are missing values in an aggregation. It explains how to handle missing values using the `missing` parameter and how to include the missing bucket in the response by setting `min_doc_count` to 0. Therefore, I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The answer is concise and uses the fewest words necessary to address the question without redundancy. It does not include any unnecessary information or repetition. Therefore, I will award 1 point for this criterion.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you explain compound queries to me?</td>\n",
       "      <td>Compound queries are used to combine multiple leaf or compound queries together using Boolean logic or other methods. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Assigns a constant relevance score to all documents matching a query or filter\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for a document</td>\n",
       "      <td>Context:\\n1. The context provides a detailed explanation of compound queries and lists different types of compound queries like bool, boosting, constant_score, dis_max, function_score, and hybrid. The generated answer covers all these types, so it uses information from the context without introducing external details. I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The question asks to explain compound queries. The generated answer provides a good overview of compound queries and lists several examples with brief descriptions. It addresses the key elements of the question based on the context. I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The generated answer is concise and uses a reasonable number of words to explain compound queries and provide examples. It avoids redundancy. I will award 1 point for this criterion.\\n\\nTotal score: 3 (1 + 1 + 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is the difference between vector search and full text search?</td>\n",
       "      <td>The main difference between vector search and full text search is:\\n\\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\\n\\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\\n\\nThere is not enough information available in the given context to explain the difference between vector search and full text search.</td>\n",
       "      <td>Context:\\n1. The context provides a detailed explanation of the difference between vector search and full text search, covering aspects like how they work, what they are used for, and when to use each type of search. Since the generated answer accurately reflects the information given in the context without introducing any external details, I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The generated answer comprehensively addresses the key elements of the question by clearly explaining the difference between vector search and full text search. It covers how vector search finds semantically similar documents based on vector representations, while full text search finds documents containing the exact query terms. The answer does not omit any important aspects based on the available context, so I will award 1 point for completeness.\\n\\nConciseness:\\n3. The generated answer is concise and uses the fewest words necessary to address the question without redundancy. It directly states the main difference between the two search types in a clear and succinct manner. Therefore, I will award 1 point for conciseness.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Why can't my coworker see my async searches, but I can see and delete theirs? This role thing is messing everything up!</td>\n",
       "      <td>Based on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa.</td>\n",
       "      <td>Context:\\n1. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. If the generated answer uses only this information from the context without introducing external details, it should be awarded 1 point.\\n\\nCompleteness:\\n2. The question asks why the coworker cannot see the user's async searches but the user can see and delete the coworker's searches. The generated answer addresses this by explaining the role-based access control mechanism described in the context, so it should be awarded 1 point for completeness.\\n\\nConciseness: \\n3. The generated answer is concise and does not contain redundant information, so it should be awarded 1 point for conciseness.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Step 9: Compare HumanEval with LLM-as-a-Judge results\n",
    "def load_json_from_s3(bucket, key):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    data = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "    return data\n",
    "\n",
    "source_bucket_name = bucket\n",
    "key = f'{prefix}/output/{labeling_job_name}/manifests/output/output.manifest'\n",
    "\n",
    "# Download the manifest file\n",
    "response = s3_client.get_object(Bucket=source_bucket_name, Key=key)\n",
    "content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Split the content into lines\n",
    "lines = content.strip().split('\\n')\n",
    "\n",
    "# Parse each line as JSON and create a list of dictionaries\n",
    "data = [json.loads(line) for line in lines]\n",
    "manifest_df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in manifest_df.iterrows():\n",
    "    label_metadata = row['label-metadata']\n",
    "    key = label_metadata['worker-response-ref'].replace(f's3://{bucket}/','')\n",
    "    data = load_json_from_s3(bucket, key)\n",
    "    completeness = 0\n",
    "    conciseness = 0\n",
    "    context = 0\n",
    "    for answer in data[\"answers\"]:\n",
    "        human_eval_score = 0\n",
    "        if answer['answerContent']['completeness']['1'] == True:\n",
    "            human_eval_score += 1\n",
    "            completeness = 1\n",
    "\n",
    "        if answer['answerContent']['conciseness']['1'] == True:\n",
    "            human_eval_score += 1\n",
    "            conciseness = 1\n",
    "        \n",
    "        if answer['answerContent']['context']['1'] == True:\n",
    "            human_eval_score += 1\n",
    "            context = 1\n",
    "        \n",
    "    manifest_df.at[index, 'completeness'] = completeness\n",
    "    manifest_df.at[index, 'conciseness'] = conciseness\n",
    "    manifest_df.at[index, 'context'] = context\n",
    "    manifest_df.at[index, 'human_eval_score'] = human_eval_score\n",
    "\n",
    "manifest_df = manifest_df.drop(columns=['label','eval_criteria','retrieved_chunks','label-metadata'])\n",
    "\n",
    "# Convert the dataframe to an HTML table with escaped HTML content\n",
    "table_html = manifest_df.to_html(index=False, classes='table table-striped', escape=False)\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b4fb3",
   "metadata": {},
   "source": [
    "## Human Eval2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29158040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. The file '4_gt_input_manifest_textranking.json' is ready.\n",
      "File uploaded to s3://sagemaker-us-east-1-026459568683/genai/4_gt_input_manifest_textranking.json\n"
     ]
    }
   ],
   "source": [
    "# Human Eval2: compare ground truth against generated anwsers to ensure it is in line with human preferences\n",
    "\n",
    "# Step 1: Load jsonl file, e.g. 4_answer_validation_opensearch_graded1.jsonl\n",
    "\n",
    "human_eval_df = pd.read_json('../data/eval-datasets/4_answer_validation_opensearch_graded1.jsonl', lines=True)\n",
    "\n",
    "# Step 2: Convert to the desired format\n",
    "import re\n",
    "ANSWER_PATTERN = r'<question_answer>(.*?)</question_answer>'\n",
    "\n",
    "# Strip out the portion of the response with regex.\n",
    "def extract_with_regex(response, regex):\n",
    "    matches = re.search(regex, response, re.DOTALL)\n",
    "    # Extract the matched content, if any\n",
    "    return matches.group(1).strip() if matches else None\n",
    "\n",
    "formatted_data = \"\"\n",
    "for index, row in human_eval_df.iterrows():\n",
    "    entry = {\n",
    "        \"source\": row[\"query_text\"],\n",
    "        \"responses\": [extract_with_regex(row[\"groundtruth_answer\"], ANSWER_PATTERN), row[\"generated_answer\"]]\n",
    "        \n",
    "    }\n",
    "    formatted_data += json.dumps(entry) + \"\\n\"\n",
    "\n",
    "# Step 3: Save the output as a new JSON file\n",
    "with open('../data/eval-datasets/4_gt_input_manifest_textranking.json', 'w') as f:\n",
    "    f.write(formatted_data)\n",
    "\n",
    "print(\"Conversion complete. The file '4_gt_input_manifest_textranking.json' is ready.\")\n",
    "\n",
    "# Upload the manifest to S3\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = bucket\n",
    "prefix = 'genai'\n",
    "local_path = '../data/eval-datasets/4_gt_input_manifest_textranking.json'\n",
    "\n",
    "# Upload the file to S3\n",
    "manifest_s3_uri = sagemaker_session.upload_data(path=local_path, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(f\"File uploaded to {manifest_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42573f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template file uploaded to s3://sagemaker-us-east-1-026459568683/genai/4_instructions_tr.template\n"
     ]
    }
   ],
   "source": [
    "# Create Review Template\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def make_template(save_fname=\"instructions_tr.template\"):\n",
    "    template = \"\"\"\n",
    "    <html>\n",
    "      <head>\n",
    "        <meta charset=\"UTF-8\" />\n",
    "        <link rel=\"stylesheet\" href=\"https://assets.crowd.aws/css/gen-ai-components.css\" />\n",
    "        <link rel=\"icon\" href=\"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>&#129351;</text></svg>\" />\n",
    "        <title>Ranking Tool</title>\n",
    "        <script src=\"https://assets.crowd.aws/gen-ai-components.js\"></script>\n",
    "      </head>\n",
    "\n",
    "      <body>\n",
    "        <div>\n",
    "          <crowd-text-ranking\n",
    "            crowd-form-element-id=\"crowd-form-submit\"\n",
    "            instructions='Rank the following responses from a RAG system according to the evaluation criteria.'\n",
    "            ordinal-ranking-dimensions='[{\"name\":\"Accuracy\",\"allowTie\":true},{\"name\":\"Clarity\",\"allowTie\":true}]'\n",
    "            text='{{ task.input.source }}'\n",
    "            responses='{{ task.input.responses | to_json }}' />\n",
    "        </div>\n",
    "        <crowd-form id=\"crowd-form-submit\" style=\"display: none\"></crowd-form>\n",
    "        <script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    with open(save_fname, \"w\") as f:\n",
    "        f.write(template)\n",
    "\n",
    "# Create the template file locally\n",
    "local_path ='../data/eval-datasets/4_instructions_tr.template'\n",
    "make_template(save_fname=local_path)\n",
    "\n",
    "# Upload the file to S3\n",
    "template_s3_uri = sagemaker_session.upload_data(path=local_path, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(f\"Template file uploaded to {template_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "488cb40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LabelingJobArn': 'arn:aws:sagemaker:us-east-1:026459568683:labeling-job/rag-system-eval-20240828-140613',\n",
       " 'ResponseMetadata': {'RequestId': 'b029fd7b-60b4-456f-90f7-82a1c608187e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b029fd7b-60b4-456f-90f7-82a1c608187e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '106',\n",
       "   'date': 'Wed, 28 Aug 2024 18:06:13 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "labeling_job_name = \"rag-system-eval-\" + timestamp_str\n",
    "\n",
    "sagemaker_client.create_labeling_job(\n",
    "    LabelingJobName=labeling_job_name,\n",
    "    LabelAttributeName='label',\n",
    "    InputConfig={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'ManifestS3Uri': manifest_s3_uri #Enter S3 URI of Input Data Json\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputPath': f's3://{bucket}/{prefix}/output/' #Enter S3 URI of Output folder\n",
    "    },\n",
    "    RoleArn=SAGEMAKER_ROLE_ARN, #Enter IAM Sagemaker Execution Role here,\n",
    "    HumanTaskConfig={\n",
    "        'WorkteamArn': WORKTEAM_ARN, #Enter Workteam ARN\n",
    "        'UiConfig': {\n",
    "            'UiTemplateS3Uri': template_s3_uri #Enter S3 URI of UI template\n",
    "        },\n",
    "        'PreHumanTaskLambdaArn': prehuman_arn,\n",
    "        'TaskKeywords': [\n",
    "            'QnA',\n",
    "        ],\n",
    "        'TaskTitle': labeling_job_name,\n",
    "        'TaskDescription': \"Rank the responses provided by the Models based on the Dimensions\",\n",
    "        'NumberOfHumanWorkersPerDataObject': 1,\n",
    "        'TaskTimeLimitInSeconds': 60*30,\n",
    "        'TaskAvailabilityLifetimeInSeconds': 60*60*24*10,\n",
    "        'MaxConcurrentTaskCount': 100,\n",
    "        'AnnotationConsolidationConfig': {\n",
    "            'AnnotationConsolidationLambdaArn': acs_arn\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3db6a224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<body>\n",
       "<h4>Gather human preference data</h4>\n",
       "<p>Please complete the human evaluation tasks available in the labeling portal.</p>\n",
       "<p><a href=\"https://f649f6hr50.labeling.us-east-1.sagemaker.aws\">https://f649f6hr50.labeling.us-east-1.sagemaker.aws</a>\n",
       "<p><b>Ensure all tasks are completed before proceeding to the next steps in this notebook.<b></p>\n",
       "<body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workforce = sagemaker_client.describe_workforce(WorkforceName=\"default\")\n",
    "worker_portal_url = 'https://' + workforce[\"Workforce\"][\"SubDomain\"]\n",
    "\n",
    "\n",
    "# Display the URL and instructions\n",
    "display(HTML(f\"\"\"\n",
    "<body>\n",
    "<h4>Gather human preference data</h4>\n",
    "<p>Please complete the human evaluation tasks available in the labeling portal.</p>\n",
    "<p><a href=\"{worker_portal_url}\">{worker_portal_url}</a>\n",
    "<p><b>Ensure all tasks are completed before proceeding to the next steps in this notebook.<b></p>\n",
    "<body>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d17e1f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LabelingJobStatus': 'Completed',\n",
       " 'LabelCounters': {'TotalLabeled': 24,\n",
       "  'HumanLabeled': 24,\n",
       "  'MachineLabeled': 0,\n",
       "  'FailedNonRetryableError': 0,\n",
       "  'Unlabeled': 0},\n",
       " 'CreationTime': datetime.datetime(2024, 8, 28, 14, 6, 14, 76000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 8, 28, 14, 38, 53, 84000, tzinfo=tzlocal()),\n",
       " 'JobReferenceCode': '026459568683UKqXoQllz4To3x2V',\n",
       " 'LabelingJobName': 'rag-system-eval-20240828-140613',\n",
       " 'LabelingJobArn': 'arn:aws:sagemaker:us-east-1:026459568683:labeling-job/rag-system-eval-20240828-140613',\n",
       " 'LabelAttributeName': 'label',\n",
       " 'InputConfig': {'DataSource': {'S3DataSource': {'ManifestS3Uri': 's3://sagemaker-us-east-1-026459568683/genai/4_gt_input_manifest_textranking.json'}}},\n",
       " 'OutputConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-026459568683/genai/output/',\n",
       "  'KmsKeyId': ''},\n",
       " 'RoleArn': 'arn:aws:iam::026459568683:role/demo-SagemakerNotebookIAMRole-026459568683',\n",
       " 'StoppingConditions': {'MaxPercentageOfInputDatasetLabeled': 100},\n",
       " 'HumanTaskConfig': {'WorkteamArn': 'arn:aws:sagemaker:us-east-1:026459568683:workteam/private-crowd/evalteam',\n",
       "  'UiConfig': {'UiTemplateS3Uri': 's3://sagemaker-us-east-1-026459568683/genai/4_instructions_tr.template'},\n",
       "  'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-PassThrough',\n",
       "  'TaskKeywords': ['QnA'],\n",
       "  'TaskTitle': 'rag-system-eval-20240828-140613',\n",
       "  'TaskDescription': 'Rank the responses provided by the Models based on the Dimensions',\n",
       "  'NumberOfHumanWorkersPerDataObject': 1,\n",
       "  'TaskTimeLimitInSeconds': 1800,\n",
       "  'TaskAvailabilityLifetimeInSeconds': 864000,\n",
       "  'MaxConcurrentTaskCount': 100,\n",
       "  'AnnotationConsolidationConfig': {'AnnotationConsolidationLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:ACS-PassThrough'}},\n",
       " 'LabelingJobOutput': {'OutputDatasetS3Uri': 's3://sagemaker-us-east-1-026459568683/genai/output/rag-system-eval-20240828-140613/manifests/output/output.manifest',\n",
       "  'FinalActiveLearningModelArn': ''},\n",
       " 'ResponseMetadata': {'RequestId': 'dc000ac2-1f1b-4ca0-9cfc-1c546d31cff3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'dc000ac2-1f1b-4ca0-9cfc-1c546d31cff3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1774',\n",
       "   'date': 'Wed, 28 Aug 2024 18:48:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# monitor the job's progress\n",
    "sagemaker_client.describe_labeling_job(LabelingJobName=labeling_job_name)\n",
    "# sagemaker_client.stop_labeling_job(LabelingJobName=labeling_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f5d766d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>source</th>\n",
       "      <th>responses</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working?</td>\n",
       "      <td>[There is not enough information available to answer this question. The context does not mention why zstd compression is not working for OpenSearch version 2.1. The context provides information about compression codecs available in OpenSearch 2.9 and later versions, but does not cover version 2.1., There is not enough information available to answer this question. The context does not mention why zstd compression is not working for your specific case with OpenSearch version 2.1. The context only provides information about the zstd compression codec being introduced in OpenSearch 2.9 and later versions.]</td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question. The context does not mention why zstd compression is not working for OpenSearch version 2.1. The context provides information about compression codecs available in OpenSearch 2.9 and later versions, but does not cover version 2.1.</span><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question. The context does not mention why zstd compression is not working for your specific case with OpenSearch version 2.1. The context only provides information about the zstd compression codec being introduced in OpenSearch 2.9 and later versions.</span></td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question. The context does not mention why zstd compression is not working for OpenSearch version 2.1. The context provides information about compression codecs available in OpenSearch 2.9 and later versions, but does not cover version 2.1.</span><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question. The context does not mention why zstd compression is not working for your specific case with OpenSearch version 2.1. The context only provides information about the zstd compression codec being introduced in OpenSearch 2.9 and later versions.</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I'm trying to set up this new aggregate view thing for saved objects in OpenSearch Dashboards, but I'm worried about messing up our existing multi-tenancy setup. The docs mention something about tenant indexes and a kibana_server role. How do I make sure I don't break anything when I turn this feature on? And what's the deal with not being able to turn it off once it's enabled</td>\n",
       "      <td>[The docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. There is no information provided about how to ensure the existing multi-tenancy setup is not broken when enabling this feature., The docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. As for the kibana_server role, it is required for OpenSearch Dashboards to perform necessary operations and has permissions defined for various indexes and actions. There is not enough information provided about how to ensure the existing multi-tenancy setup is not broken when enabling the aggregate view feature.]</td>\n",
       "      <td>The docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. There is no information provided about how to ensure the existing multi-tenancy setup is not broken when enabling this feature.The docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. As for the kibana_server role, it is required for OpenSearch Dashboards to perform necessary operations and has permissions defined for various indexes and actions. There is not enough information provided about how to ensure the existing multi-tenancy setup is not broken when enabling the aggregate view feature.</td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">The docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. There is no information provided about how to ensure the existing multi-tenancy setup is not broken when enabling this feature.</span><span style=\"background-color: #CCFFCC;\">The docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. As for the kibana_server role, it is required for OpenSearch Dashboards to perform necessary operations and has permissions defined for various indexes and actions. There is not enough information provided about how to ensure the existing multi-tenancy setup is not broken when enabling the aggregate view feature.</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What software license does Opensearch use?</td>\n",
       "      <td>[There is not enough information available to answer this question, There is not enough information available to answer this question]</td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question</span><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question</span></td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question</span><span style=\"background-color: #CCFFCC;\">There is not enough information available to answer this question</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Does GPU accelerated nodes support Pytorch?</td>\n",
       "      <td>[Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report., Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report.]</td>\n",
       "      <td>Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report.Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report.</td>\n",
       "      <td>Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report.Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Does opensearch support hugging face models? If so which ones?</td>\n",
       "      <td>[Yes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context., Yes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context.]</td>\n",
       "      <td>Yes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context.Yes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context.</td>\n",
       "      <td>Yes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context.Yes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I have a custom model, can I run it in Opensearch?</td>\n",
       "      <td>[Yes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search., Yes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search.]</td>\n",
       "      <td>Yes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search.Yes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search.</td>\n",
       "      <td>Yes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search.Yes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I have a model and some ML nodes, how do I boost it's performance?</td>\n",
       "      <td>[To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration., To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.]</td>\n",
       "      <td>To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.</td>\n",
       "      <td>To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.To boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you show me an example of how to use lat/long coordinates?</td>\n",
       "      <td>[The report provides several examples of how to use latitude and longitude coordinates to index a geopoint field:\\n\\n- An object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\\n- A string with latitude and longitude separated by a comma: \"40.71,74.00\"\\n- A geohash string: \"txhxegj0uyp3\"\\n- An array with longitude first, then latitude: [74.00, 40.71]\\n- A Well-Known Text string: \"POINT (74.00 40.71)\"\\n- A GeoJSON object with \"type\": \"Point\" and \"coordinates\" array: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }, The report provides several examples of how to use latitude and longitude coordinates to index a geopoint field in OpenSearch:\\n\\n- As an object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\\n- As a string with latitude and longitude separated by a comma: \"40.71,74.00\"\\n- As a geohash string: \"txhxegj0uyp3\"\\n- As an array with longitude first, then latitude: [74.00, 40.71]\\n- As a Well-Known Text string: \"POINT (74.00 40.71)\"\\n- As a GeoJSON object: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }]</td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">The report provides several examples of how to use latitude and longitude coordinates to index a geopoint field:\\n\\n- An object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\\n- A string with latitude and longitude separated by a comma: \"40.71,74.00\"\\n- A geohash string: \"txhxegj0uyp3\"\\n- An array with longitude first, then latitude: [74.00, 40.71]\\n- A Well-Known Text string: \"POINT (74.00 40.71)\"\\n- A GeoJSON object with \"type\": \"Point\" and \"coordinates\" array: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }</span><span style=\"background-color: #CCFFCC;\">The report provides several examples of how to use latitude and longitude coordinates to index a geopoint field in OpenSearch:\\n\\n- As an object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\\n- As a string with latitude and longitude separated by a comma: \"40.71,74.00\"\\n- As a geohash string: \"txhxegj0uyp3\"\\n- As an array with longitude first, then latitude: [74.00, 40.71]\\n- As a Well-Known Text string: \"POINT (74.00 40.71)\"\\n- As a GeoJSON object: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }</span></td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">The report provides several examples of how to use latitude and longitude coordinates to index a geopoint field:\\n\\n- An object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\\n- A string with latitude and longitude separated by a comma: \"40.71,74.00\"\\n- A geohash string: \"txhxegj0uyp3\"\\n- An array with longitude first, then latitude: [74.00, 40.71]\\n- A Well-Known Text string: \"POINT (74.00 40.71)\"\\n- A GeoJSON object with \"type\": \"Point\" and \"coordinates\" array: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }</span><span style=\"background-color: #CCFFCC;\">The report provides several examples of how to use latitude and longitude coordinates to index a geopoint field in OpenSearch:\\n\\n- As an object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\\n- As a string with latitude and longitude separated by a comma: \"40.71,74.00\"\\n- As a geohash string: \"txhxegj0uyp3\"\\n- As an array with longitude first, then latitude: [74.00, 40.71]\\n- As a Well-Known Text string: \"POINT (74.00 40.71)\"\\n- As a GeoJSON object: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I use vector search?</td>\n",
       "      <td>[There is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search., There is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search.]</td>\n",
       "      <td>There is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search.There is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search.</td>\n",
       "      <td>There is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search.There is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I understand the memory requirements for using hnsw?</td>\n",
       "      <td>[The memory requirements for using HNSW with vector quantization depend on the quantization method used:\\n\\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\\n\\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes/vector.\\n\\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension., The memory requirements for using HNSW with vector quantization depend on the quantization method used:\\n\\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\\n\\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes/vector.\\n\\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension.]</td>\n",
       "      <td>The memory requirements for using HNSW with vector quantization depend on the quantization method used:\\n\\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\\n\\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes/vector.\\n\\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension.The memory requirements for using HNSW with vector quantization depend on the quantization method used:\\n\\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\\n\\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes/vector.\\n\\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension.</td>\n",
       "      <td>The memory requirements for using HNSW with vector quantization depend on the quantization method used:\\n\\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\\n\\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes/vector.\\n\\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension.The memory requirements for using HNSW with vector quantization depend on the quantization method used:\\n\\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\\n\\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes/vector.\\n\\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you show me some different examples of using different quantization methods for vectors?</td>\n",
       "      <td>[There is not enough information available to answer this question. The context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods., There is not enough information available to answer this question. The given context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.]</td>\n",
       "      <td>There is not enough information available to answer this question. The context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.There is not enough information available to answer this question. The given context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.</td>\n",
       "      <td>There is not enough information available to answer this question. The context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.There is not enough information available to answer this question. The given context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Would you recommend I use ReRank? If so, what type of model would you recommend for ReRank?</td>\n",
       "      <td>[Based on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model., Based on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model.]</td>\n",
       "      <td>Based on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model.Based on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model.</td>\n",
       "      <td>Based on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model.Based on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?</td>\n",
       "      <td>[To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed., To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.]</td>\n",
       "      <td>To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.</td>\n",
       "      <td>To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I need to combine relevance scores from different types of searches.</td>\n",
       "      <td>[A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline., A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.]</td>\n",
       "      <td>A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.</td>\n",
       "      <td>A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What metrics does query metrics give me out of the box?</td>\n",
       "      <td>[According to the report, query metrics provide the following out-of-the-box metrics:\\n\\n- Number of queries per query type (e.g. match, regex)\\n- Number of queries per aggregation type (e.g. terms)\\n- Number of queries per sort order (e.g. ascending, descending)\\n- Histograms of latency for each query type, aggregation type, and sort order\\n- Histograms of CPU usage for each query type, aggregation type, and sort order\\n- Histograms of memory usage for each query type, aggregation type, and sort order, According to the report, query metrics provide the following out-of-the-box metrics:\\n\\n- Number of queries per query type (e.g. match, regex)\\n- Number of queries per aggregation type (e.g. terms)\\n- Number of queries per sort order (e.g. ascending, descending)\\n- Histograms of latency for each query type, aggregation type, and sort order\\n- Histograms of CPU usage for each query type, aggregation type, and sort order\\n- Histograms of memory usage for each query type, aggregation type, and sort order]</td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">According to the report, query metrics provide the following out-of-the-box metrics:\\n\\n- Number of queries per query type (e.g. match, regex)\\n- Number of queries per aggregation type (e.g. terms)\\n- Number of queries per sort order (e.g. ascending, descending)\\n- Histograms of latency for each query type, aggregation type, and sort order\\n- Histograms of CPU usage for each query type, aggregation type, and sort order\\n- Histograms of memory usage for each query type, aggregation type, and sort order</span><span style=\"background-color: #CCFFCC;\">According to the report, query metrics provide the following out-of-the-box metrics:\\n\\n- Number of queries per query type (e.g. match, regex)\\n- Number of queries per aggregation type (e.g. terms)\\n- Number of queries per sort order (e.g. ascending, descending)\\n- Histograms of latency for each query type, aggregation type, and sort order\\n- Histograms of CPU usage for each query type, aggregation type, and sort order\\n- Histograms of memory usage for each query type, aggregation type, and sort order</span></td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">According to the report, query metrics provide the following out-of-the-box metrics:\\n\\n- Number of queries per query type (e.g. match, regex)\\n- Number of queries per aggregation type (e.g. terms)\\n- Number of queries per sort order (e.g. ascending, descending)\\n- Histograms of latency for each query type, aggregation type, and sort order\\n- Histograms of CPU usage for each query type, aggregation type, and sort order\\n- Histograms of memory usage for each query type, aggregation type, and sort order</span><span style=\"background-color: #CCFFCC;\">According to the report, query metrics provide the following out-of-the-box metrics:\\n\\n- Number of queries per query type (e.g. match, regex)\\n- Number of queries per aggregation type (e.g. terms)\\n- Number of queries per sort order (e.g. ascending, descending)\\n- Histograms of latency for each query type, aggregation type, and sort order\\n- Histograms of CPU usage for each query type, aggregation type, and sort order\\n- Histograms of memory usage for each query type, aggregation type, and sort order</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Does opensearch work with open telemetry?</td>\n",
       "      <td>[There is not enough information available to answer this question, There is not enough information available to answer this question]</td>\n",
       "      <td>There is not enough information available to answer this questionThere is not enough information available to answer this question</td>\n",
       "      <td>There is not enough information available to answer this questionThere is not enough information available to answer this question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?</td>\n",
       "      <td>[To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT /_plugins/_notifications/configs/<config_id>\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/new-webhook-url\"\\n    }\\n  }\\n}, To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT _plugins/_notifications/configs/<config_id>\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/sample-url\"\\n    }\\n  }\\n}]</td>\n",
       "      <td>To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT /_plugins/_notifications/configs/<config_id>\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/new-webhook-url\"\\n    }\\n  }\\n}To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT _plugins/_notifications/configs/<config_id>\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/sample-url\"\\n    }\\n  }\\n}</td>\n",
       "      <td>To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT /_plugins/_notifications/configs/<config_id>\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/new-webhook-url\"\\n    }\\n  }\\n}To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT _plugins/_notifications/configs/<config_id>\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/sample-url\"\\n    }\\n  }\\n}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?</td>\n",
       "      <td>[The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object., The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.]</td>\n",
       "      <td>The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.</td>\n",
       "      <td>The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What environment variable and YAML structure must be used to customize the admin password in a helm chart?</td>\n",
       "      <td>[To customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\\n\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password>, To customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\\n\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password>]</td>\n",
       "      <td>To customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\\n\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password>To customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\\n\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password></td>\n",
       "      <td>To customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\\n\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password>To customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\\n\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?</td>\n",
       "      <td>[The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query., The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.]</td>\n",
       "      <td>The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.</td>\n",
       "      <td>The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Make it simpler like what happens when I have missing values in an agg?</td>\n",
       "      <td>[When there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N/A\"` will place missing values in a bucket named \"N/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0., When there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N/A\"` will place missing values in a bucket named \"N/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0.]</td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">When there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N/A\"` will place missing values in a bucket named \"N/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0.</span><span style=\"background-color: #CCFFCC;\">When there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N/A\"` will place missing values in a bucket named \"N/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0.</span></td>\n",
       "      <td><span style=\"background-color: #CCFFCC;\">When there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N/A\"` will place missing values in a bucket named \"N/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0.</span><span style=\"background-color: #CCFFCC;\">When there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N/A\"` will place missing values in a bucket named \"N/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0.</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you explain compound queries to me?</td>\n",
       "      <td>[Compound queries are used to combine multiple leaf or compound queries together using Boolean logic or other operations. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Wraps a query and assigns a constant relevance score to all matching documents\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for each document, Compound queries are used to combine multiple leaf or compound queries together using Boolean logic or other methods. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Assigns a constant relevance score to all documents matching a query or filter\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for a document]</td>\n",
       "      <td><span style=\"background-color: yellow;\">Compound queries are used to combine multiple leaf or compound queries together using Boolean logic or other methods. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Assigns a constant relevance score to all documents matching a query or filter\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for a document</span><span style=\"background-color: yellow;\">Compound queries are used to combine multiple leaf or compound queries together using Boolean logic or other operations. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Wraps a query and assigns a constant relevance score to all matching documents\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for each document</span></td>\n",
       "      <td>Compound queries are used to combine multiple leaf or compound queries together using Boolean logic or other operations. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Wraps a query and assigns a constant relevance score to all matching documents\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for each documentCompound queries are used to combine multiple leaf or compound queries together using Boolean logic or other methods. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Assigns a constant relevance score to all documents matching a query or filter\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for a document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is the difference between vector search and full text search?</td>\n",
       "      <td>[The main difference between vector search and full text search is:\\n\\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\\n\\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\\n\\nThere is not enough information available in the given context to explain the difference between vector search and full text search., The main difference between vector search and full text search is:\\n\\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\\n\\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\\n\\nThere is not enough information available in the given context to explain the difference between vector search and full text search.]</td>\n",
       "      <td>The main difference between vector search and full text search is:\\n\\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\\n\\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\\n\\nThere is not enough information available in the given context to explain the difference between vector search and full text search.The main difference between vector search and full text search is:\\n\\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\\n\\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\\n\\nThere is not enough information available in the given context to explain the difference between vector search and full text search.</td>\n",
       "      <td>The main difference between vector search and full text search is:\\n\\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\\n\\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\\n\\nThere is not enough information available in the given context to explain the difference between vector search and full text search.The main difference between vector search and full text search is:\\n\\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\\n\\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\\n\\nThere is not enough information available in the given context to explain the difference between vector search and full text search.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Why can't my coworker see my async searches, but I can see and delete theirs? This role thing is messing everything up!</td>\n",
       "      <td>[Based on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa., Based on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa.]</td>\n",
       "      <td>Based on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa.Based on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa.</td>\n",
       "      <td>Based on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa.Based on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def load_json_from_s3(bucket, key):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    data = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "    return data\n",
    "\n",
    "source_bucket_name = bucket\n",
    "key = f'{prefix}/output/{labeling_job_name}/manifests/output/output.manifest'\n",
    "\n",
    "# Download the manifest file\n",
    "response = s3_client.get_object(Bucket=source_bucket_name, Key=key)\n",
    "content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Split the content into lines\n",
    "lines = content.strip().split('\\n')\n",
    "\n",
    "# Parse each line as JSON and create a list of dictionaries\n",
    "data = [json.loads(line) for line in lines]\n",
    "manifest_df = pd.DataFrame(data)\n",
    "\n",
    "def highlight_response(original, ranked, rank, is_changed, has_same_rank):\n",
    "    if is_changed:\n",
    "        return f'<span style=\"background-color: yellow;\">{ranked}</span>'\n",
    "    elif has_same_rank:\n",
    "        return f'<span style=\"background-color: #CCFFCC;\">{ranked}</span>'\n",
    "    return ranked\n",
    "\n",
    "for index, row in manifest_df.iterrows():\n",
    "    responses = row['responses']\n",
    "    # print(f'responses: {responses}')\n",
    "    label_metadata = row['label-metadata']\n",
    "    key = label_metadata['worker-response-ref'].replace(f's3://{bucket}/','')\n",
    "    data = load_json_from_s3(bucket, key)\n",
    "    for answer in data[\"answers\"]:\n",
    "        for dimension in answer[\"answerContent\"][\"ordinalRankingDimensions\"]:\n",
    "            rankings = dimension[\"responseRankings\"]\n",
    "            \n",
    "            # Store the original rankings in a new field\n",
    "            dimension[\"responseIndex\"] = rankings\n",
    "            # print(f'rankings: {rankings}')\n",
    "            \n",
    "            # Create a list to store the response texts based on rankings\n",
    "            ranked_responses = [responses[i - 1] for i, _ in sorted(enumerate(rankings, start=1), key=lambda x: x[1])]\n",
    "            \n",
    "            # Identify responses with the same ranking\n",
    "            same_rank_indices = [i for i, rank in enumerate(rankings) if rankings.count(rank) > 1]\n",
    "            \n",
    "            # Compare ranked_responses with original responses and highlight differences\n",
    "            highlighted_responses = []\n",
    "            for i, (orig, ranked) in enumerate(zip(responses, ranked_responses)):\n",
    "                is_changed = orig != ranked\n",
    "                has_same_rank = i in same_rank_indices\n",
    "                highlighted_responses.append(highlight_response(orig, ranked, rankings[i], is_changed, has_same_rank))\n",
    "            \n",
    "            dimension[\"responseRankings\"] = highlighted_responses\n",
    "            if dimension['name'] == 'Accuracy':\n",
    "                manifest_df.at[index, 'Accuracy'] = ''.join(highlighted_responses)\n",
    "            if dimension['name'] == 'Clarity':\n",
    "                manifest_df.at[index, 'Clarity'] = ''.join(highlighted_responses)\n",
    "\n",
    "manifest_df = manifest_df.drop(columns=['label','label-metadata'])\n",
    "\n",
    "# Convert the dataframe to an HTML table with escaped HTML content\n",
    "table_html = manifest_df.to_html(index=False, classes='table table-striped', escape=False)\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f14b1e-b99d-468f-a574-5b7b44251eac",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* With the first human evaluation, we ensure alignment between human preferences and the LLM-as-a-Judge, which is more scalable.\n",
    "* With the second human evaluation, we assess if the generated response and ground truth is aligned with the human preference.\n",
    "* An important consideration is how you want to improve the system. Being able to quickly find poor performing prompts / inputs is key to improving it. In a feedback loop, it’s common to take poor performing examples, correct them, and use them as dynamic few shot examples in future prompts. If you’re going down the model training path, it’s important to collect data points in pairs. RLHF whether that’s PPO or DPO relies on pair wise data when going through data cycles. If you output diverse pairs of answers, it saves data annotators time because they are rating their preference between pairs vs. having to hand write the answer they prefer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
