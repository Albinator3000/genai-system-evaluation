{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52226f6b-2660-4851-818e-efecea8766c9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this notebook is to demonstrate how to evaluate a rerank model. In the previous notebook, we saw that as we increased k where k is the number of chunks returned, we got better recall@k scores. This is fairly intuitive, but also poses another problem. If we increase K too much, won't that just mean higher input token costs and a greater chance that the LLM glosses the answer since it's receiving too much info?  \n",
    "\n",
    "This is one of the key benefits of adding a rerank model into your IR system. If we can narrow down the list of possible context chunks to < 10, we can add a separate model to output a relevance score. We can remove inputs that are below a threshold and use these scores to \"rerank\" the outputs so that the model gets only the most relevant outputs from our vector search. \n",
    "\n",
    "That's exactly what we'll do in this notebook. We'll incorporate a ReRank model using Bedrock and validate whether it's improving core metrics.\n",
    "\n",
    "# Background\n",
    "When evaluating a rerank model, it's crucial to focus on metrics that reflect both the quality and relevance of the reranked results, as well as the model's ability to improve upon the initial ranking. Normalized Discounted Cumulative Gain (NDCG) is often considered one of the most important metrics, as it accounts for the position of relevant items in the ranked list and can handle graded relevance judgments. Mean Average Precision (MAP) is another valuable metric that provides a single figure of merit for the overall ranking quality across multiple queries. For scenarios where the top results are particularly important, metrics like Precision@k and Mean Reciprocal Rank (MRR) can offer insights into the model's performance at specific cut-off points.\n",
    " \n",
    "In addition to these standard information retrieval metrics, it's beneficial to consider comparative metrics that directly measure the improvement over the base ranking. This can include the percentage of queries improved, the average change in relevant document positions, or a paired statistical test (such as a Wilcoxon signed-rank test) comparing the reranked results to the original ranking. It's also important to evaluate the model's efficiency, considering factors like inference time and computational resources required, especially for applications with strict latency requirements. Ultimately, the choice of metrics should align with the specific goals of your reranking task and the priorities of your system, balancing between relevance, user satisfaction, and operational constraints.\n",
    "\n",
    "How to Evaluate\n",
    "To evaluate a rerank model using these metrics, start by preparing a test set consisting of queries, their corresponding initial rankings, and human-annotated relevance judgments for each query-document pair. Run your rerank model on the initial rankings to produce a new set of reranked results. Then, calculate the chosen metrics for both the initial and reranked results. For example, to compute NDCG@k, sort the documents for each query by their relevance scores, calculate the Discounted Cumulative Gain (DCG) for the top k results, and normalize it by the Ideal DCG. For MAP, calculate the average precision for each query at every position where a relevant document is retrieved, then take the mean across all queries. To assess improvement, compare the metric scores between the initial and reranked results. You can use statistical tests like paired t-tests or Wilcoxon signed-rank tests to determine if the improvements are significant. It's also valuable to analyze per-query performance to identify where the rerank model excels or struggles. Finally, consider evaluating on different subsets of your data to ensure consistent performance across various query types or document categories.\n",
    "\n",
    "\n",
    "\n",
    "# What Will We Do? \n",
    "* We'll the best run from our previous model, run it through a ReRank model, and then recalculate the results.\n",
    "\n",
    "**Lets get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25742ac5-32d5-4961-9772-288e66c4f007",
   "metadata": {},
   "source": [
    "# Get Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d0514-977c-403f-84cb-612305010b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_clean_eval_dataset():\n",
    "    EVAL_PATH = '../data/eval-datasets/2_rerank_validation.csv'\n",
    "    eval_df = pd.read_csv(EVAL_PATH)\n",
    "\n",
    "    # Clean up the DataFrame\n",
    "    eval_df = eval_df.rename(columns=lambda x: x.strip())  # Remove any leading/trailing whitespace from column names\n",
    "    eval_df = eval_df.drop(columns=[col for col in eval_df.columns if col.startswith('Unnamed')])  # Remove unnamed columns\n",
    "    eval_df = eval_df.dropna(how='all')  # Remove rows that are all NaN\n",
    "    \n",
    "    # Strip whitespace from string columns\n",
    "    for col in eval_df.select_dtypes(['object']):\n",
    "        eval_df[col] = eval_df[col].str.strip()\n",
    "    \n",
    "    # Ensure 'relevant_doc_ids' is a string column\n",
    "    eval_df['relevant_doc_ids'] = eval_df['relevant_doc_ids'].astype(str)\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "previous_run_df = get_clean_eval_dataset()\n",
    "eval_df = get_clean_eval_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41096d8d-ad21-49ed-8883-5419d17f02f6",
   "metadata": {},
   "source": [
    "## Cross-Encoders in Natural Language Processing\n",
    "\n",
    "Cross-encoders are powerful neural network models used for comparing two pieces of text, such as a query and a document. Unlike bi-encoders which encode texts separately, cross-encoders process both texts simultaneously, allowing for rich interactions between them at every layer of the network. This approach often yields higher accuracy in tasks like passage reranking, as it can capture complex relationships between the query and potential answers. However, cross-encoders are computationally intensive and less efficient for large-scale retrieval tasks. They're typically used to rerank a small set of candidates initially retrieved by faster methods, striking a balance between accuracy and efficiency in information retrieval systems.\n",
    "\n",
    "Because we only have 5 candidates, we can get through the ReRank pretty fast!\n",
    "\n",
    "## CrossEncoder Reranking with Long Passages\n",
    "This implementation addresses the challenge of reranking long text passages (2056 tokens) using a CrossEncoder model with a 512 token limit. The `CrossEncoderReRankTask` class includes a `chunk_text` method that splits long passages into smaller, overlapping chunks. During reranking, each chunk is scored separately against the query. The final score for a passage is determined by taking the maximum score across all its chunks. This approach allows the reranker to consider the entire content of long passages while respecting the model's token limit, potentially improving the accuracy of the reranking process for documents that exceed the standard BERT (what this cross encoder is based off) model's sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47b221-ec47-4214-bec4-c2bc673607ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder as SentenceTransformerCrossEncoder\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Passage(BaseModel):\n",
    "    chunk: str\n",
    "    file_name: str\n",
    "    score: float = 0.0\n",
    "\n",
    "class BaseReRankTask(ABC):\n",
    "    @abstractmethod\n",
    "    def rerank(self, query_text: str, passages: List[Passage]) -> List[Passage]:\n",
    "        pass\n",
    "\n",
    "class CrossEncoderReRankTask(BaseReRankTask):\n",
    "    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-12-v2', score_threshold: float = -0.999, max_length: int = 512):\n",
    "        self.cross_encoder = SentenceTransformerCrossEncoder(model_name)\n",
    "        self.score_threshold = score_threshold\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def chunk_text(self, text: str, max_length: int) -> List[str]:\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for word in words:\n",
    "            if current_length + len(word) + 1 > max_length:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "                current_length = len(word)\n",
    "            else:\n",
    "                current_chunk.append(word)\n",
    "                current_length += len(word) + 1\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def rerank(self, query: str, passages: List[Passage]) -> List[Passage]:\n",
    "        all_input_pairs = []\n",
    "        chunk_map = {}\n",
    "\n",
    "        for i, passage in enumerate(passages):\n",
    "            chunks = self.chunk_text(passage.chunk, self.max_length)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                all_input_pairs.append([query, chunk])\n",
    "                chunk_map[(i, j)] = chunk\n",
    "\n",
    "        # Get scores from the cross-encoder\n",
    "        scores = self.cross_encoder.predict(all_input_pairs)\n",
    "\n",
    "        # Aggregate scores for each original passage\n",
    "        passage_scores = {}\n",
    "        for (i, j), score in zip(chunk_map.keys(), scores):\n",
    "            if i not in passage_scores:\n",
    "                passage_scores[i] = []\n",
    "            passage_scores[i].append(score)\n",
    "\n",
    "        # Calculate final score for each passage (e.g., using max score)\n",
    "        final_scores = {i: max(scores) for i, scores in passage_scores.items()}\n",
    "\n",
    "        # Sort passages based on their scores in descending order\n",
    "        sorted_passages = sorted([(score, passages[i]) for i, score in final_scores.items()], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Update passage scores and return\n",
    "        result = []\n",
    "        for score, passage in sorted_passages:\n",
    "            passage.score = float(score)\n",
    "            result.append(passage)\n",
    "\n",
    "        return result\n",
    "\n",
    "# Define the ReRank task\n",
    "reranker: BaseReRankTask = CrossEncoderReRankTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b64aef-1c62-4dcd-86bb-8a1ac6c4bb79",
   "metadata": {},
   "source": [
    "# Copy IRMetrics Calculator\n",
    "This class is copied from the previous notebook. We included it here vs. pushing this to a utility class for ease of use when modifying the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca7c8e-6e77-4ca3-9a5a-32cace5f40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class IRMetricsCalculator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / k if k > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def recall_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def dcg_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        dcg = 0\n",
    "        for i, item in enumerate(retrieved_k):\n",
    "            if item in relevant:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        return dcg\n",
    "\n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevant, retrieved, k):\n",
    "        dcg = IRMetricsCalculator.dcg_at_k(relevant, retrieved, k)\n",
    "        idcg = IRMetricsCalculator.dcg_at_k(relevant, relevant, k)\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_json_list(json_string):\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {json_string} with error {e}\")\n",
    "            return []\n",
    "\n",
    "    def calculate_metrics(self, k_values=[1, 3, 5]):\n",
    "        for k in k_values:\n",
    "            self.df[f'precision@{k}'] = self.df.apply(lambda row: self.precision_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'recall@{k}'] = self.df.apply(lambda row: self.recall_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'ndcg@{k}'] = self.df.apply(lambda row: self.ndcg_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db5084-a57f-4a87-a464-7d47fb6d8481",
   "metadata": {},
   "source": [
    "# Setup Task Runner\n",
    "Similiar to what we did with the VectorDB search, we'll setup a task runner to iterate through our validation dataset to recalculate ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35a30b-7e53-4e51-97e1-303ab41fea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "class ReRankTaskRunner:\n",
    "    def __init__(self, eval_df: pd.DataFrame, reranker: BaseReRankTask):\n",
    "        self.eval_df = eval_df\n",
    "        self.reranker = reranker\n",
    "\n",
    "    def _get_unique_file_paths(self, results: List[Passage]) -> List[str]:\n",
    "        # Since Python 3.7, dicts retain insertion order.\n",
    "        return list(dict.fromkeys(r.file_name for r in results))\n",
    "\n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        # Make a copy of the dataframe so we don't modify the original.\n",
    "        df = pd.DataFrame(self.eval_df)\n",
    "        \n",
    "        results = []\n",
    "        for index, row in df.iterrows():\n",
    "            query: str = row['query_text']\n",
    "            \n",
    "            # Run retrieval task\n",
    "            chunks: dict = json.loads(row['retrieved_chunks'])\n",
    "            passages: List[Passage] = [Passage(chunk=chunk['chunk'], file_name=chunk['relative_path']) for chunk in chunks]\n",
    "\n",
    "            reranked_passages = self.reranker.rerank(query, passages)\n",
    "            \n",
    "            # Extract unique page numbers for comparison with validation dataset.\n",
    "            ordered_filepaths: List[str] = self._get_unique_file_paths(reranked_passages)\n",
    "\n",
    "            # retrieved_chunks = [ {'relative_path': r.metadata['relative_path'], 'chunk': r.document} for r in retrieval_results ]\n",
    "\n",
    "            # Create new record\n",
    "            result = {\n",
    "                'query_text': query,\n",
    "                'relevant_doc_ids': row['relevant_doc_ids'],\n",
    "                'retrieved_doc_ids': json.dumps(ordered_filepaths),\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        new_dataframe = pd.DataFrame(results)\n",
    "\n",
    "        ir_calc: IRMetricsCalculator = IRMetricsCalculator(new_dataframe)\n",
    "        return ir_calc.calculate_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f12063-d0ae-45ef-a976-8e2477566f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the validation\n",
    "reranked_results_df = ReRankTaskRunner(eval_df, reranker).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01963e-8044-4c38-b2d9-ff2b63568051",
   "metadata": {},
   "source": [
    "# Compare Original Ranking with New Ranking\n",
    "We'll copy the Experiment summarizer from the previous notebook as well and compare the results of the ReRanked results to the previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd26ef2-7f33-4da5-b708-7a129c681ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class ExperimentSummarizer:\n",
    "    def __init__(self, df):\n",
    "        self.df = pd.DataFrame(df)\n",
    "        self.summary_df = None\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ap(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        relevant_count = 0\n",
    "        total_precision = 0\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                relevant_count += 1\n",
    "                total_precision += relevant_count / i\n",
    "        \n",
    "        return total_precision / len(relevant_set) if relevant_set else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_reciprocal_rank(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                return 1 / i\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def calculate_map(self):\n",
    "        self.df['AP'] = self.df.apply(lambda row: self.calculate_ap(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['AP'].mean()\n",
    "\n",
    "    def calculate_mrr(self):\n",
    "        self.df['RR'] = self.df.apply(lambda row: self.calculate_reciprocal_rank(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['RR'].mean()\n",
    "\n",
    "    def calculate_mean_metrics(self):\n",
    "        return self.df[[\n",
    "            'precision@1', 'recall@1', 'ndcg@1',\n",
    "            'precision@3', 'recall@3', 'ndcg@3',\n",
    "            'precision@5', 'recall@5', 'ndcg@5'\n",
    "        ]].mean()\n",
    "\n",
    "    def calculate_top_k_percentages(self):\n",
    "        top_1 = (self.df['precision@1'] > 0).mean() * 100\n",
    "        top_3 = (self.df['precision@3'] > 0).mean() * 100\n",
    "        top_5 = (self.df['precision@5'] > 0).mean() * 100\n",
    "        return top_1, top_3, top_5\n",
    "\n",
    "    def analyze(self):\n",
    "        map_score = self.calculate_map()\n",
    "        mrr_score = self.calculate_mrr()\n",
    "        mean_metrics = self.calculate_mean_metrics()\n",
    "        top_1, top_3, top_5 = self.calculate_top_k_percentages()\n",
    "\n",
    "        self.summary_df = pd.DataFrame({\n",
    "            'Metric': [\n",
    "                'MAP (Mean Average Precision)',\n",
    "                'MRR (Mean Reciprocal Rank)',\n",
    "                'Mean Precision@1', 'Mean Recall@1', 'Mean NDCG@1',\n",
    "                'Mean Precision@3', 'Mean Recall@3', 'Mean NDCG@3',\n",
    "                'Mean Precision@5', 'Mean Recall@5', 'Mean NDCG@5',\n",
    "                '% Queries with Relevant Doc in Top 1',\n",
    "                '% Queries with Relevant Doc in Top 3',\n",
    "                '% Queries with Relevant Doc in Top 5'\n",
    "            ],\n",
    "            'Value': [\n",
    "                map_score,\n",
    "                mrr_score,\n",
    "                mean_metrics['precision@1'], mean_metrics['recall@1'], mean_metrics['ndcg@1'],\n",
    "                mean_metrics['precision@3'], mean_metrics['recall@3'], mean_metrics['ndcg@3'],\n",
    "                mean_metrics['precision@5'], mean_metrics['recall@5'], mean_metrics['ndcg@5'],\n",
    "                top_1, top_3, top_5\n",
    "            ]\n",
    "        })\n",
    "        return self.summary_df\n",
    "\n",
    "    def get_summary(self):\n",
    "        if self.summary_df is None:\n",
    "            self.analyze()\n",
    "        return self.summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4035cb-5706-46fb-8b41-5018f9122b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary of the reranked results\n",
    "# Lets use the class above to create aggregate metrics to see how well the system performs.\n",
    "original_summary = ExperimentSummarizer(eval_df).analyze()\n",
    "rerank_summary = ExperimentSummarizer(reranked_results_df).analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df0a33-3694-4700-a5e9-402e74677a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentComparator:\n",
    "    def __init__(self, *experiment_data):\n",
    "        self.experiments = experiment_data\n",
    "\n",
    "    def compare_metrics(self):\n",
    "        merged_df = pd.DataFrame({'Metric': self.experiments[0][0]['Metric']})\n",
    "        for df, name in self.experiments:\n",
    "            merged_df = pd.merge(merged_df, df, on='Metric', how='left')\n",
    "            merged_df = merged_df.rename(columns={'Value': name})\n",
    "        \n",
    "        base_exp = self.experiments[0][1]\n",
    "        for df, name in self.experiments[1:]:\n",
    "            merged_df[f'Change_{name}_vs_{base_exp}'] = merged_df[name] - merged_df[base_exp]\n",
    "            merged_df[f'PercentChange_{name}_vs_{base_exp}'] = ((merged_df[name] - merged_df[base_exp]) / merged_df[base_exp]) * 100\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "    def print_comparison(self):\n",
    "        comparison = self.compare_metrics()\n",
    "        \n",
    "        def color_change(val):\n",
    "            if pd.isna(val):\n",
    "                return ''\n",
    "            return 'color: red' if val < 0 else 'color: green' if val > 0 else ''\n",
    "        \n",
    "        def background_color_change(val):\n",
    "            if pd.isna(val):\n",
    "                return ''\n",
    "            return 'background-color: #ffcccb' if val < 0 else 'background-color: #90ee90' if val > 0 else ''\n",
    "        \n",
    "        change_columns = [col for col in comparison.columns if col.startswith('Change_') or col.startswith('PercentChange_')]\n",
    "        styled = comparison.style\n",
    "        \n",
    "        for col in change_columns:\n",
    "            styled = styled.map(color_change, subset=[col])\n",
    "            styled = styled.map(background_color_change, subset=[col])\n",
    "        \n",
    "        numeric_columns = comparison.select_dtypes(include=[np.number]).columns\n",
    "        format_dict = {col: '{:.6f}' for col in numeric_columns}\n",
    "        \n",
    "        for col in change_columns:\n",
    "            if col.startswith('PercentChange_'):\n",
    "                format_dict[col] = '{:.2f}%'\n",
    "        \n",
    "        styled = styled.format(format_dict)\n",
    "        return styled\n",
    "\n",
    "    def analyze(self):\n",
    "        return self.print_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabd334-fdb6-4dd0-a740-e36af2113980",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_comparator = ExperimentComparator(\n",
    "    (original_summary, \"Original\"),\n",
    "    (rerank_summary, \"ReRanked\")\n",
    ")\n",
    "experiment_comparator.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069c6c3-b5e0-42ff-b4bc-b425b44b41e0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As you can see, the ReRanker improved our MAP by 80%, MRR by 50%, and improved our precision metrics! It was also able to successfully improve our Precision@3. This is important. Because we don't want to put 5 documents into our RAG solution, precision@k, MAP, and MRR are more important in this step than Recall.\n",
    "\n",
    "# Next Steps\n",
    "For now we'll skip validating the entire IR system. The validation dataset for this ReRanker came straight out of the first notebook's outputs so we effectively evaluated the IR system already. \n",
    "\n",
    "Move to the next notebook to start getting into LLM Validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
