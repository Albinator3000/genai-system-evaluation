{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52226f6b-2660-4851-818e-efecea8766c9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this notebook is to demonstrate how to evaluate a rerank model. In the previous notebook, we saw that as we increased k where k is the number of chunks returned, we got better recall@k scores. This is fairly intuitive, but also poses another problem. If we increase K too much, won't that just mean higher input token costs and a greater chance that the LLM glosses the answer since it's receiving too much info?  \n",
    "\n",
    "This is one of the key benefits of adding a rerank model into your IR system. If we can narrow down the list of possible context chunks to < 10, we can add a separate model to output a relevance score. We can remove inputs that are below a threshold and use these scores to \"rerank\" the outputs so that the model gets only the most relevant outputs from our vector search. \n",
    "\n",
    "That's exactly what we'll do in this notebook. We'll incorporate a ReRank model using Bedrock and validate whether it's improving core metrics.\n",
    "\n",
    "# Background\n",
    "When evaluating a rerank model, it's crucial to focus on metrics that reflect both the quality and relevance of the reranked results, as well as the model's ability to improve upon the initial ranking. Normalized Discounted Cumulative Gain (NDCG) is often considered one of the most important metrics, as it accounts for the position of relevant items in the ranked list and can handle graded relevance judgments. Mean Average Precision (MAP) is another valuable metric that provides a single figure of merit for the overall ranking quality across multiple queries. For scenarios where the top results are particularly important, metrics like Precision@k and Mean Reciprocal Rank (MRR) can offer insights into the model's performance at specific cut-off points.\n",
    " \n",
    "In addition to these standard information retrieval metrics, it's beneficial to consider comparative metrics that directly measure the improvement over the base ranking. This can include the percentage of queries improved, the average change in relevant document positions, or a paired statistical test (such as a Wilcoxon signed-rank test) comparing the reranked results to the original ranking. It's also important to evaluate the model's efficiency, considering factors like inference time and computational resources required, especially for applications with strict latency requirements. Ultimately, the choice of metrics should align with the specific goals of your reranking task and the priorities of your system, balancing between relevance, user satisfaction, and operational constraints.\n",
    "\n",
    "How to Evaluate\n",
    "To evaluate a rerank model using these metrics, start by preparing a test set consisting of queries, their corresponding initial rankings, and human-annotated relevance judgments for each query-document pair. Run your rerank model on the initial rankings to produce a new set of reranked results. Then, calculate the chosen metrics for both the initial and reranked results. For example, to compute NDCG@k, sort the documents for each query by their relevance scores, calculate the Discounted Cumulative Gain (DCG) for the top k results, and normalize it by the Ideal DCG. For MAP, calculate the average precision for each query at every position where a relevant document is retrieved, then take the mean across all queries. To assess improvement, compare the metric scores between the initial and reranked results. You can use statistical tests like paired t-tests or Wilcoxon signed-rank tests to determine if the improvements are significant. It's also valuable to analyze per-query performance to identify where the rerank model excels or struggles. Finally, consider evaluating on different subsets of your data to ensure consistent performance across various query types or document categories.\n",
    "\n",
    "\n",
    "\n",
    "# What Will We Do? \n",
    "* We'll the best run from our previous model, run it through a ReRank model, and then recalculate the results.\n",
    "\n",
    "**Lets get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65b599-7663-4785-bd75-0f139216fa17",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c04eab6-7444-45ae-8ce8-f731398c51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25742ac5-32d5-4961-9772-288e66c4f007",
   "metadata": {},
   "source": [
    "# Get Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73d0514-977c-403f-84cb-612305010b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_clean_eval_dataset():\n",
    "    EVAL_PATH = '../data/eval-datasets/1_rerank_validation.csv'\n",
    "    eval_df = pd.read_csv(EVAL_PATH)\n",
    "\n",
    "    # Clean up the DataFrame\n",
    "    eval_df = eval_df.rename(columns=lambda x: x.strip())  # Remove any leading/trailing whitespace from column names\n",
    "    eval_df = eval_df.drop(columns=[col for col in eval_df.columns if col.startswith('Unnamed')])  # Remove unnamed columns\n",
    "    eval_df = eval_df.dropna(how='all')  # Remove rows that are all NaN\n",
    "    \n",
    "    # Strip whitespace from string columns\n",
    "    for col in eval_df.select_dtypes(['object']):\n",
    "        eval_df[col] = eval_df[col].str.strip()\n",
    "    \n",
    "    # Ensure 'relevant_doc_ids' is a string column\n",
    "    eval_df['relevant_doc_ids'] = eval_df['relevant_doc_ids'].astype(str)\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "previous_run_df = get_clean_eval_dataset()\n",
    "eval_df = get_clean_eval_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41096d8d-ad21-49ed-8883-5419d17f02f6",
   "metadata": {},
   "source": [
    "# Setup Cross Encoder For ReRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c66bdb-36ce-4fb2-bbb5-5d78782ed770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huthmac/.pyenv/versions/3.11.7/envs/llm-system-eval/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/huthmac/.pyenv/versions/3.11.7/envs/llm-system-eval/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder as SentenceTransformerCrossEncoder\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Passage(BaseModel):\n",
    "    chunk: str\n",
    "    file_name: str\n",
    "    score: float = 0.0\n",
    "\n",
    "\n",
    "class BaseReRankTask(ABC):\n",
    "    @abstractmethod\n",
    "    def rerank(self, query_text: str, passages: List[Passage]) -> List[Passage]:\n",
    "        \"\"\"\n",
    "        Retrieve documents based on the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string to search for.\n",
    "\n",
    "        Returns:\n",
    "            List[RetrievalResult]: A list of RetrievalResult objects that are relevant to the query.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class CrossEncoderReRankTask(BaseReRankTask):\n",
    "    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-12-v2', score_threshold: float = -0.999):\n",
    "        self.cross_encoder = SentenceTransformerCrossEncoder(model_name)\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "    def rerank(self, query: str, passages: List[Passage]) -> List[Passage]:\n",
    "        # Prepare input pairs for the cross-encoder\n",
    "        input_pairs = [[query, passage.chunk] for passage in passages]\n",
    "\n",
    "        # Get scores from the cross-encoder\n",
    "        scores = self.cross_encoder.predict(input_pairs)\n",
    "\n",
    "        # Create a list of (score, passage) tuples and filter based on threshold\n",
    "        scored_passages = [(float(score), passage) for score, passage in zip(scores, passages)]\n",
    "\n",
    "        # Sort the passages based on their scores in descending order\n",
    "        sorted_passages = sorted(scored_passages, key=lambda x: x[0], reverse=False)\n",
    "\n",
    "        # Update passage scores and return\n",
    "        result = []\n",
    "        for score, passage in sorted_passages:\n",
    "            passage.score = score\n",
    "            result.append(passage)\n",
    "\n",
    "        return result\n",
    "# Define the ReRank task. By default we use ms-mini-marco from the the HuggingFace Sentence Transformer Library\n",
    "reranker: BaseReRankTask =  CrossEncoderReRankTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ca7c8e-6e77-4ca3-9a5a-32cace5f40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class IRMetricsCalculator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / k if k > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def recall_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def dcg_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        dcg = 0\n",
    "        for i, item in enumerate(retrieved_k):\n",
    "            if item in relevant:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        return dcg\n",
    "\n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevant, retrieved, k):\n",
    "        dcg = IRMetricsCalculator.dcg_at_k(relevant, retrieved, k)\n",
    "        idcg = IRMetricsCalculator.dcg_at_k(relevant, relevant, k)\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_json_list(json_string):\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {json_string} with error {e}\")\n",
    "            return []\n",
    "\n",
    "    def calculate_metrics(self, k_values=[1, 3, 5]):\n",
    "        for k in k_values:\n",
    "            self.df[f'precision@{k}'] = self.df.apply(lambda row: self.precision_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'recall@{k}'] = self.df.apply(lambda row: self.recall_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'ndcg@{k}'] = self.df.apply(lambda row: self.ndcg_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f35a30b-7e53-4e51-97e1-303ab41fea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "class ReRankTaskRunner:\n",
    "    def __init__(self, eval_df: pd.DataFrame, reranker: BaseReRankTask):\n",
    "        self.eval_df = eval_df\n",
    "        self.reranker = reranker\n",
    "\n",
    "    def _get_unique_file_paths(self, results: List[Passage]) -> List[str]:\n",
    "        # Since Python 3.7, dicts retain insertion order.\n",
    "        return list(dict.fromkeys(r.file_name for r in results))\n",
    "\n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        # Make a copy of the dataframe so we don't modify the original.\n",
    "        df = pd.DataFrame(self.eval_df)\n",
    "        \n",
    "        results = []\n",
    "        for index, row in df.iterrows():\n",
    "            query: str = row['query_text']\n",
    "            \n",
    "            # Run retrieval task\n",
    "            chunks: dict = json.loads(row['retrieved_chunks'])\n",
    "            passages: List[Passage] = [Passage(chunk=chunk['chunk'], file_name=chunk['relative_path']) for chunk in chunks]\n",
    "\n",
    "            reranked_passages = self.reranker.rerank(query, passages)\n",
    "            \n",
    "            # Extract unique page numbers for comparison with validation dataset.\n",
    "            ordered_filepaths: List[str] = self._get_unique_file_paths(reranked_passages)\n",
    "\n",
    "            # retrieved_chunks = [ {'relative_path': r.metadata['relative_path'], 'chunk': r.document} for r in retrieval_results ]\n",
    "\n",
    "            # Create new record\n",
    "            result = {\n",
    "                'query_text': query,\n",
    "                'relevant_doc_ids': row['relevant_doc_ids'],\n",
    "                'retrieved_doc_ids': json.dumps(ordered_filepaths),\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        new_dataframe = pd.DataFrame(results)\n",
    "\n",
    "        ir_calc: IRMetricsCalculator = IRMetricsCalculator(new_dataframe)\n",
    "        return ir_calc.calculate_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f12063-d0ae-45ef-a976-8e2477566f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_text           I'm using version 2.1 of open search and tryin...\n",
       "relevant_doc_ids     [\"_im-plugin/index-codecs.md\", \"_tuning-your-c...\n",
       "retrieved_doc_ids    [\"_benchmark/quickstart.md\", \"_upgrade-to/upgr...\n",
       "precision@1                                                        0.0\n",
       "recall@1                                                           0.0\n",
       "ndcg@1                                                             0.0\n",
       "precision@3                                                        0.0\n",
       "recall@3                                                           0.0\n",
       "ndcg@3                                                             0.0\n",
       "precision@5                                                        0.0\n",
       "recall@5                                                           0.0\n",
       "ndcg@5                                                             0.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the validation\n",
    "reranked_results_df = ReRankTaskRunner(eval_df, reranker).run()\n",
    "reranked_results_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01963e-8044-4c38-b2d9-ff2b63568051",
   "metadata": {},
   "source": [
    "# Compare Original Ranking with New Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd26ef2-7f33-4da5-b708-7a129c681ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class ExperimentSummarizer:\n",
    "    def __init__(self, df):\n",
    "        self.df = pd.DataFrame(df)\n",
    "        self.summary_df = None\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ap(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        relevant_count = 0\n",
    "        total_precision = 0\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                relevant_count += 1\n",
    "                total_precision += relevant_count / i\n",
    "        \n",
    "        return total_precision / len(relevant_set) if relevant_set else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_reciprocal_rank(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                return 1 / i\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def calculate_map(self):\n",
    "        self.df['AP'] = self.df.apply(lambda row: self.calculate_ap(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['AP'].mean()\n",
    "\n",
    "    def calculate_mrr(self):\n",
    "        self.df['RR'] = self.df.apply(lambda row: self.calculate_reciprocal_rank(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['RR'].mean()\n",
    "\n",
    "    def calculate_mean_metrics(self):\n",
    "        return self.df[[\n",
    "            'precision@1', 'recall@1', 'ndcg@1',\n",
    "            'precision@3', 'recall@3', 'ndcg@3',\n",
    "            'precision@5', 'recall@5', 'ndcg@5'\n",
    "        ]].mean()\n",
    "\n",
    "    def calculate_top_k_percentages(self):\n",
    "        top_1 = (self.df['precision@1'] > 0).mean() * 100\n",
    "        top_3 = (self.df['precision@3'] > 0).mean() * 100\n",
    "        top_5 = (self.df['precision@5'] > 0).mean() * 100\n",
    "        return top_1, top_3, top_5\n",
    "\n",
    "    def analyze(self):\n",
    "        map_score = self.calculate_map()\n",
    "        mrr_score = self.calculate_mrr()\n",
    "        mean_metrics = self.calculate_mean_metrics()\n",
    "        top_1, top_3, top_5 = self.calculate_top_k_percentages()\n",
    "\n",
    "        self.summary_df = pd.DataFrame({\n",
    "            'Metric': [\n",
    "                'MAP (Mean Average Precision)',\n",
    "                'MRR (Mean Reciprocal Rank)',\n",
    "                'Mean Precision@1', 'Mean Recall@1', 'Mean NDCG@1',\n",
    "                'Mean Precision@3', 'Mean Recall@3', 'Mean NDCG@3',\n",
    "                'Mean Precision@5', 'Mean Recall@5', 'Mean NDCG@5',\n",
    "                '% Queries with Relevant Doc in Top 1',\n",
    "                '% Queries with Relevant Doc in Top 3',\n",
    "                '% Queries with Relevant Doc in Top 5'\n",
    "            ],\n",
    "            'Value': [\n",
    "                map_score,\n",
    "                mrr_score,\n",
    "                mean_metrics['precision@1'], mean_metrics['recall@1'], mean_metrics['ndcg@1'],\n",
    "                mean_metrics['precision@3'], mean_metrics['recall@3'], mean_metrics['ndcg@3'],\n",
    "                mean_metrics['precision@5'], mean_metrics['recall@5'], mean_metrics['ndcg@5'],\n",
    "                top_1, top_3, top_5\n",
    "            ]\n",
    "        })\n",
    "        return self.summary_df\n",
    "\n",
    "    def get_summary(self):\n",
    "        if self.summary_df is None:\n",
    "            self.analyze()\n",
    "        return self.summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4035cb-5706-46fb-8b41-5018f9122b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary of the reranked results\n",
    "# Lets use the class above to create aggregate metrics to see how well the system performs.\n",
    "original_summary = ExperimentSummarizer(eval_df).analyze()\n",
    "rerank_summary = ExperimentSummarizer(reranked_results_df).analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6df0a33-3694-4700-a5e9-402e74677a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentComparator:\n",
    "    def __init__(self, *experiment_data):\n",
    "        self.experiments = experiment_data\n",
    "\n",
    "    def compare_metrics(self):\n",
    "        merged_df = pd.DataFrame({'Metric': self.experiments[0][0]['Metric']})\n",
    "        for df, name in self.experiments:\n",
    "            merged_df = pd.merge(merged_df, df, on='Metric', how='left')\n",
    "            merged_df = merged_df.rename(columns={'Value': name})\n",
    "        \n",
    "        base_exp = self.experiments[0][1]\n",
    "        for df, name in self.experiments[1:]:\n",
    "            merged_df[f'Change_{name}_vs_{base_exp}'] = merged_df[name] - merged_df[base_exp]\n",
    "            merged_df[f'PercentChange_{name}_vs_{base_exp}'] = ((merged_df[name] - merged_df[base_exp]) / merged_df[base_exp]) * 100\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "    def print_comparison(self):\n",
    "        comparison = self.compare_metrics()\n",
    "        \n",
    "        def color_change(val):\n",
    "            if pd.isna(val):\n",
    "                return ''\n",
    "            return 'color: red' if val < 0 else 'color: green' if val > 0 else ''\n",
    "        \n",
    "        def background_color_change(val):\n",
    "            if pd.isna(val):\n",
    "                return ''\n",
    "            return 'background-color: #ffcccb' if val < 0 else 'background-color: #90ee90' if val > 0 else ''\n",
    "        \n",
    "        change_columns = [col for col in comparison.columns if col.startswith('Change_') or col.startswith('PercentChange_')]\n",
    "        styled = comparison.style\n",
    "        \n",
    "        for col in change_columns:\n",
    "            styled = styled.map(color_change, subset=[col])\n",
    "            styled = styled.map(background_color_change, subset=[col])\n",
    "        \n",
    "        numeric_columns = comparison.select_dtypes(include=[np.number]).columns\n",
    "        format_dict = {col: '{:.6f}' for col in numeric_columns}\n",
    "        \n",
    "        for col in change_columns:\n",
    "            if col.startswith('PercentChange_'):\n",
    "                format_dict[col] = '{:.2f}%'\n",
    "        \n",
    "        styled = styled.format(format_dict)\n",
    "        return styled\n",
    "\n",
    "    def analyze(self):\n",
    "        return self.print_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efabd334-fdb6-4dd0-a740-e36af2113980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2c2e7_row0_col3, #T_2c2e7_row0_col4, #T_2c2e7_row1_col3, #T_2c2e7_row1_col4 {\n",
       "  color: green;\n",
       "  background-color: #90ee90;\n",
       "}\n",
       "#T_2c2e7_row2_col3, #T_2c2e7_row2_col4, #T_2c2e7_row3_col3, #T_2c2e7_row3_col4, #T_2c2e7_row4_col3, #T_2c2e7_row4_col4, #T_2c2e7_row5_col3, #T_2c2e7_row5_col4, #T_2c2e7_row6_col3, #T_2c2e7_row6_col4, #T_2c2e7_row7_col3, #T_2c2e7_row7_col4, #T_2c2e7_row10_col3, #T_2c2e7_row10_col4, #T_2c2e7_row11_col3, #T_2c2e7_row11_col4, #T_2c2e7_row12_col3, #T_2c2e7_row12_col4 {\n",
       "  color: red;\n",
       "  background-color: #ffcccb;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2c2e7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2c2e7_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_2c2e7_level0_col1\" class=\"col_heading level0 col1\" >Original</th>\n",
       "      <th id=\"T_2c2e7_level0_col2\" class=\"col_heading level0 col2\" >ReRanked</th>\n",
       "      <th id=\"T_2c2e7_level0_col3\" class=\"col_heading level0 col3\" >Change_ReRanked_vs_Original</th>\n",
       "      <th id=\"T_2c2e7_level0_col4\" class=\"col_heading level0 col4\" >PercentChange_ReRanked_vs_Original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2c2e7_row0_col0\" class=\"data row0 col0\" >MAP (Mean Average Precision)</td>\n",
       "      <td id=\"T_2c2e7_row0_col1\" class=\"data row0 col1\" >0.056713</td>\n",
       "      <td id=\"T_2c2e7_row0_col2\" class=\"data row0 col2\" >0.086806</td>\n",
       "      <td id=\"T_2c2e7_row0_col3\" class=\"data row0 col3\" >0.030093</td>\n",
       "      <td id=\"T_2c2e7_row0_col4\" class=\"data row0 col4\" >53.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2c2e7_row1_col0\" class=\"data row1 col0\" >MRR (Mean Reciprocal Rank)</td>\n",
       "      <td id=\"T_2c2e7_row1_col1\" class=\"data row1 col1\" >0.097222</td>\n",
       "      <td id=\"T_2c2e7_row1_col2\" class=\"data row1 col2\" >0.166667</td>\n",
       "      <td id=\"T_2c2e7_row1_col3\" class=\"data row1 col3\" >0.069444</td>\n",
       "      <td id=\"T_2c2e7_row1_col4\" class=\"data row1 col4\" >71.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2c2e7_row2_col0\" class=\"data row2 col0\" >Mean Precision@1</td>\n",
       "      <td id=\"T_2c2e7_row2_col1\" class=\"data row2 col1\" >0.500000</td>\n",
       "      <td id=\"T_2c2e7_row2_col2\" class=\"data row2 col2\" >0.375000</td>\n",
       "      <td id=\"T_2c2e7_row2_col3\" class=\"data row2 col3\" >-0.125000</td>\n",
       "      <td id=\"T_2c2e7_row2_col4\" class=\"data row2 col4\" >-25.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2c2e7_row3_col0\" class=\"data row3 col0\" >Mean Recall@1</td>\n",
       "      <td id=\"T_2c2e7_row3_col1\" class=\"data row3 col1\" >0.420139</td>\n",
       "      <td id=\"T_2c2e7_row3_col2\" class=\"data row3 col2\" >0.267361</td>\n",
       "      <td id=\"T_2c2e7_row3_col3\" class=\"data row3 col3\" >-0.152778</td>\n",
       "      <td id=\"T_2c2e7_row3_col4\" class=\"data row3 col4\" >-36.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_2c2e7_row4_col0\" class=\"data row4 col0\" >Mean NDCG@1</td>\n",
       "      <td id=\"T_2c2e7_row4_col1\" class=\"data row4 col1\" >0.500000</td>\n",
       "      <td id=\"T_2c2e7_row4_col2\" class=\"data row4 col2\" >0.375000</td>\n",
       "      <td id=\"T_2c2e7_row4_col3\" class=\"data row4 col3\" >-0.125000</td>\n",
       "      <td id=\"T_2c2e7_row4_col4\" class=\"data row4 col4\" >-25.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_2c2e7_row5_col0\" class=\"data row5 col0\" >Mean Precision@3</td>\n",
       "      <td id=\"T_2c2e7_row5_col1\" class=\"data row5 col1\" >0.291667</td>\n",
       "      <td id=\"T_2c2e7_row5_col2\" class=\"data row5 col2\" >0.236111</td>\n",
       "      <td id=\"T_2c2e7_row5_col3\" class=\"data row5 col3\" >-0.055556</td>\n",
       "      <td id=\"T_2c2e7_row5_col4\" class=\"data row5 col4\" >-19.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_2c2e7_row6_col0\" class=\"data row6 col0\" >Mean Recall@3</td>\n",
       "      <td id=\"T_2c2e7_row6_col1\" class=\"data row6 col1\" >0.649306</td>\n",
       "      <td id=\"T_2c2e7_row6_col2\" class=\"data row6 col2\" >0.531250</td>\n",
       "      <td id=\"T_2c2e7_row6_col3\" class=\"data row6 col3\" >-0.118056</td>\n",
       "      <td id=\"T_2c2e7_row6_col4\" class=\"data row6 col4\" >-18.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_2c2e7_row7_col0\" class=\"data row7 col0\" >Mean NDCG@3</td>\n",
       "      <td id=\"T_2c2e7_row7_col1\" class=\"data row7 col1\" >0.584511</td>\n",
       "      <td id=\"T_2c2e7_row7_col2\" class=\"data row7 col2\" >0.455088</td>\n",
       "      <td id=\"T_2c2e7_row7_col3\" class=\"data row7 col3\" >-0.129423</td>\n",
       "      <td id=\"T_2c2e7_row7_col4\" class=\"data row7 col4\" >-22.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_2c2e7_row8_col0\" class=\"data row8 col0\" >Mean Precision@5</td>\n",
       "      <td id=\"T_2c2e7_row8_col1\" class=\"data row8 col1\" >0.183333</td>\n",
       "      <td id=\"T_2c2e7_row8_col2\" class=\"data row8 col2\" >0.183333</td>\n",
       "      <td id=\"T_2c2e7_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "      <td id=\"T_2c2e7_row8_col4\" class=\"data row8 col4\" >0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_2c2e7_row9_col0\" class=\"data row9 col0\" >Mean Recall@5</td>\n",
       "      <td id=\"T_2c2e7_row9_col1\" class=\"data row9 col1\" >0.670139</td>\n",
       "      <td id=\"T_2c2e7_row9_col2\" class=\"data row9 col2\" >0.670139</td>\n",
       "      <td id=\"T_2c2e7_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "      <td id=\"T_2c2e7_row9_col4\" class=\"data row9 col4\" >0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_2c2e7_row10_col0\" class=\"data row10 col0\" >Mean NDCG@5</td>\n",
       "      <td id=\"T_2c2e7_row10_col1\" class=\"data row10 col1\" >0.592227</td>\n",
       "      <td id=\"T_2c2e7_row10_col2\" class=\"data row10 col2\" >0.513608</td>\n",
       "      <td id=\"T_2c2e7_row10_col3\" class=\"data row10 col3\" >-0.078618</td>\n",
       "      <td id=\"T_2c2e7_row10_col4\" class=\"data row10 col4\" >-13.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_2c2e7_row11_col0\" class=\"data row11 col0\" >% Queries with Relevant Doc in Top 1</td>\n",
       "      <td id=\"T_2c2e7_row11_col1\" class=\"data row11 col1\" >50.000000</td>\n",
       "      <td id=\"T_2c2e7_row11_col2\" class=\"data row11 col2\" >37.500000</td>\n",
       "      <td id=\"T_2c2e7_row11_col3\" class=\"data row11 col3\" >-12.500000</td>\n",
       "      <td id=\"T_2c2e7_row11_col4\" class=\"data row11 col4\" >-25.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_2c2e7_row12_col0\" class=\"data row12 col0\" >% Queries with Relevant Doc in Top 3</td>\n",
       "      <td id=\"T_2c2e7_row12_col1\" class=\"data row12 col1\" >75.000000</td>\n",
       "      <td id=\"T_2c2e7_row12_col2\" class=\"data row12 col2\" >66.666667</td>\n",
       "      <td id=\"T_2c2e7_row12_col3\" class=\"data row12 col3\" >-8.333333</td>\n",
       "      <td id=\"T_2c2e7_row12_col4\" class=\"data row12 col4\" >-11.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c2e7_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_2c2e7_row13_col0\" class=\"data row13 col0\" >% Queries with Relevant Doc in Top 5</td>\n",
       "      <td id=\"T_2c2e7_row13_col1\" class=\"data row13 col1\" >79.166667</td>\n",
       "      <td id=\"T_2c2e7_row13_col2\" class=\"data row13 col2\" >79.166667</td>\n",
       "      <td id=\"T_2c2e7_row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "      <td id=\"T_2c2e7_row13_col4\" class=\"data row13 col4\" >0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1044c0e90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_comparator = ExperimentComparator(\n",
    "    (original_summary, \"Original\"),\n",
    "    (rerank_summary, \"ReRanked\")\n",
    ")\n",
    "experiment_comparator.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069c6c3-b5e0-42ff-b4bc-b425b44b41e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f14b1e-b99d-468f-a574-5b7b44251eac",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "Even though re-rank is considered best practice for IR tasks, we can use our validation dataset to prove that in this use case, we don't actually want to include re-rank. As we expand the vector database to increase the variety of topics, we may decide that we need it at a future date. For now, we'll leave it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e7d9b-d9b1-4b4e-8558-525881992753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
