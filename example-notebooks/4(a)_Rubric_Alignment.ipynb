{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f977c758-fa54-40a3-b4ce-98918f576087",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this notebook is to demonstrate how to align an LLM-As-A-Judge Prompt to human preference. It's time intensive and not scalable to use human annotators for every change. To get around this constraint, we can use an LLM to judge our own answers. We do this by using a grading rubric (similar to grading rubrics in school). You define your sucess criteria and ask the model to evaluate the results. \n",
    "\n",
    "\n",
    "#### How do we trust an LLM to grade the answers correctly? \n",
    "To use LLM-As-A-Judge, you have to iterate on the evaluation prompt until the human annotations generally agree with the LLMs grades. An evaluation dataset should be created and graded by a human. That same dataset is run through an LLM using a grading rubric. If the responses align then the evaluation prompt is ready to be used. If not, you need to iterate on the prompt until the humans and LLM agrees. \n",
    "\n",
    "\n",
    "#### What Metrics Should I Care About.\n",
    "In this notebook, we'll be generating a score from 0-5 based on our grading rubric. Because of that, the metrics we care about are exact matches, scores within one point of each other, and typical metrics for numeric answers like MSE and RMSE.\n",
    "\n",
    "\n",
    "# What Will We Do? \n",
    "* Curate a dataset of gold standard answers and human grades to a list of questions (we've created one already)\n",
    "* Create a rubric to define how we want to grade our LLM responses\n",
    "* Invoke our model using our validation dataset to generate answers\n",
    "* Generate metrics to understand how well our LLM-As-A-Judge rubric matches human grades\n",
    "\n",
    "At this end of this notebook, we should have a grading rubric that aligns to our expectations allowing us to trust this rubric moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7584a-0171-4d52-9a8d-c711bd5e40bb",
   "metadata": {},
   "source": [
    "# Load Validation Dataset\n",
    "This dataset contains the query, context, sample llm response, human grade, and the human thinking. We will use this to then test out our rubric to see how it compares to the human grades. \n",
    "\n",
    "**Note:** These initial grades were performed by Claude. However, we went through each one and updated it based our own \"preference\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b73458-471e-4073-857d-d2322144fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('../data/eval-datasets/4(a)_rubric_alignment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8f97f-be1b-4193-b4a2-8cce039bca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the evaluation dataset\n",
    "eval_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a922a52-c338-4138-9406-b5d9cc74ea09",
   "metadata": {},
   "source": [
    "# Define Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23daf859-249c-48fb-8c47-3565ac9968a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "RUBRIC_SYSTEM_PROMPT = \"\"\"You are an expert judge evaluating Retrieval Augmented Generation (RAG) applications.\n",
    "Your task is to evaluate given answers based on context and questions using the criteria provided.\n",
    "Evaluation Criteria (Score either 0 or 1 for each, total score is the sum):\n",
    "1. Context Utilization: Does the answer use only information provided in the context, without introducing external or fabricated details?\n",
    "2. Completeness: Does the answer thoroughly address all key elements of the question based on the available context, without significant omissions?\n",
    "3. Conciseness: Does the answer efficiently use words to address the question and avoid unnecessary redundancy?\n",
    "4. Accuracy: Is the answer factually correct based on the given context?\n",
    "5. Clarity: Is the answer easy to understand and follow?\n",
    "Your role is to provide a fair and thorough evaluation for each criterion, explaining your reasoning clearly.\"\"\"\n",
    "\n",
    "# User Prompt\n",
    "RUBRIC_USER_PROMPT = \"\"\"Please evaluate the following RAG response:\n",
    "\n",
    "Question:\n",
    "<query_text>\n",
    "{query_text}\n",
    "</query_text>\n",
    "\n",
    "Generated answer:\n",
    "<llm_response>\n",
    "{llm_response}\n",
    "</llm_response>\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Evaluation Steps:\n",
    "1. Carefully read the provided context, question, and answer.\n",
    "2. For each evaluation criterion, assign a score of either 0 or 1:\n",
    "   - Context Utilization\n",
    "   - Completeness\n",
    "   - Conciseness\n",
    "   - Accuracy\n",
    "   - Clarity\n",
    "3. Provide a clear explanation for each score, referencing specific aspects of the response.\n",
    "4. Calculate the total score by adding up the points awarded (minimum 0, maximum 5).\n",
    "5. Present your evaluation inside <thinking></thinking> tags.\n",
    "6. Include individual criterion scores (0 or 1) in the thinking tags and the total score inside <score></score> tags.\n",
    "7. Ensure your response is valid XML and provides a comprehensive evaluation.\n",
    "\n",
    "Example Output Format:\n",
    "<thinking>\n",
    "Context Utilization: 1 - The answer strictly uses information from the context without introducing external details.\n",
    "Completeness: 1 - The response covers all key elements of the question based on the available context.\n",
    "Conciseness: 1 - The answer is helpful and doesn't repeat the same information more than once.\n",
    "Accuracy: 1 - All stated facts align perfectly with the provided context.\n",
    "Clarity: 1 - The response is clear and easy to follow.\n",
    "</thinking>\n",
    "<score>4</score>\n",
    "\n",
    "Please provide your detailed evaluation.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd873671-bac7-4eb1-bbc3-4e1ebbfea837",
   "metadata": {},
   "source": [
    "## Bedrock Helpers\n",
    "Below is a helper class that makes it easier for us to call bedrock in a threaded way to speed this portion up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a8cb1-2e79-46df-b56a-37ddf1681a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class GradingRubricClient:\n",
    "    \n",
    "    def __init__(self, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        self.client = boto3.client('bedrock-runtime')\n",
    "        self.user_prompt = user_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model_id = model_id\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "    def create_chat_payload(self, inputs: dict) -> list[dict]:\n",
    "        user_prompt = self.user_prompt.format(**inputs)\n",
    "        user_msg = {\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}\n",
    "        return [user_msg]\n",
    "\n",
    "    def call(self, messages: list[dict]) -> str:\n",
    "        response = self.client.converse(\n",
    "            modelId=self.model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=self.hyper_params,\n",
    "            system=[{\n",
    "                \"text\": self.system_prompt\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        return response['output']['message']['content'][0]['text']\n",
    "\n",
    "    def call_threaded(self, message_lists: List[List[Dict[str, Any]]]) -> List[str]:\n",
    "        '''\n",
    "        This is a bit funky. We're dumping all the requests into a thread pool\n",
    "        and storing the index for the order in which they were submitted. \n",
    "        Lastly, we're inserting them into the response array at their index to ensure order.\n",
    "        '''\n",
    "        future_to_position = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for i, request in enumerate(message_lists):\n",
    "                future = executor.submit(self.call, request)\n",
    "                future_to_position[future] = i\n",
    "            \n",
    "            responses = [None] * len(message_lists)\n",
    "            for future in as_completed(future_to_position):\n",
    "                position = future_to_position[future]\n",
    "                try:\n",
    "                    response: str = future.result()\n",
    "                    responses[position] = response\n",
    "                except Exception as exc:\n",
    "                    print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                    responses[position] = None\n",
    "            \n",
    "        return responses\n",
    "\n",
    "    def extract_score_and_thinking(self, llm_output: str) -> tuple:\n",
    "        thinking_match = re.search(r'<thinking>(.*?)</thinking>', llm_output, re.DOTALL)\n",
    "        score_match = re.search(r'<score>(.*?)</score>', llm_output, re.DOTALL)\n",
    "\n",
    "        thinking = thinking_match.group(1).strip() if thinking_match else \"No thinking found\"\n",
    "        score = float(score_match.group(1)) if score_match else None\n",
    "\n",
    "        return score, thinking\n",
    "\n",
    "    def grade_examples(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result_df = df.copy()\n",
    "\n",
    "        message_lists = [self.create_chat_payload({\n",
    "            \"query_text\": row[\"query_text\"],\n",
    "            \"context\": row[\"context\"],\n",
    "            \"llm_response\": row[\"llm_response\"]\n",
    "        }) for _, row in df.iterrows()]\n",
    "\n",
    "        responses = self.call_threaded(message_lists)\n",
    "\n",
    "        llm_scores = []\n",
    "        llm_thinking = []\n",
    "\n",
    "        for response in responses:\n",
    "            if response is not None:\n",
    "                score, thinking = self.extract_score_and_thinking(response)\n",
    "                llm_scores.append(score)\n",
    "                llm_thinking.append(thinking)\n",
    "            else:\n",
    "                llm_scores.append(None)\n",
    "                llm_thinking.append(\"Error occurred during processing\")\n",
    "\n",
    "        result_df['llm_grade'] = llm_scores\n",
    "        result_df['llm_reasoning'] = llm_thinking\n",
    "\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2d8f3-dddc-4003-b071-766af45b4c27",
   "metadata": {},
   "source": [
    "# Run Validation\n",
    "In this section we will run our LLM as judge prompt and compare the scores to the human annotated scores. If all goes well, our rubric should match pretty closely to our human currated scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748195cd-3144-46a4-94b3-002581b7d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different models we can use to evaluate. \n",
    "\n",
    "SONNET_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "HAIKU_ID = \"anthropic.claude-3-haiku-20240307-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5bb223-9f97-4f53-bc13-169cd1735d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our grading client\n",
    "HYPER_PARAMS = {\"temperature\": 0.3, \"maxTokens\": 4096}\n",
    "\n",
    "grading_client: GradingRubricClient = GradingRubricClient(\n",
    "    RUBRIC_USER_PROMPT,\n",
    "    RUBRIC_SYSTEM_PROMPT,\n",
    "    HAIKU_ID,\n",
    "    HYPER_PARAMS\n",
    ")\n",
    "\n",
    "experiment_1_df = grading_client.grade_examples(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb838e3a-20dc-4abf-8532-a68920ca4d4c",
   "metadata": {},
   "source": [
    "# Generate A Report\n",
    "Going through these 1 by 1 is painful. We only have ~25 examples, but if you have 100+ it's not a small ask. Lets write a helper class that can generate a report for us. \n",
    "\n",
    "# Understanding Rubric Validation Metrics\n",
    "\n",
    "When comparing LLM-generated grades to human-assigned grades, several metrics help us quantify the alignment and accuracy of the AI model:\n",
    "\n",
    "1. **Mean Squared Error (MSE) & Root Mean Squared Error (RMSE)**:\n",
    "   - These metrics measure the average squared difference between the LLM and human grades. They penalize larger errors more heavily, giving us insight into the magnitude of discrepancies.\n",
    "   - Interpretation: Lower is better. For a 5-point scale:\n",
    "     - Excellent: MSE < 0.25 (RMSE < 0.5)\n",
    "     - Good: 0.25 ≤ MSE < 1 (0.5 ≤ RMSE < 1)\n",
    "     - Fair: 1 ≤ MSE < 2.25 (1 ≤ RMSE < 1.5)\n",
    "     - Poor: MSE ≥ 2.25 (RMSE ≥ 1.5)\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**:\n",
    "   - This represents the average absolute difference between LLM and human grades, providing a straightforward measure of error magnitude.\n",
    "   - Interpretation: Lower is better. For a 5-point scale:\n",
    "     - Excellent: MAE < 0.5\n",
    "     - Good: 0.5 ≤ MAE < 1\n",
    "     - Fair: 1 ≤ MAE < 1.5\n",
    "     - Poor: MAE ≥ 1.5\n",
    "\n",
    "3. **R-squared (R²)**:\n",
    "   - This metric indicates how well the LLM grades explain the variation in human grades. A higher R² suggests better alignment between the two.\n",
    "   - Interpretation: Higher is better.\n",
    "     - Excellent: R² > 0.9\n",
    "     - Good: 0.7 < R² ≤ 0.9\n",
    "     - Fair: 0.5 < R² ≤ 0.7\n",
    "     - Poor: R² ≤ 0.5\n",
    "\n",
    "4. **Pearson & Spearman Correlations**:\n",
    "   - These measure the strength and direction of the relationship between LLM and human grades. High positive correlations indicate strong agreement in ranking and relative scoring.\n",
    "   - Interpretation: Closer to 1 is better.\n",
    "     - Excellent: > 0.9\n",
    "     - Good: 0.7 - 0.9\n",
    "     - Fair: 0.5 - 0.7\n",
    "     - Poor: < 0.5\n",
    "\n",
    "5. **Exact Match Ratio**:\n",
    "   - This shows the proportion of cases where the LLM grade exactly matches the human grade, giving us a clear measure of perfect agreement.\n",
    "   - Interpretation: Higher is better.\n",
    "     - Excellent: > 0.7\n",
    "     - Good: 0.5 - 0.7\n",
    "     - Fair: 0.3 - 0.5\n",
    "     - Poor: < 0.3\n",
    "\n",
    "6. **Within 1 Point Ratio**:\n",
    "   - This metric tells us how often the LLM grade is within one point of the human grade, allowing for slight disagreements that may not be practically significant.\n",
    "   - Interpretation: Higher is better.\n",
    "     - Excellent: > 0.9\n",
    "     - Good: 0.8 - 0.9\n",
    "     - Fair: 0.7 - 0.8\n",
    "     - Poor: < 0.7\n",
    "\n",
    "By analyzing these metrics together, we can gain a comprehensive understanding of how well the LLM rubric aligns with human grading patterns. This information is crucial for assessing the reliability and potential biases of the AI grading system, and for identifying areas where the model may need improvement or human oversight.\n",
    "\n",
    "Note: The interpretation guidelines provided are general and may need to be adjusted based on the specific context of your grading system, the importance of precision in your application, and the inherent variability in human grading for your particular use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756afca-91ce-45ec-9ed6-72cb8f8d18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "class RubricValidator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.human_scores = self.convert_to_float(df['human_grade'])\n",
    "        self.llm_scores = self.convert_to_float(df['llm_grade'])\n",
    "\n",
    "    def convert_to_float(self, series):\n",
    "        return series.astype(float)\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        mse = mean_squared_error(self.human_scores, self.llm_scores)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(self.human_scores, self.llm_scores)\n",
    "        r2 = r2_score(self.human_scores, self.llm_scores)\n",
    "        \n",
    "        # Handle constant input for correlations. This can sometimes happen when the LLM hands out all 5.0s for example. \n",
    "        # In this case, pearsonr or spearmanr can't be computted because all the inputs are the same (i.e. constant)\n",
    "        if len(set(self.human_scores)) == 1 or len(set(self.llm_scores)) == 1:\n",
    "            pearson_corr = spearman_corr = \"N/A (constant input)\"\n",
    "        else:\n",
    "            pearson_corr, _ = pearsonr(self.human_scores, self.llm_scores)\n",
    "            spearman_corr, _ = spearmanr(self.human_scores, self.llm_scores)\n",
    "        \n",
    "        exact_match = np.mean(self.human_scores == self.llm_scores)\n",
    "        within_1_point = np.mean(np.abs(self.human_scores - self.llm_scores) <= 1)\n",
    "\n",
    "        return {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R-squared': r2,\n",
    "            'Pearson Correlation': pearson_corr,\n",
    "            'Spearman Correlation': spearman_corr,\n",
    "            'Exact Match Ratio': exact_match,\n",
    "            'Within 1 Point Ratio': within_1_point\n",
    "        }\n",
    "\n",
    "    def generate_report(self):\n",
    "        metrics = self.calculate_metrics()\n",
    "        report = \"LLM Rubric Validation Report\\n\"\n",
    "        report += \"===========================\\n\\n\"\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, str):\n",
    "                report += f\"{metric}: {value}\\n\"\n",
    "            else:\n",
    "                report += f\"{metric}: {value:.4f}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def analyze_discrepancies(self):\n",
    "        discrepancies = self.df[self.human_scores != self.llm_scores]\n",
    "        return discrepancies[['query_text', 'human_grade', 'llm_grade', 'human_reasoning', 'llm_reasoning']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a0b43-f26b-44e7-af06-71072e75b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Report\n",
    "validator = RubricValidator(experiment_1_df)\n",
    "print(validator.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5049c16-c040-4e4a-bd9a-1dd215808364",
   "metadata": {},
   "source": [
    "# Experiment 1 Summary\n",
    "LLMs are non-deterministic so it's possible your results vary. In our iterations, this first experiment isn't up to par. Our RMSE and pearson correlation are important, but we care about most is getting most of the answers within 1 point of each other. In our run, we got 70%. It's extremely difficult to align your rubric so that it's a 100% match. If you have more than 1 human grading results, their opinions vary and you rarely get a perfect human graded validation set. If we can get to 90%+, we'll be happy as a starting place.\n",
    "\n",
    "We encourage you to open up the dataframe and see where the descrepancies are. \n",
    "\n",
    "The takeway **should** be that our grading rubric is a little too forgiving. Before we adjust the prompt though, lets see if Sonnet gives us better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b7a0e-da9f-4d7f-8b4e-94f98ca7acd9",
   "metadata": {},
   "source": [
    "# Experiment 2 - Use Sonnet\n",
    "In this experiment, we'll use the same hyper parameters but just use sonnet instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa676f-bb03-48df-b7e2-7022d794c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our grading client this time with haiku\n",
    "grading_client: GradingRubricClient = GradingRubricClient(\n",
    "    RUBRIC_USER_PROMPT,\n",
    "    RUBRIC_SYSTEM_PROMPT,\n",
    "    SONNET_ID,\n",
    "    HYPER_PARAMS\n",
    ")\n",
    "\n",
    "experiment_2_df = grading_client.grade_examples(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfa1b2-1dc1-4d67-9db0-148bbbf81b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Report\n",
    "validator = RubricValidator(experiment_2_df)\n",
    "print(validator.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55767a4-d86f-42c9-9532-d9b658ea13d4",
   "metadata": {},
   "source": [
    "# Experiment 2 Summary\n",
    "In our experiment 2 run, the results are even worse. It turns out the Sonnet really likes its own answers :). If you want, you can open up the experiment_2_df to see what's happening. Sonnet is grading every answers as a 5. Not only is this not what we want, it also makes calculating the Perason and Spearman Correlations impossible. It requires variations in the grading outputs. If it's all 5's, it's considered constant.\n",
    "\n",
    "This is a good time to point out that LLMs in general prefer AI generated responses. We'll discuss more about how to overcome that in the conclusion\n",
    "\n",
    "Now let's adjust our prompt and let the model know that it can be a little tougher. We can also use Haiku for this because there's no indication that Sonnet perform better. It's generally better to pick the smallest/cheapest model that gives you the performance you're looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751988b5-bb5e-409a-8e88-e1b148452050",
   "metadata": {},
   "source": [
    "# Experiment 3 - Grade more difficult\n",
    "In this next step, we're going to grade the results a little harder. We'll also bump up the temperature a bit to give it more wiggle room to be creative. We'll also modify the user prompt by append a sentence to it telling the model to grade harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c7202-0767-4d94-9acf-209126347581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the space is needed since the last sentence ends with .\n",
    "HARDER_SUFFIX = ''' Be very tough with the grades, especially on the Conciseness and Clarity. \n",
    "\n",
    "If it's not very concise, give it a 0. \n",
    "If the response contains information not in the context, give it a 0.\n",
    "Assume you are not a technical person when giving out the grade.\n",
    "\n",
    "You should rarely give out a 5. There's no free handouts!'''\n",
    "\n",
    "\n",
    "HARDER_USER_PROMPT = RUBRIC_USER_PROMPT + HARDER_SUFFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca80c6-a9f8-4598-9b60-b7a9c16d3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our grading client\n",
    "HYPER_PARAMS = {\"temperature\": 0.6, \"maxTokens\": 4096}\n",
    "\n",
    "grading_client: GradingRubricClient = GradingRubricClient(\n",
    "    HARDER_USER_PROMPT,\n",
    "    RUBRIC_SYSTEM_PROMPT,\n",
    "    HAIKU_ID,\n",
    "    HYPER_PARAMS\n",
    ")\n",
    "\n",
    "experiment_3_df = grading_client.grade_examples(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b2275-3a0a-4679-80d6-0e7a2b48b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Report\n",
    "validator = RubricValidator(experiment_3_df)\n",
    "print(validator.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783f421-1935-45f0-87c1-6e2bb332014e",
   "metadata": {},
   "source": [
    "# Experiment 3 - Results\n",
    "We got a much lower error, higher Pearson & spearman correlation and we got 92% of the scores within one point. For the purpose of this example, that's acceptable. It's okay if your results don't look the same. We turned the temperature up and it's not expected to get the same results every time. LLM-As-A-Judge isn't a magic bullet. For this notebook, we care more that the majority of grades are within 1 point of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cea973-6127-4ec6-a64d-88ebce72f07d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we did our best to align our LLM to our own preference. It turns out, we needed our rubric to grade our results much harder. You can continue to tinker with the prompt, hyper params, and model to maximize the metrics that make sense for you.\n",
    "\n",
    "One interesting takeaway from LLM-As-A-Judge noted above is that models tend to favor their own AI generated content. In this case, Sonnet really liked it's own answers while Haiku aligned better to our human grades. This is why it's important to test these things out with a validation dataset.\n",
    "\n",
    "You can overcome this preference by providing human examples in your prompt. As you adjust this over time, it would be valuable to build up a repository of human currated example grades and pass them into the model as few shot examples.\n",
    "\n",
    "# Next Steps\n",
    "Move to the next notebook so we can evaluate our LLM Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d4a14-892c-4088-939f-deab35327c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
