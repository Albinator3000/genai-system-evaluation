{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae33cc75-66e7-4378-bd45-9d7951416e70",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook builds off of the previous notebook where we played with our LLM-As-A-Judge prompt to align it to our own grading preferences. In this notebook, instead of trying to align our scores like 4(a), we're trying to maximize our scores!\n",
    "\n",
    "#### What Metrics Should I Care About.\n",
    "This is a bit simpler than previous notebooks. We're essentially trying to get all 5s from our grading rubric. The higher the average score, the better we like our prompt. The validation dataset already contains our relevant context and we aren't focused on information retrieval.\n",
    "\n",
    "\n",
    "# What Will We Do?\n",
    "* Curate a dataset of questions and relevant context (we've created one already)\n",
    "* Reuse our grading rubric from the previous notebook\n",
    "* Invoke our model using our validation dataset to generate answers\n",
    "* Generate scores to understand how well our prompt works.\n",
    "\n",
    "At this end of this notebook, we should have a prompt that works very well for the types of answers we're looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e79c4-48e7-4ddf-a9f2-fa72dfd786be",
   "metadata": {},
   "source": [
    "# Load Validation Dataset\n",
    "This dataset contains the query and relevant context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c62e1-4f7e-4fae-b35a-955d4e33be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('../data/eval-datasets/4(b)_prompt_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea937a48-939f-4851-9454-3d9c4cb66be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541e519-6620-42af-995d-d1edba003c40",
   "metadata": {},
   "source": [
    "# Evaluation Helper Classes\n",
    "Because we're reusing a lot of the same code for calling bedrock within the RAG portion and the evaluation portion, it makes sense to push that to a base class and use inheritance to reuse code. The following functions allow us to perform the RAG and then perform the validation in a 2 step process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217aa50-3f5f-49c1-aa57-a1c3b6b24326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class BaseBedrockClient:\n",
    "    def __init__(self, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        self.client = boto3.client('bedrock-runtime')\n",
    "        self.user_prompt = user_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model_id = model_id\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "    def create_chat_payload(self, inputs: dict) -> list[dict]:\n",
    "        prompt = self.user_prompt.format(**inputs)\n",
    "        return [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    def call(self, messages: list[dict]) -> str:\n",
    "        response = self.client.converse(\n",
    "            modelId=self.model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=self.hyper_params,\n",
    "            system=[{\"text\": self.system_prompt}]\n",
    "        )\n",
    "        return response['output']['message']['content'][0]['text']\n",
    "\n",
    "    def call_threaded(self, message_lists: List[List[Dict[str, Any]]]) -> List[str]:\n",
    "        future_to_position = {}\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for i, request in enumerate(message_lists):\n",
    "                future = executor.submit(self.call, request)\n",
    "                future_to_position[future] = i\n",
    "            \n",
    "            responses = [None] * len(message_lists)\n",
    "            for future in as_completed(future_to_position):\n",
    "                position = future_to_position[future]\n",
    "                try:\n",
    "                    response: str = future.result()\n",
    "                    responses[position] = response\n",
    "                except Exception as exc:\n",
    "                    print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                    responses[position] = None\n",
    "        return responses\n",
    "\n",
    "class RAGClient(BaseBedrockClient):\n",
    "    def __init__(self, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        super().__init__(user_prompt, system_prompt, model_id, hyper_params)\n",
    "\n",
    "    def extract_response(self, llm_output: str) -> str:\n",
    "        response_match = re.search(r'<response>(.*?)</response>', llm_output, re.DOTALL)\n",
    "        return response_match.group(1).strip() if response_match else \"No response found\"\n",
    "\n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        message_lists = [self.create_chat_payload({\n",
    "            \"query_text\": row[\"query_text\"],\n",
    "            \"context\": row[\"context\"]\n",
    "        }) for _, row in df.iterrows()]\n",
    "        \n",
    "        responses = self.call_threaded(message_lists)\n",
    "        df['llm_response'] = [self.extract_response(r) for r in responses]\n",
    "        return df\n",
    "\n",
    "class EvaluationClient(BaseBedrockClient):\n",
    "    def __init__(self, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        super().__init__(user_prompt, system_prompt, model_id, hyper_params)\n",
    "\n",
    "    def extract_score_and_thinking(self, llm_output: str) -> tuple:\n",
    "        thinking_match = re.search(r'<thinking>(.*?)</thinking>', llm_output, re.DOTALL)\n",
    "        score_match = re.search(r'<score>(.*?)</score>', llm_output, re.DOTALL)\n",
    "\n",
    "        thinking = thinking_match.group(1).strip() if thinking_match else \"No thinking found\"\n",
    "        score = float(score_match.group(1)) if score_match else None\n",
    "        \n",
    "        return score, thinking\n",
    "\n",
    "    def evaluate(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        message_lists = [self.create_chat_payload({\n",
    "            \"query_text\": row[\"query_text\"],\n",
    "            \"context\": row[\"context\"],\n",
    "            \"llm_response\": row[\"llm_response\"]\n",
    "        }) for _, row in df.iterrows()]\n",
    "        \n",
    "        responses = self.call_threaded(message_lists)\n",
    "\n",
    "        llm_scores = []\n",
    "        llm_thinking = []\n",
    "\n",
    "        for response in responses:\n",
    "            if response is not None:\n",
    "                score, thinking = self.extract_score_and_thinking(response)\n",
    "                llm_scores.append(score)\n",
    "                llm_thinking.append(thinking)\n",
    "            else:\n",
    "                llm_scores.append(None)\n",
    "                llm_thinking.append(\"Error occurred during processing\")\n",
    "\n",
    "        df['grade'] = llm_scores\n",
    "        df['reasoning'] = llm_thinking\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d678756-20e8-48f9-9377-93121f45bb46",
   "metadata": {},
   "source": [
    "# Define RAG Prompt\n",
    "Before we evaluate anything, we need to construct a prompt that can take in context and generate answers. This example below is purposefully not good. It's here to set a baseline for which we can improve on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d15a4f-14a5-49ed-b200-79b138216a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an advanced AI assistant specialized in Retrieval Augmented Generation (RAG).\n",
    "Your primary function is to provide accurate, concise, and relevant answers based solely on the given context.\n",
    "Follow these guidelines strictly:\n",
    "\n",
    "1. Use only information from the provided context. Do not introduce external knowledge or make assumptions.\n",
    "2. Ensure your answers are complete, addressing all aspects of the question using available information.\n",
    "3. Be extremely concise. Use as few words as possible while maintaining clarity and completeness.\n",
    "4. Maintain 100% accuracy based on the given context. If the context doesn't contain enough information to answer fully, state this clearly.\n",
    "5. Structure your responses for maximum clarity. Use bullet points or numbered lists when appropriate.\n",
    "6. If the context contains technical information, explain it in simple terms as if speaking to a non-technical person.\n",
    "7. Do not apologize or use phrases like \"Based on the context provided\" or \"According to the information given\".\n",
    "8. If asked about something not in the context, simply state \"The provided context does not contain information about [topic].\"\n",
    "\n",
    "Your goal is to achieve the highest possible score on context utilization, completeness, conciseness, accuracy, and clarity.\"\"\"\n",
    "\n",
    "# User Prompt\n",
    "RAG_USER_PROMPT = \"\"\"Answer the following question using only the provided context:\n",
    "\n",
    "<query>\n",
    "{query_text}\n",
    "</query>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Instructions:\n",
    "1. Read the question and context carefully.\n",
    "2. Formulate a concise and accurate answer based solely on the given context.\n",
    "3. Ensure your response is clear and easily understandable to a non-technical person.\n",
    "4. Do not include any information not present in the context.\n",
    "5. If the context doesn't contain relevant information, state this clearly and concisely.\n",
    "6. Place your response in <response></response> tags.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b7676-f259-49b0-aa6b-4e0f62af85cf",
   "metadata": {},
   "source": [
    "# Reuse Rubric\n",
    "We will reuse the rubric from our previous notebook that gave us the most consistant scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea75f5-8f03-4b93-8a58-3c523c0ef0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "RUBRIC_SYSTEM_PROMPT = \"\"\"You are an expert judge evaluating Retrieval Augmented Generation (RAG) applications.\n",
    "Your task is to evaluate given answers based on context and questions using the criteria provided.\n",
    "Evaluation Criteria (Score either 0 or 1 for each, total score is the sum):\n",
    "1. Context Utilization: Does the answer use only information provided in the context, without introducing external or fabricated details?\n",
    "2. Completeness: Does the answer thoroughly address all key elements of the question based on the available context, without significant omissions?\n",
    "3. Conciseness: Does the answer efficiently use words to address the question and avoid unnecessary redundancy?\n",
    "4. Accuracy: Is the answer factually correct based on the given context?\n",
    "5. Clarity: Is the answer easy to understand and follow?\n",
    "Your role is to provide a fair and thorough evaluation for each criterion, explaining your reasoning clearly.\"\"\"\n",
    "\n",
    "# User Prompt\n",
    "RUBRIC_USER_PROMPT = \"\"\"Please evaluate the following RAG response:\n",
    "\n",
    "Question:\n",
    "<query_text>\n",
    "{query_text}\n",
    "</query_text>\n",
    "\n",
    "Generated answer:\n",
    "<llm_response>\n",
    "{llm_response}\n",
    "</llm_response>\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Evaluation Steps:\n",
    "1. Carefully read the provided context, question, and answer.\n",
    "2. For each evaluation criterion, assign a score of either 0 or 1:\n",
    "   - Context Utilization\n",
    "   - Completeness\n",
    "   - Conciseness\n",
    "   - Accuracy\n",
    "   - Clarity\n",
    "3. Provide a clear explanation for each score, referencing specific aspects of the response.\n",
    "4. Calculate the total score by adding up the points awarded (minimum 0, maximum 5).\n",
    "5. Present your evaluation inside <thinking></thinking> tags.\n",
    "6. Include individual criterion scores (0 or 1) in the thinking tags and the total score inside <score></score> tags.\n",
    "7. Ensure your response is valid XML and provides a comprehensive evaluation.\n",
    "\n",
    "Example Output Format:\n",
    "<thinking>\n",
    "Context Utilization: 1 - The answer strictly uses information from the context without introducing external details.\n",
    "Completeness: 1 - The response covers all key elements of the question based on the available context.\n",
    "Conciseness: 1 - The answer is helpful and doesn't repeat the same information more than once.\n",
    "Accuracy: 0 - The model introduced facts that were not present in the context.\n",
    "Clarity: 1 - The response is clear and easy to follow.\n",
    "</thinking>\n",
    "<score>4</score>\n",
    "\n",
    "Please provide your detailed evaluation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c53916-9566-470d-b750-ce811e9c4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different models we can use to evaluate. \n",
    "SONNET_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "HAIKU_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Initialize RAG Client\n",
    "rag_client = RAGClient(\n",
    "    RAG_USER_PROMPT, \n",
    "    RAG_SYSTEM_PROMPT, \n",
    "    HAIKU_ID, \n",
    "    {\"temperature\": 0.5, \"maxTokens\": 4096}\n",
    ")\n",
    "\n",
    "# Initialize Eval Client\n",
    "eval_client = EvaluationClient(\n",
    "    RUBRIC_USER_PROMPT, \n",
    "    RUBRIC_SYSTEM_PROMPT, \n",
    "    HAIKU_ID, \n",
    "    {\"temperature\": 0.7, \"maxTokens\": 4096}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9649e6d-033e-47f4-a2fe-9421b6a30a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RAG responses\n",
    "rag_df = rag_client.process(eval_df)\n",
    "\n",
    "# Evaluate RAG responses\n",
    "experiment_1_df = eval_client.evaluate(rag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb1143-bf3d-41d7-8c47-0cd3e5646c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textwrap import fill\n",
    "\n",
    "class PromptEvaluator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.grades = df['grade'].astype(float)\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        return {\n",
    "            'Mean': np.mean(self.grades),\n",
    "            'Median': np.median(self.grades),\n",
    "            'Standard Deviation': np.std(self.grades),\n",
    "            'Minimum Grade': np.min(self.grades),\n",
    "            'Maximum Grade': np.max(self.grades)\n",
    "        }\n",
    "    \n",
    "    def generate_report(self):\n",
    "        metrics = self.calculate_metrics()\n",
    "        report = \"Prompt Evaluation Report\\n\"\n",
    "        report += \"========================\\n\\n\"\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            report += f\"{metric}: {value:.2f}\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def analyze_grade_distribution(self):\n",
    "        return self.df['grade'].value_counts().sort_index()\n",
    "\n",
    "    def pretty_print_lowest_results(self, n=3, width=80):\n",
    "        lowest_results = self.df.nsmallest(n, 'grade')\n",
    "        for index, row in lowest_results.iterrows():\n",
    "            print(f\"{'='*width}\\n\")\n",
    "            print(f\"Grade: {row['grade']}\\n\")\n",
    "            print(\"Query Text:\")\n",
    "            print(fill(row['query_text'], width=width))\n",
    "            print(\"\\nLLM Response:\")\n",
    "            print(fill(row['llm_response'], width=width))\n",
    "            print(\"\\nReasoning:\")\n",
    "            print(fill(row['reasoning'], width=width))\n",
    "            print(f\"\\n{'='*width}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d188d0f-9dcc-4979-b68f-f2bcacc43f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named 'df'\n",
    "evaluator = PromptEvaluator(experiment_1_df)\n",
    "\n",
    "# Generate and print the report\n",
    "print(evaluator.generate_report())\n",
    "\n",
    "# Analyze grade distribution\n",
    "print(evaluator.analyze_grade_distribution())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf3e8f-b05f-4305-bc31-1fc398b5332d",
   "metadata": {},
   "source": [
    "# Experiment 1 Results\n",
    "\n",
    "Here was my results. Your results may vary. Remember, LLMs are non-deterministic. Your outputs might vary.\n",
    "```bash\n",
    "Mean: 5.00\n",
    "Median: 5.00\n",
    "Standard Deviation: 0.00\n",
    "Minimum Grade: 5.00\n",
    "Maximum Grade: 5.00\n",
    "```\n",
    "\n",
    "Based on my results, the model is performing extremely well. This is a relatively simple task and we can guarantee accurate context in this task, it makes sense we'd get 100% or close to it. Because we're getting the accuracy we care about, it doesn't make sense to iterate much further for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631d459-39ee-4808-9f53-bb1b2c9982cf",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we did a bit of prompt engineering with our RAG prompt to get the performance we were looking for in our models response. It's worth noting that a basic RAG example doesn't require much prompt tuning. However, when we go to evaluate the E2E system, we can feel confident that any issues that arise are not because of our prompt!\n",
    "\n",
    "# Next Steps\n",
    "Move to the final E2E Notebook to evaluate our entire system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
