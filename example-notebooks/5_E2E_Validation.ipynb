{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52226f6b-2660-4851-818e-efecea8766c9",
   "metadata": {},
   "source": [
    "# End-to-End Evaluation PLACEHOLDER\n",
    "\n",
    "TBD\n",
    "\n",
    "## What Will We Do? \n",
    "* Create E2E solution including Guardrails\n",
    "* Run LLM-As-A-Judge against E2E solution \n",
    "* Run Human-Eval on sampled LLM-As-A-Judge \n",
    "\n",
    "\n",
    "So let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65b599-7663-4785-bd75-0f139216fa17",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c04eab6-7444-45ae-8ce8-f731398c51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda create -y --name llm-system-eval python=3.11.7\n",
    "# !conda init && activate llm-system-eval\n",
    "# !conda install -n llm-system-eval ipykernel --update-deps --force-reinstall -y\n",
    "# OR\n",
    "# !pyenv virtualenv 3.11.7 llm-system-eval\n",
    "# !pyenv activate llm-system-eval\n",
    "\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c38224",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b58a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/huthmac/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huthmac/.pyenv/versions/3.11.7/envs/llm-system-eval/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:lambda:us-east-1:432418664414:function:PRE-PassThrough\n",
      "arn:aws:lambda:us-east-1:432418664414:function:ACS-PassThrough\n"
     ]
    }
   ],
   "source": [
    "# set variables\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import sagemaker\n",
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import json\n",
    "from botocore.config import Config\n",
    "from io import StringIO\n",
    "import time\n",
    "import typing as t\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sentence_transformers import CrossEncoder as SentenceTransformerCrossEncoder\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Tuple, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'llm-system-eval.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "REGION = os.environ['REGION']\n",
    "os.environ['WORKTEAM_ARN'] = os.getenv('WORKTEAM_ARN')\n",
    "WORKTEAM_ARN = os.environ['WORKTEAM_ARN']\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "os.environ['SAGEMAKER_ROLE_ARN'] = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "SAGEMAKER_ROLE_ARN = os.environ['SAGEMAKER_ROLE_ARN'] # OR sagemaker.get_execution_role()\n",
    "os.environ['GUARDRAIL_ID'] = os.getenv('GUARDRAIL_ID')\n",
    "GUARDRAIL_ID = os.environ['GUARDRAIL_ID']\n",
    "\n",
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"./chroma_db\"))\n",
    "\n",
    "# Also initialize the bedrock client so we can call some embedding models!\n",
    "session = boto3.Session(profile_name='default')\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "bedrock_client = boto3.client(service_name='bedrock', region_name=REGION)\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "role_name = SAGEMAKER_ROLE_ARN.split(\"/\")[-1]\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "# Specify ARNs for resources needed to run an text classification job.\n",
    "ac_arn_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\"\n",
    "}\n",
    "# PreHumanTaskLambdaArn for text classification(single)\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-PassThrough\".format(\n",
    "    REGION, ac_arn_map[REGION]\n",
    ")\n",
    "\n",
    "# AnnotationConsolidationConfig for text classification(single)\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-PassThrough\".format(REGION, ac_arn_map[REGION])\n",
    "\n",
    "print(prehuman_arn)\n",
    "print(acs_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135ba081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_BEDROCK_GUARDRAIL_ID: ajw9itww9krb\n"
     ]
    }
   ],
   "source": [
    "# Create Bedrock Guardrail\n",
    "\n",
    "import json, random, string\n",
    "\n",
    "response = bedrock_client.create_guardrail(\n",
    "    name=\"doc-opensearch-guardrail\" + \"\".join(random.choices(string.ascii_lowercase, k=8)),\n",
    "    description='string',\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {\n",
    "                \"name\": \"Discussing topics that are not related to Amazon OpenSearch\",\n",
    "                \"definition\": \"Offering advice that is not related to Amazon OpenSearch.\",\n",
    "                \"examples\": [\n",
    "                    \"What stock should I invest in?\",\n",
    "                    \"Can you diagnose my vision problems based on my symptoms?\",\n",
    "                    \"Can you tell what I should do to get promoted?\",\n",
    "                ],\n",
    "                \"type\": \"DENY\",\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Discussing pricing for Amazon OpenSearch\",\n",
    "                \"definition\": \"Offering pricing information.\",\n",
    "                \"examples\": [\n",
    "                    \"How much does OpenSearch cost?\",\n",
    "                    \"Is OpenSearch expensive?\",\n",
    "                    \"Is OpenSearch expensive?\",\n",
    "                ],\n",
    "                \"type\": \"DENY\",\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    contentPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {\"type\": \"SEXUAL\", \"inputStrength\": \"HIGH\", \"outputStrength\": \"HIGH\"},\n",
    "            {\"type\": \"HATE\", \"inputStrength\": \"HIGH\", \"outputStrength\": \"HIGH\"},\n",
    "            {\"type\": \"VIOLENCE\", \"inputStrength\": \"HIGH\", \"outputStrength\": \"HIGH\"},\n",
    "            {\"type\": \"INSULTS\", \"inputStrength\": \"HIGH\", \"outputStrength\": \"HIGH\"},\n",
    "            {\"type\": \"MISCONDUCT\", \"inputStrength\": \"HIGH\", \"outputStrength\": \"HIGH\"},\n",
    "            {\"type\": \"PROMPT_ATTACK\", \"inputStrength\": \"HIGH\", \"outputStrength\": \"NONE\"},\n",
    "        ]\n",
    "    },\n",
    "    wordPolicyConfig={\n",
    "        \"wordsConfig\": [{\"text\": \"AnyCompany\"}],\n",
    "        \"managedWordListsConfig\": [{\"type\": \"PROFANITY\"}],\n",
    "    },\n",
    "    sensitiveInformationPolicyConfig={\n",
    "        \"piiEntitiesConfig\": [\n",
    "            {\"type\": \"NAME\", \"action\": \"ANONYMIZE\"},\n",
    "            {\"type\": \"EMAIL\", \"action\": \"ANONYMIZE\"},\n",
    "            {\"type\": \"US_SOCIAL_SECURITY_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            \n",
    "        ],\n",
    "    },\n",
    "    # contextualGroundingPolicyConfig={\n",
    "    #     'filtersConfig': [\n",
    "    #         {\n",
    "    #             'type': 'GROUNDING', # &/ RELEVANCE\n",
    "    #             'threshold': 0.75\n",
    "    #         },\n",
    "    #     ]\n",
    "    # },\n",
    "    blockedInputMessaging=\"I apologize, but I'm not able to provide the information you're looking for. As an AI assistant, my role is to answer questions regarding Amazon OpenSearch.\",\n",
    "    blockedOutputsMessaging=\"I apologize, but I'm not able to provide the information you're looking for. As an AI assistant, my role is to answer questions regarding Amazon OpenSearch.\",\n",
    ")\n",
    "\n",
    "\n",
    "os.environ['GUARDRAIL_ID'] = response['guardrailId']\n",
    "GUARDRAIL_ID = os.environ['GUARDRAIL_ID']\n",
    "\n",
    "# Update environment variable\n",
    "set_key(local_env_filename, 'GUARDRAIL_ID', GUARDRAIL_ID)\n",
    "\n",
    "print(f'AWS_BEDROCK_GUARDRAIL_ID: {GUARDRAIL_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe740d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION: 1\n"
     ]
    }
   ],
   "source": [
    "# Create a guardrail version\n",
    "response = bedrock_client.create_guardrail_version(\n",
    "    guardrailIdentifier=GUARDRAIL_ID,\n",
    ")\n",
    "VERSION = response.get('version')\n",
    "\n",
    "# Update environment variable\n",
    "set_key(local_env_filename, 'GUARDRAIL_VERSION', VERSION)\n",
    "print(f'VERSION: {VERSION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278dfdfa",
   "metadata": {},
   "source": [
    "### RAG system definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c0ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG helper classes\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import sagemaker\n",
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import json\n",
    "from botocore.config import Config\n",
    "from io import StringIO\n",
    "import time\n",
    "import typing as t\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sentence_transformers import CrossEncoder as SentenceTransformerCrossEncoder\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Tuple, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Create a class to use instead of LlamaIndex Nodes. This way we decouple our chroma collections from LlamaIndexes\n",
    "class RAGChunk(BaseModel):\n",
    "    id_: str\n",
    "    text: str\n",
    "    metadata: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "class SentenceSplitterChunkingStrategy:\n",
    "    def __init__(self, input_dir: str, chunk_size: int = 256, chunk_overlap: int = 128):\n",
    "        self.input_dir = input_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "        # Helper to get regex pattern for normalizing relative file paths.\n",
    "        self.relative_path_pattern = rf\"{re.escape(input_dir)}(/.*)\"\n",
    "\n",
    "    def _extract_relative_path(self, full_path):\n",
    "        # Get Regex pattern\n",
    "        pattern = self.relative_path_pattern\n",
    "        match = re.search(pattern, full_path)\n",
    "        if match:\n",
    "            return match.group(1).lstrip('/')\n",
    "        return None\n",
    "\n",
    "    def _create_pipeline(self) -> IngestionPipeline:\n",
    "        transformations = [\n",
    "            SentenceSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap),\n",
    "        ]\n",
    "        return IngestionPipeline(transformations=transformations)\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        # If you're using a different type of file besides md, you'll want to change this. \n",
    "        return SimpleDirectoryReader(\n",
    "            input_dir=self.input_dir, \n",
    "            recursive=True,\n",
    "            required_exts=['.md']\n",
    "        ).load_data()\n",
    "\n",
    "    def to_ragchunks(self, nodes: List[Node]) -> List[RAGChunk]:\n",
    "        return [\n",
    "            RAGChunk(\n",
    "                id_=node.node_id,\n",
    "                text=node.text,\n",
    "                metadata={\n",
    "                    **node.metadata,\n",
    "                    'relative_path': self._extract_relative_path(node.metadata['file_path'])\n",
    "                }\n",
    "            )\n",
    "            for node in nodes\n",
    "        ]\n",
    "\n",
    "    def process(self) -> List[RAGChunk]:\n",
    "        documents = self.load_documents()\n",
    "        nodes = self.pipeline.run(documents=documents)\n",
    "        rag_chunks = self.to_ragchunks(nodes)\n",
    "        \n",
    "        print(f\"Processing complete. Created {len(rag_chunks)} chunks.\")\n",
    "        return rag_chunks\n",
    "    \n",
    "\n",
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    id: str\n",
    "    document: str\n",
    "    embedding: List[float]\n",
    "    distance: float\n",
    "    metadata: Dict = {}\n",
    "\n",
    "# Base retrieval class. Can be reused if you decide to implement a different retrieval class.\n",
    "class BaseRetrievalTask(ABC):\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query_text: str, n_results: int) -> List[RetrievalResult]:\n",
    "        \"\"\"\n",
    "        Retrieve documents based on the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string to search for.\n",
    "\n",
    "        Returns:\n",
    "            List[RetrievalResult]: A list of RetrievalResult objects that are relevant to the query.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Example of a concrete implementation\n",
    "class ChromaDBRetrievalTask(BaseRetrievalTask):\n",
    "\n",
    "    def __init__(self, chroma_client, collection_name: str, embedding_function, chunks: List[RAGChunk]):\n",
    "        self.client = chroma_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        self.chunks = chunks\n",
    "\n",
    "        # Create the collection\n",
    "        self.collection = self._create_collection()\n",
    "        self.add_chunks_to_collection()\n",
    "\n",
    "    def _create_collection(self):\n",
    "        return self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "\n",
    "    def add_chunks_to_collection(self, batch_size: int = 20, num_workers: int = 10):\n",
    "        batches = [self.chunks[i:i + batch_size] for i in range(0, len(self.chunks), batch_size)]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = [executor.submit(self._add_batch, batch) for batch in batches]\n",
    "            for future in as_completed(futures):\n",
    "                future.result()  # This will raise an exception if one occurred during the execution\n",
    "        print('Finished Ingesting Chunks Into Collection')\n",
    "\n",
    "    def _add_batch(self, batch: List[RAGChunk]):\n",
    "        self.collection.add(\n",
    "            ids=[chunk.id_ for chunk in batch],\n",
    "            documents=[chunk.text for chunk in batch],\n",
    "            metadatas=[chunk.metadata for chunk in batch]\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query_text: str, n_results: int = 5) -> List[RetrievalResult]:\n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=['embeddings', 'documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        # Transform the results into RetrievalResult objects\n",
    "        retrieval_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                id=results['ids'][0][i],\n",
    "                document=results['documents'][0][i],\n",
    "                embedding=results['embeddings'][0][i],\n",
    "                distance=results['distances'][0][i],\n",
    "                metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "            ))\n",
    "\n",
    "        return retrieval_results\n",
    "\n",
    "class Passage(BaseModel):\n",
    "    chunk: str\n",
    "    file_name: str\n",
    "    score: float = 0.0\n",
    "\n",
    "\n",
    "class BaseReRankTask(ABC):\n",
    "    @abstractmethod\n",
    "    def rerank(self, query_text: str, passages: List[Passage]) -> List[Passage]:\n",
    "        \"\"\"\n",
    "        Retrieve documents based on the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string to search for.\n",
    "\n",
    "        Returns:\n",
    "            List[RetrievalResult]: A list of RetrievalResult objects that are relevant to the query.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class CrossEncoderReRankTask(BaseReRankTask):\n",
    "    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-12-v2', score_threshold: float = -0.999):\n",
    "        self.cross_encoder = SentenceTransformerCrossEncoder(model_name)\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "    def rerank(self, query: str, passages: List[Passage]) -> List[Passage]:\n",
    "        # Prepare input pairs for the cross-encoder\n",
    "        input_pairs = [[query, passage.chunk] for passage in passages]\n",
    "\n",
    "        # Get scores from the cross-encoder\n",
    "        scores = self.cross_encoder.predict(input_pairs)\n",
    "\n",
    "        # Create a list of (score, passage) tuples and filter based on threshold\n",
    "        scored_passages = [(float(score), passage) for score, passage in zip(scores, passages)]\n",
    "\n",
    "        # Sort the passages based on their scores in descending order\n",
    "        sorted_passages = sorted(scored_passages, key=lambda x: x[0], reverse=False)\n",
    "\n",
    "        # Update passage scores and return\n",
    "        result = []\n",
    "        for score, passage in sorted_passages:\n",
    "            passage.score = score\n",
    "            result.append(passage)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class Util():\n",
    "    def __init__(self,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.debug = debug\n",
    "        self.wrapper = BedrockLLMWrapper(model_id='anthropic.claude-3-sonnet-20240229-v1:0', temperature=0,max_token_count=1000)\n",
    "\n",
    "    REASONING_PATTERN = r'<thinking>(.*?)</thinking>'\n",
    "    SCORE_PATTERN = r'<score>(.*?)</score>'\n",
    "    ANSWER_PATTERN = r'<question_answer>(.*?)</question_answer>'\n",
    "\n",
    "    # Strip out the portion of the response with regex.\n",
    "    def extract_with_regex(self, response, regex):\n",
    "        matches = re.search(regex, response, re.DOTALL)\n",
    "        # Extract the matched content, if any\n",
    "        return matches.group(1).strip() if matches else None\n",
    "\n",
    "    def format_results(self, grade: str, chat_conversation: list[dict]) -> dict:\n",
    "        reasoning: str = self.extract_with_regex(grade, self.REASONING_PATTERN)\n",
    "        score: str =  self.extract_with_regex(grade, self.SCORE_PATTERN)\n",
    "        \n",
    "        return {\n",
    "            'chat_conversation': chat_conversation,\n",
    "            'reasoning': reasoning,\n",
    "            'score': score\n",
    "        }\n",
    "\n",
    "    def compare_results(self, answer_results1, answer_results2):\n",
    "        \n",
    "\n",
    "        # # Function to convert 'score' column\n",
    "        def convert_score(df):\n",
    "            # df['score'] = df['score'].map({'correct': 1, 'incorrect': 0})\n",
    "            df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0).astype(int)\n",
    "            return df\n",
    "\n",
    "        # Apply the conversion to both dataframes\n",
    "        answer_results1 = convert_score(answer_results1)\n",
    "        answer_results2 = convert_score(answer_results2)\n",
    "\n",
    "        # Calculate the average values for each metric\n",
    "        metrics = ['score', 'faithfulness' ,'answer_relevancy', 'latency', 'cost']\n",
    "        avg_results1 = [answer_results1[metric].mean() for metric in metrics]\n",
    "        avg_results2 = [answer_results2[metric].mean() for metric in metrics]\n",
    "\n",
    "        # Calculate percentage change, handling divide-by-zero and infinite cases\n",
    "        def safe_percent_change(a, b):\n",
    "            if pd.isna(a) or pd.isna(b):\n",
    "                return 0\n",
    "            if a == 0 and b == 0:\n",
    "                return 0\n",
    "            elif a == 0:\n",
    "                return 100  # Arbitrarily set to 100% increase if original value was 0\n",
    "            else:\n",
    "                change = (b - a) / a * 100\n",
    "                return change if np.isfinite(change) else 0\n",
    "\n",
    "        percent_change = [safe_percent_change(a, b) for a, b in zip(avg_results1, avg_results2)]\n",
    "\n",
    "        # Set up the bar chart\n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.5\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Create the bars\n",
    "        bars = ax.bar(x, percent_change, width)\n",
    "\n",
    "        # Customize the chart\n",
    "        ax.set_ylabel('Percentage Change (%)')\n",
    "        ax.set_title('Percentage Change in Metrics (Results 2 vs Results 1)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics)\n",
    "\n",
    "        # Add a horizontal line at y=0\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "        # Add value labels on top of each bar\n",
    "        def autolabel(rects):\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                ax.annotate(f'{height:.2f}%',\n",
    "                            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                            xytext=(0, 3 if height >= 0 else -3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom' if height >= 0 else 'top')\n",
    "\n",
    "        autolabel(bars)\n",
    "\n",
    "        # Color the bars based on positive (green) or negative (red) change\n",
    "        # For latency & cost, reverse the color logic\n",
    "        for bar, change, metric in zip(bars, percent_change, metrics):\n",
    "            if metric == 'latency' or metric == 'cost':\n",
    "                bar.set_color('green' if change <= 0 else 'red')\n",
    "            else:\n",
    "                bar.set_color('green' if change >= 0 else 'red')\n",
    "            \n",
    "\n",
    "        # Adjust layout and display the chart\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_faithfulness(self, answer, context, similarity_threshold=0.3):\n",
    "        # Tokenize the answer into sentences (potential claims)\n",
    "        claims = sent_tokenize(answer)\n",
    "        \n",
    "        # Get stop words as a list\n",
    "        stop_words = list(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "        \n",
    "        # Fit the vectorizer on the context and claims\n",
    "        all_text = [context] + claims\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "        \n",
    "        # Get the context vector (first row of the matrix)\n",
    "        context_vector = tfidf_matrix[0]\n",
    "        \n",
    "        # Initialize counters\n",
    "        total_claims = len(claims)\n",
    "        # print(f'total_claims: {total_claims}')\n",
    "        supported_claims = 0\n",
    "        \n",
    "        # Calculate similarity between each claim and the context\n",
    "        for i, claim in enumerate(claims, start=1):\n",
    "            claim_vector = tfidf_matrix[i]\n",
    "            similarity = cosine_similarity(context_vector, claim_vector)[0][0]\n",
    "            # print(f'similarity: {similarity}')\n",
    "            # If similarity is above a threshold, consider it supported\n",
    "            if similarity > similarity_threshold:  # You can adjust this threshold\n",
    "                supported_claims += 1\n",
    "        \n",
    "        # Calculate faithfulness score\n",
    "        faithfulness_score = supported_claims / total_claims if total_claims > 0 else 0\n",
    "        rounded_score = round(faithfulness_score, 3)\n",
    "        return rounded_score\n",
    "\n",
    "    def calculate_answer_relevance(self, actual_question, generated_answer, n=3):\n",
    "\n",
    "        # Step 1: Generate 'n' variants of the question using Amazon Bedrock\n",
    "        prompt = f\"\"\"Human: \n",
    "        Given the following answer, generate {n} possible questions that could have led to this answer:\n",
    "\n",
    "        Answer: {generated_answer}\n",
    "\n",
    "        Generate {n} different questions.\n",
    "        Assistant:\n",
    "        \"\"\"\n",
    "        response = self.wrapper.generate(prompt)\n",
    "        \n",
    "        generated_questions = response[0]\n",
    "        # mean_cosine_similarity = self.cosine_similarity(actual_question, generated_questions)\n",
    "\n",
    "        # Step 2: Vectorize the actual question and generated questions\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        all_questions = [actual_question] + generated_questions.split('\\n')\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "        # Step 3: Calculate cosine similarity between actual question and each generated question\n",
    "        actual_question_vector = tfidf_matrix[0]\n",
    "        generated_questions_vectors = tfidf_matrix[1:]\n",
    "        similarities = cosine_similarity(actual_question_vector, generated_questions_vectors)\n",
    "\n",
    "        # Step 4: Calculate the mean cosine similarity\n",
    "        mean_cosine_similarity = similarities.mean()\n",
    "        rounded_score = round(mean_cosine_similarity, 3)\n",
    "        return rounded_score\n",
    "            \n",
    "    \n",
    "\n",
    "    def calculate_cost(self, usage, model_id):\n",
    "        '''\n",
    "        Takes the usage tokens returned by Bedrock in input and output, and coverts to cost in dollars.\n",
    "        '''\n",
    "        \n",
    "        input_token_haiku = 0.25/1000000\n",
    "        output_token_haiku = 1.25/1000000\n",
    "        input_token_sonnet = 3.00/1000000\n",
    "        output_token_sonnet = 15.00/1000000\n",
    "        input_token_opus = 15.00/1000000\n",
    "        output_token_opus = 75.00/1000000\n",
    "        \n",
    "        input_token_titan_embeddingv1 = 0.1/1000000\n",
    "        input_token_titan_embeddingv2 = 0.02/1000000\n",
    "        input_token_titan_embeddingmultimodal = 0.8/1000000\n",
    "        input_token_titan_premier = 0.5/1000000\n",
    "        output_token_titan_premier = 1.5/1000000\n",
    "        input_token_titan_lite = 0.15/1000000\n",
    "        output_token_titan_lite = 0.2/1000000\n",
    "        input_token_titan_express = 0.2/1000000\n",
    "        output_token_titan_express = 0.6/1000000\n",
    "       \n",
    "        input_token_cohere_command = 0.15/1000000\n",
    "        output_token_cohere_command = 2/1000000\n",
    "        input_token_cohere_commandlight = 0.3/1000000\n",
    "        output_token_cohere_commandlight = 0.6/1000000\n",
    "        input_token_cohere_commandrplus = 3/1000000\n",
    "        output_token_cohere_commandrplus = 15/1000000\n",
    "        input_token_cohere_commandr = 5/1000000\n",
    "        output_token_cohere_commandr = 1.5/1000000\n",
    "        input_token_cohere_embedenglish = 0.1/1000000\n",
    "        input_token_cohere_embedmultilang = 0.1/1000000\n",
    "\n",
    "        input_token_llama3_8b = 0.4/1000000\n",
    "        output_token_llama3_8b = 0.6/1000000\n",
    "        input_token_llama3_70b = 2.6/1000000\n",
    "        output_token_llama3_70b = 3.5/1000000\n",
    "\n",
    "        input_token_mistral_8b = 0.15/1000000\n",
    "        output_token_mistral_8b = 0.2/1000000\n",
    "        input_token_mistral_large = 4/1000000\n",
    "        output_token_mistral_large = 12/1000000\n",
    "\n",
    "        cost = 0\n",
    "\n",
    "        if 'haiku' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_haiku\n",
    "            cost+= usage['outputTokens']*output_token_haiku\n",
    "        if 'sonnet' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_sonnet\n",
    "            cost+= usage['outputTokens']*output_token_sonnet\n",
    "        if 'opus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_opus\n",
    "            cost+= usage['outputTokens']*output_token_opus\n",
    "        if 'amazon.titan-embed-text-v1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv1\n",
    "        if 'amazon.titan-embed-text-v2' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv2\n",
    "        if 'cohere.embed-multilingual' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedmultilang\n",
    "        if 'cohere.embed-english' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedenglish \n",
    "        if 'meta.llama3-8b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_8b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_8b\n",
    "        if 'meta.llama3-70b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_70b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_70b\n",
    "        if 'cohere.command-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_command\n",
    "            cost+= usage['outputTokens']*output_token_cohere_command\n",
    "        if 'cohere.command-light-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandlight\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandlight\n",
    "        if 'cohere.command-r-plus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandrplus\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandrplus\n",
    "        if 'cohere.command-r' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandr\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandr\n",
    "        if 'amazon.titan-text-express' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_express\n",
    "            cost+= usage['outputTokens']*output_token_titan_express\n",
    "        if 'amazon.titan-text-lite' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_lite\n",
    "            cost+= usage['outputTokens']*output_token_titan_lite\n",
    "        if 'amazon.titan-text-premier' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_premier\n",
    "            cost+= usage['outputTokens']*output_token_titan_premier\n",
    "        if 'mistral.mixtral-8x7b-instruct-v0:1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_mistral_8b\n",
    "            cost+= usage['outputTokens']*output_token_mistral_8b\n",
    "\n",
    "        return cost\n",
    "\n",
    "\n",
    "class BedrockLLMWrapper():\n",
    "    def __init__(self,\n",
    "        model_id: str = 'anthropic.claude-3-haiku-20240307-v1:0',\n",
    "        guardrail_id: str = '',\n",
    "        guardrail_version: str = '',\n",
    "        top_k: int = 5,\n",
    "        top_p: int = 0.7,\n",
    "        temperature: float = 0.0,\n",
    "        max_token_count: int = 4000,\n",
    "        max_attempts: int = 3,\n",
    "        debug: bool = False,\n",
    "        region: str ='us-east-1'\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.guardrail_id = guardrail_id,\n",
    "        self.guardrail_version = guardrail_version,\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.region = region\n",
    "        self.debug = debug\n",
    "        config = Config(\n",
    "            retries = {\n",
    "                'max_attempts': 10,\n",
    "                'mode': 'standard'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", config=config, region_name=self.region)\n",
    "    \n",
    "    def get_embedding(self, body, modelId, accept, contentType):\n",
    "        response = self.bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding = response_body.get('embedding')\n",
    "        return embedding\n",
    "    \n",
    "    def generate(self,prompt):\n",
    "        if self.debug: \n",
    "            print('entered BedrockLLMWrapper generate')\n",
    "        attempt = 1\n",
    "\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "        messages = []\n",
    "        messages.append(message)\n",
    "\n",
    "        if len(self.guardrail_id) > 1:\n",
    "            # print(f'GUARDRAIL_ID: {self.guardrail_id}')\n",
    "            # print(type(self.guardrail_id))\n",
    "            # print(type(self.guardrail_version))\n",
    "            guardrail_config = {\n",
    "                \"guardrailIdentifier\": str(self.guardrail_id[0]),\n",
    "                \"guardrailVersion\": str(self.guardrail_version[0]),\n",
    "                \"trace\": \"enabled\"\n",
    "            }\n",
    "        \n",
    "        # model specific inference parameters to use.\n",
    "        if \"anthropic\" in self.model_id.lower():\n",
    "            # system_prompts = [{\"text\": \"You are a helpful AI Assistant.\"}]\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                                \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "                                \"topP\": self.top_p,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "        else:\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "\n",
    "        if self.debug: \n",
    "            print(\"Sending:\\nSystem:\\n\",str(system_prompts),\"\\nMessages:\\n\",str(messages))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Send the message.\n",
    "                \n",
    "                if len(self.guardrail_id) > 1:\n",
    "                    response = self.bedrock_runtime.converse(\n",
    "                        modelId=self.model_id,\n",
    "                        messages=messages,\n",
    "                        system=system_prompts,\n",
    "                        inferenceConfig=inference_config,\n",
    "                        additionalModelRequestFields=additional_model_fields,\n",
    "                        guardrailConfig=guardrail_config\n",
    "                    )\n",
    "                else:\n",
    "                    response = self.bedrock_runtime.converse(\n",
    "                        modelId=self.model_id,\n",
    "                        messages=messages,\n",
    "                        system=system_prompts,\n",
    "                        inferenceConfig=inference_config,\n",
    "                        additionalModelRequestFields=additional_model_fields\n",
    "                    )\n",
    "                # Log token usage.\n",
    "                text = response['output'].get('message').get('content')[0].get('text')\n",
    "                usage = response['usage']\n",
    "                latency = response['metrics'].get('latencyMs')\n",
    "\n",
    "                if self.debug: \n",
    "                    print(f'text: {text} ; and token usage: {usage} ; and query_time: {latency}')    \n",
    "                \n",
    "                break\n",
    "               \n",
    "            except Exception as e:\n",
    "                print(\"Error with calling Bedrock: \"+str(e))\n",
    "                attempt+=1\n",
    "                if attempt>self.max_attempts:\n",
    "                    print(\"Max attempts reached!\")\n",
    "                    result_text = str(e)\n",
    "                    break\n",
    "                else:#retry in 10 seconds\n",
    "                    print(\"retry\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "        # return result_text\n",
    "        return [text,usage,latency]\n",
    "\n",
    "    # Threaded function for queue processing.\n",
    "    def thread_request(self, q, results):\n",
    "        while True:\n",
    "            try:\n",
    "                index, prompt = q.get(block=False)\n",
    "                data = self.generate(prompt)\n",
    "                results[index] = data\n",
    "            except Queue.Empty:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'Error with prompt: {str(e)}')\n",
    "                results[index] = str(e)\n",
    "            finally:\n",
    "                q.task_done()\n",
    "\n",
    "    def generate_threaded(self, prompts, max_workers=15):\n",
    "        results = [None] * len(prompts)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_index = {executor.submit(self.generate, prompt): i for i, prompt in enumerate(prompts)}\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {exc}')\n",
    "                    results[index] = str(exc)\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8097ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG System Class\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, \n",
    "                 model_id:str,\n",
    "                 guardrail_id:str,\n",
    "                 guardrail_version:str,\n",
    "                 chroma_client, \n",
    "                 input_dir: str, \n",
    "                 collection_name: str, \n",
    "                 embedding_function: EmbeddingFunction, \n",
    "                 prompt_template: str):\n",
    "        self.model_id = model_id\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "        self.chunking_strategy = SentenceSplitterChunkingStrategy(input_dir)\n",
    "        self.chroma_client = chroma_client\n",
    "        self.embedding_function = embedding_function\n",
    "        self.collection_name = collection_name\n",
    "        self.retrieval_task = None\n",
    "        self.prompt_template = prompt_template\n",
    "        self.rerank_task = CrossEncoderReRankTask()\n",
    "        self.llm_wrapper = BedrockLLMWrapper(model_id=self.model_id,\n",
    "                                             guardrail_id=self.guardrail_id, \n",
    "                                             guardrail_version=self.guardrail_version)\n",
    "        self.util = Util()\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        chunks = self.chunking_strategy.process()\n",
    "        self.retrieval_task = ChromaDBRetrievalTask(\n",
    "            self.chroma_client,\n",
    "            self.collection_name,\n",
    "            self.embedding_function,\n",
    "            chunks\n",
    "        )\n",
    "\n",
    "    \n",
    "    def query(self, query_text: str, n_results: int = 5) -> str:\n",
    "        # Retrieve relevant passages\n",
    "        retrieval_results = self.retrieval_task.retrieve(query_text, n_results)\n",
    "\n",
    "        # Convert retrieval results to passages for reranking\n",
    "        passages = [\n",
    "            Passage(chunk=result.document, file_name=result.metadata.get('relative_path', ''))\n",
    "            for result in retrieval_results\n",
    "        ]\n",
    "\n",
    "        # Rerank passages\n",
    "        reranked_passages = self.rerank_task.rerank(query_text, passages)\n",
    "\n",
    "        # Construct context from top reranked passages\n",
    "        context = \"\\n\\n\".join([f\"Passage from {p.file_name}:\\n{p.chunk}\" for p in reranked_passages[:n_results]])\n",
    "        context = \"\\n\\n\".join([f\"Passage from {p.file_name}:\\n{p.chunk}\" for p in reranked_passages[:n_results]])\n",
    "        # print(f'context: {str(context)}')\n",
    "        prompt = self.prompt_template.format(question=query_text, context=context)\n",
    "        answer = self.llm_wrapper.generate(prompt)\n",
    "        return answer\n",
    "    \n",
    "    def generate_threaded(self, prompts, max_workers=15):\n",
    "        results = [None] * len(prompts)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_index = {executor.submit(self.query, prompt): i for i, prompt in enumerate(prompts)}\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {exc}')\n",
    "                    results[index] = str(exc)\n",
    "        \n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45c3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystemTaskRunner:\n",
    "    def __init__(self, eval_df: pd.DataFrame, \n",
    "                 model_id:str = 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "                 rag_system: RAGSystem = None,\n",
    "                 eval_model_id:str = 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "                 temperature: float = 0.0,\n",
    "                 max_token_count: int = 2000,\n",
    "                 max_attempts: int = 3, \n",
    "                 prompt_template: str = '',\n",
    "                 prompt_eval_template: str = ''):\n",
    "        self.eval_df = eval_df\n",
    "        self.model_id = model_id\n",
    "        self.rag_system = rag_system\n",
    "        self.eval_model_id = eval_model_id\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.prompt_template = prompt_template\n",
    "        self.prompt_eval_template = prompt_eval_template\n",
    "        self.wrapper = BedrockLLMWrapper(model_id=self.model_id, \n",
    "                                         max_token_count=self.max_token_count,\n",
    "                                         temperature=self.temperature\n",
    "                                         )\n",
    "        self.eval_wrapper = BedrockLLMWrapper(model_id=self.eval_model_id, \n",
    "                                         max_token_count=self.max_token_count,\n",
    "                                         temperature=self.temperature\n",
    "                                         )\n",
    "\n",
    "    def get_prompt(self, question, context_list):\n",
    "        context = context_list\n",
    "        prompt = self.prompt_template.format(question=question, context=context)\n",
    "        return prompt\n",
    "\n",
    "    def build_grader_prompt(self, original_query: str, llm_system_response: str, ground_truth_answer:str, context_list, answer_relevancy:str):\n",
    "        context = context_list\n",
    "\n",
    "        prompt = self.prompt_eval_template.format(\n",
    "                        original_query= original_query,\n",
    "                        llm_system_response= llm_system_response,\n",
    "                        ground_truth_answer=ground_truth_answer,\n",
    "                        context=context,\n",
    "                        answer_relevancy = answer_relevancy,\n",
    "                        \n",
    "                    ) \n",
    "        return prompt\n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(self.eval_df)\n",
    "        util = Util()\n",
    "        \n",
    "        # Prepare prompts for answer generation\n",
    "        answer_prompts = [self.get_prompt(row['query_text'], row['context']) for _, row in df.iterrows()]\n",
    "        \n",
    "        # Generate answers in parallel\n",
    "        answer_responses = self.wrapper.generate_threaded(answer_prompts)\n",
    "        \n",
    "        eval_prompts = []\n",
    "        for (_, row), response in zip(df.iterrows(), answer_responses):\n",
    "            query = row['query_text']\n",
    "            groundtruth_answer = row['question_answer']\n",
    "            retrieved_chunks = row['context']\n",
    "            \n",
    "            generated_answer = util.extract_with_regex(str(response[0]), util.ANSWER_PATTERN)\n",
    "            \n",
    "            answer_relevancy = util.calculate_answer_relevance(query, generated_answer) if generated_answer else 0\n",
    "            \n",
    "            eval_prompts.append(self.build_grader_prompt(query, generated_answer, groundtruth_answer, retrieved_chunks, answer_relevancy))\n",
    "        \n",
    "        # Generate evaluations in parallel\n",
    "        if self.rag_system == None:\n",
    "            eval_responses = self.eval_wrapper.generate_threaded(eval_prompts)\n",
    "        else:\n",
    "            eval_responses = self.rag_system.generate_threaded(eval_prompts)\n",
    "\n",
    "        \n",
    "        results = []\n",
    "        for (_, row), answer_response, eval_response in zip(df.iterrows(), answer_responses, eval_responses):\n",
    "            query = row['query_text']\n",
    "            groundtruth_answer = row['question_answer']\n",
    "            retrieved_chunks = row['context']\n",
    "            \n",
    "            generated_answer = util.extract_with_regex(str(answer_response[0]), util.ANSWER_PATTERN)\n",
    "            \n",
    "            reasoning = util.extract_with_regex(str(eval_response[0]), util.REASONING_PATTERN)\n",
    "            score = util.extract_with_regex(str(eval_response[0]), util.SCORE_PATTERN)\n",
    "            \n",
    "            answer_relevancy = util.calculate_answer_relevance(query, generated_answer) if generated_answer else 0\n",
    "            faithfulness = util.calculate_faithfulness(generated_answer, retrieved_chunks) if generated_answer else 0\n",
    "            \n",
    "            cost = util.calculate_cost(answer_response[1], self.model_id)\n",
    "            \n",
    "            result = {\n",
    "                'query_text': query,\n",
    "                'groundtruth_answer': groundtruth_answer,\n",
    "                'retrieved_chunks': json.dumps(retrieved_chunks),\n",
    "                'generated_answer': generated_answer,\n",
    "                'usage': json.dumps(answer_response[1]),\n",
    "                'latency': answer_response[2],\n",
    "                'reasoning': str(reasoning),\n",
    "                'score': score,\n",
    "                'faithfulness': faithfulness,\n",
    "                'answer_relevancy': answer_relevancy,\n",
    "                'cost': cost,\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead14b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huthmac/.pyenv/versions/3.11.7/envs/llm-system-eval/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Created 16549 chunks.\n",
      "Finished Ingesting Chunks Into Collection\n"
     ]
    }
   ],
   "source": [
    "# Define some experiment variables\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "TITAN_TEXT_EMBED_V2_ID: str = \"amazon.titan-embed-text-v2:0\"\n",
    "COLLECTION_NAME: str = 'experiment_3_collection'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=TITAN_TEXT_EMBED_V2_ID\n",
    ")\n",
    "\n",
    "# Template 1: Long prompt with XML tags context chunks, and step by step instructions, and reiterating how the response should be provided in the end.\n",
    "prompt_template_claude_1 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest research assistant, dedicated to providing valuable and accurate information.\n",
    "        You will be provided with a report extract between <report></report> XML tags, please read it and analyse the content.\n",
    "        Please answer the following question: \n",
    "        {question} \n",
    "        \n",
    "        The answer must only be based on the information from the context below.\n",
    "\n",
    "        If a particular bit of information is not present, return \"There is not enough information available to answer this question\" inside the XML tags.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The context will be given between <context></context> XML tags.\n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "rag_system = RAGSystem(model_id=MODEL_ID,\n",
    "                       guardrail_id=\"\",\n",
    "                       guardrail_version=\"\",\n",
    "                       chroma_client=chroma_client,\n",
    "                       input_dir=\"../data/opensearch-docs\",\n",
    "                       collection_name=COLLECTION_NAME,\n",
    "                       embedding_function=embedding_function,\n",
    "                       prompt_template=prompt_template_claude_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614e97fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<question_answer>\\nYes, OpenSearch supports the zstd compression codec. According to the documentation, as of OpenSearch 2.9, two new codecs based on the Zstandard compression algorithm are available: `zstd` and `zstd_no_dict`. These codecs provide a good balance between compression ratio and speed.\\n</question_answer>',\n",
       " {'inputTokens': 1316, 'outputTokens': 85, 'totalTokens': 1401},\n",
       " 1016]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test RAG system with guardrails\n",
    "rag_system.query(\"Does openearch support zstd compression?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a395ade",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b915cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Evaluation Template\n",
    "## this template could be improved by adding eval examples.\n",
    "\n",
    "prompt_eval_template = \"\"\"You are an expert judge evaluating the Retrieval Augmented Generation applications.\n",
    "                   Your task is to evaluate a given answer based on a context and question using the criteria provided below.\n",
    " \n",
    "                    Evaluation Criteria (Additive Score, 0-3):\n",
    "                    1. Context: Award 1 point if the answer uses only information provided in the context, without introducing external or fabricated details.\n",
    "                    2. Completeness: Add 1 point if the answer addresses all key elements of the question based on the available context, without omissions.\n",
    "                    3. Conciseness: Add a final point if the answer uses the fewest words possible to address the question and avoids redundancy.\n",
    "                    \n",
    "                    Evaluation Steps:\n",
    "                    1. Read provided context, question and answer carefully.\n",
    "                    2. Go through each evaluation criterion one by one and assess whether the answer meets the criteria.\n",
    "                    3. Compose your reasoning for each critera, explaining why you did or did not award a point. You can only award full points. \n",
    "                    4. Calculate the total score by summing the points awarded.\n",
    "                    5. Think through the evaluation criteria inside <thinking></thinking> tags. \n",
    "                    Then, output the total score inside <score></score> tags.\n",
    "                    Review your formatted response. It needs to be valid XML.\n",
    "\n",
    "                    Now, please evaluate the following:\n",
    "\n",
    "                    Question:\n",
    "                    <original_query>\n",
    "                    {original_query}\n",
    "                    </original_query>\n",
    "\n",
    "                    Generated answer:\n",
    "                    <llm_system_response>\n",
    "                    {llm_system_response}\n",
    "                    </llm_system_response>\n",
    "\n",
    "                    Ground truth answer:\n",
    "                    <ground_truth_answer>\n",
    "                    {ground_truth_answer}\n",
    "                    </ground_truth_answer>\n",
    "\n",
    "                    Context:\n",
    "                    <context>\n",
    "                    {context}\n",
    "                    </context>\n",
    "\n",
    "                    Here is the answer_relevancy score based of the original question and generated questions based of the generated answer.\n",
    "                    <answer_relevancy>\n",
    "                    {answer_relevancy}\n",
    "                    </answer_relevancy>\n",
    "                    The answer_relevancy score range between 0 and 1 where higher scores indicate better relevancy.\n",
    "                \n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c63ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated an exception: An error occurred (ValidationException) when calling the InvokeModel operation: 400 Bad Request: Too many input tokens. Max input tokens: 8192, request input token count: 8519 \n",
      "Generated an exception: An error occurred (ValidationException) when calling the InvokeModel operation: 400 Bad Request: Too many input tokens. Max input tokens: 8192, request input token count: 10753 \n",
      "Generated an exception: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: expected maxLength: 50000, actual: 57938, please reformat your input and try again.\n"
     ]
    }
   ],
   "source": [
    "# RAG eval 1\n",
    "EVAL_PATH = '../data/eval-datasets/4_answer_validation_opensearch.jsonl'\n",
    "answer_eval_df = pd.read_json(EVAL_PATH,lines=True)\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "prompt_template_claude_1 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest research assistant, dedicated to providing valuable and accurate information.\n",
    "        You will be provided with a report extract between <report></report> XML tags, please read it and analyse the content.\n",
    "        Please answer the following question: \n",
    "        {question} \n",
    "        \n",
    "        The answer must only be based on the information from the context below.\n",
    "\n",
    "        If a particular bit of information is not present, return \"There is not enough information available to answer this question\" inside the XML tags.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The context will be given between <context></context> XML tags.\n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "llm_results1: pd.DataFrame = RAGSystemTaskRunner(answer_eval_df,rag_system=rag_system, model_id=MODEL_ID, prompt_template=prompt_template_claude_1, prompt_eval_template=prompt_eval_template).run()\n",
    "llm_results1.to_json('../data/eval-datasets/5_llm_validation_opensearch_without_guardrail.jsonl', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2f5f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huthmac/.pyenv/versions/3.11.7/envs/llm-system-eval/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Created 16549 chunks.\n",
      "Finished Ingesting Chunks Into Collection\n",
      "Generated an exception: An error occurred (ValidationException) when calling the InvokeModel operation: 400 Bad Request: Too many input tokens. Max input tokens: 8192, request input token count: 8714 \n",
      "Generated an exception: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: expected maxLength: 50000, actual: 58153, please reformat your input and try again.\n",
      "Generated an exception: An error occurred (ValidationException) when calling the InvokeModel operation: 400 Bad Request: Too many input tokens. Max input tokens: 8192, request input token count: 10766 \n"
     ]
    }
   ],
   "source": [
    "# RAG eval 2\n",
    "\n",
    "# Define some experiment variables\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "TITAN_TEXT_EMBED_V2_ID: str = \"amazon.titan-embed-text-v2:0\"\n",
    "COLLECTION_NAME: str = 'experiment_3_collection'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=TITAN_TEXT_EMBED_V2_ID\n",
    ")\n",
    "\n",
    "# Template 1: Long prompt with XML tags context chunks, and step by step instructions, and reiterating how the response should be provided in the end.\n",
    "prompt_template_claude_1 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest research assistant, dedicated to providing valuable and accurate information.\n",
    "        You will be provided with a report extract between <report></report> XML tags, please read it and analyse the content.\n",
    "        Please answer the following question: \n",
    "        {question} \n",
    "        \n",
    "        The answer must only be based on the information from the context below.\n",
    "\n",
    "        If a particular bit of information is not present, return \"There is not enough information available to answer this question\" inside the XML tags.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The context will be given between <context></context> XML tags.\n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "rag_system2 = RAGSystem(model_id=MODEL_ID,\n",
    "                       guardrail_id=str(GUARDRAIL_ID),\n",
    "                       guardrail_version=str(VERSION),\n",
    "                       chroma_client=chroma_client,\n",
    "                       input_dir=\"../data/opensearch-docs\",\n",
    "                       collection_name=COLLECTION_NAME,\n",
    "                       embedding_function=embedding_function,\n",
    "                       prompt_template=prompt_template_claude_1)\n",
    "\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "llm_results2: pd.DataFrame = RAGSystemTaskRunner(answer_eval_df, rag_system=rag_system2, model_id=MODEL_ID, prompt_template=prompt_template_claude_1, prompt_eval_template=prompt_eval_template).run()\n",
    "llm_results2.to_json('../data/eval-datasets/5_llm_validation_opensearch_with_guardrails.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734304de",
   "metadata": {},
   "source": [
    "### Review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31aa863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util = Util()\n",
    "util.compare_results(llm_results1, llm_results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d4a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct for answer_results1: 0.00%\n",
      "Percentage correct for answer_results2: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>query_text</th>\n",
       "      <th>groundtruth_answer</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>usage</th>\n",
       "      <th>latency</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>score</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>What software license does Opensearch use?</td>\n",
       "      <td>&lt;question_answer&gt;There is not enough information available to answer this question&lt;/question_answer&gt;</td>\n",
       "      <td>There is not enough information available to answer this question</td>\n",
       "      <td>{\"inputTokens\": 2098, \"outputTokens\": 23, \"totalTokens\": 2121}</td>\n",
       "      <td>2426</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.006639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I need to combine relevance scores from different types of searches.</td>\n",
       "      <td>&lt;question_answer&gt;\\nA hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.\\n&lt;/question_answer&gt;</td>\n",
       "      <td>A hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.</td>\n",
       "      <td>{\"inputTokens\": 570, \"outputTokens\": 63, \"totalTokens\": 633}</td>\n",
       "      <td>1807</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.002655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working?</td>\n",
       "      <td>&lt;question_answer&gt;\\nThere is not enough information available to answer this question. The context does not mention why zstd compression is not working for OpenSearch version 2.1. The context provides information about compression codecs available in OpenSearch 2.9 and later versions, but does not cover version 2.1.\\n&lt;/question_answer&gt;</td>\n",
       "      <td>There is not enough information available to answer this question. The context does not mention why zstd compression is not working for OpenSearch version 2.1. The context provides information about compression codecs available in OpenSearch 2.9 and later versions, but does not cover version 2.1.</td>\n",
       "      <td>{\"inputTokens\": 4945, \"outputTokens\": 79, \"totalTokens\": 5024}</td>\n",
       "      <td>6225</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.016020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>query_text</th>\n",
       "      <th>groundtruth_answer</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>usage</th>\n",
       "      <th>latency</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>score</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial review of grades from LLM-as-a-Judge\n",
    "\n",
    "llm_results1['score'] = llm_results1['score'].astype('int64')\n",
    "llm_results2['score'] = llm_results2['score'].astype('int64')\n",
    "\n",
    "def calculate_percentage_correct(df, threshold=2):\n",
    "    total_count = len(df)\n",
    "    correct_count = len(df[df['score'] > threshold])\n",
    "    return (correct_count / total_count) * 100\n",
    "\n",
    "percentage_correct1 = calculate_percentage_correct(llm_results1)\n",
    "print(f\"Percentage correct for answer_results1: {percentage_correct1:.2f}%\")\n",
    "\n",
    "percentage_correct2 = calculate_percentage_correct(llm_results2)\n",
    "print(f\"Percentage correct for answer_results2: {percentage_correct2:.2f}%\")\n",
    "\n",
    "# sample a subsection of 3 incorrect responses for answer_results1\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sample_size1 = 3 if len(llm_results1[(llm_results1['score'] <= 2)])  > 3 else len(llm_results1[(llm_results1['score'] <= 2)])\n",
    "incorrect_rows = llm_results1[(llm_results1['score'] <= 2)].sample(n=sample_size1)\n",
    "incorrect_rows_no_context = incorrect_rows.drop(columns=['retrieved_chunks'])\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = incorrect_rows_no_context.to_html(index=False, classes='table table-striped')\n",
    "display(HTML(table_html))\n",
    "\n",
    "# sample a subsection of 3 correct responses for answer_results1\n",
    "sample_size2 = 3 if len(llm_results1[(llm_results1['score'] > 2)]) > 3 else len(llm_results1[(llm_results1['score'] > 2)])\n",
    "correct_rows = llm_results1[(llm_results1['score'] > 2)].sample(n=sample_size2)\n",
    "correct_rows_no_context = correct_rows.drop(columns=['retrieved_chunks'])\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = correct_rows_no_context.to_html(index=False, classes='table table-striped')\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f14b1e-b99d-468f-a574-5b7b44251eac",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* If accuracy as evaluated by LLM-as-a-Judge remains similar, but other metrics such as cost and latency improve, then the alternative LLM (e.g. Haiku) is a viable option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c58c7",
   "metadata": {},
   "source": [
    "### SCRATCHPAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a34cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
