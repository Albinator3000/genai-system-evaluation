{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52226f6b-2660-4851-818e-efecea8766c9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this notebook is to demonstrate how to evaluate responses from a RAG system (touch point 4: https://quip-amazon.com/FtXLAsgGOWYn/LLM-Validation-Evaluation-lifecycle-and-best-practices).\n",
    "\n",
    "\n",
    "## What Metrics Should I Care About.\n",
    "Metrics for LLM calls can be broken up into two categories. Subjective and Absolute. Absolute metrics like latency, throughput, etc.. are easier to calculate. Subjective metrics are more difficult. These subjective categories range from truthfulness, faithfulness, answer relevancy, to any custom metric your business cares about.\n",
    "In all the subjective metrics, it typically requires a level of human reasoning to determine a numeric answer.\n",
    "This notebook covers the following metrics:\n",
    "* faithfulness\n",
    "* answer relevancy\n",
    "* latency\n",
    "* cost\n",
    "* use case specific specific evaluation score from LLM-as-a-Judge (& from human annotators in notebook 4b)\n",
    "\n",
    "## Techniques\n",
    "\n",
    "For Subjective metrics you have 2 options. \n",
    "(1) use human annotators or \n",
    "(2) use another LLM to judge your responses. Both have pros and cons and we’ll discuss how to do each below. \n",
    "\n",
    "### Human Evaluators\n",
    "This is a time intensive process. It requires humans to go through and evaluate your answer. You need to select the humans carefully and make sure their instructions on how to grade are clear. Typically, you give the evaluators a rubric just like a teacher might use when grading a task in school. \n",
    "\n",
    "### LLM-As-A-Judge\n",
    "This is a newer technique where you give an LLM a grading rubric and it performs the same evaluation that the human annotators would do in the section above. This poses an important question. \n",
    "\n",
    "## How do we trust an LLM to grade the answers correctly? \n",
    "To use LLM-As-A-Judge, you have to iterate on the evaluation prompt until the human annotators generally agree with the LLMs grades. An evaluation dataset should be created and graded by a human. That same dataset is run through an LLM using a grading rubric. If the responses align then the evaluation prompt is ready to be used. If not, you need to iterate on the prompt until the humans and LLM agrees. \n",
    "\n",
    "## Evaluating in Production\n",
    "An important benefit of LLM-As-A-Judge is that it can be run in production. You often won’t have the correct answer when seeing net new requests so the grading rubric needs to be tweaked to not include the gold standard answer. The LLM can output a metric on the systems response which can be pushed into an observability solution to monitor it’s performance on production traffic. \n",
    "\n",
    "This can become costly. You’re increasing the number of invocations to the LLM so it’s common practice to sample responses and only run the evaluation on a subset of traffic to keep costs down.\n",
    "\n",
    "Note: An important consideration is how you want to improve the system. Being able to quickly find poor performing prompts / inputs is key to improving it. In a feedback loop, it’s common to take poor performing examples, correct them, and use them as dynamic few shot examples in future prompts. If you’re going down the model training path, it’s important to collect data points in pairs. RLHF whether that’s PPO or DPO relies on pair wise data when going through data cycles. If you output diverse pairs of answers, it saves data annotators time because they are rating their preference between pairs vs. having to hand write the answer they prefer.\n",
    "\n",
    "# What Will We Do? \n",
    "* **Create an eval prompt template (notebook 4a)** \n",
    "* **Create two different RAG prompt templates (notebook 4a)** \n",
    "* **Run LLM-As-A-Judge against evaluation dataset and grade RAG system responses (notebook 4a)** \n",
    "* Run Human-Eval to ensure LLM-As-A-Judge is aligned with human preferences (notebook 4b) \n",
    "* Run Human-Eval and compare GroundTruth and RAG system response (notebook 4b)\n",
    "* Run through evaluation with different LLMs (notebook 4c) \n",
    "\n",
    "So let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65b599-7663-4785-bd75-0f139216fa17",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c04eab6-7444-45ae-8ce8-f731398c51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda create -y --name llm-system-eval python=3.11.7\n",
    "# !conda init && activate llm-system-eval\n",
    "# !conda install -n llm-system-eval ipykernel --update-deps --force-reinstall -y\n",
    "# OR\n",
    "# !pyenv virtualenv 3.11.7 llm-system-eval\n",
    "# !pyenv activate llm-system-eval\n",
    "\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c38224",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b58a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:lambda:us-east-1:432418664414:function:PRE-PassThrough\n",
      "arn:aws:lambda:us-east-1:432418664414:function:ACS-PassThrough\n"
     ]
    }
   ],
   "source": [
    "# set variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'llm-system-eval.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "REGION = os.environ['REGION']\n",
    "os.environ['WORKTEAM_ARN'] = os.getenv('WORKTEAM_ARN')\n",
    "WORKTEAM_ARN = os.environ['WORKTEAM_ARN']\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "os.environ['SAGEMAKER_ROLE_ARN'] = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "SAGEMAKER_ROLE_ARN = os.environ['SAGEMAKER_ROLE_ARN'] # OR sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"./chroma_db\"))\n",
    "\n",
    "# Also initialize the bedrock client so we can call some embedding models!\n",
    "session = boto3.Session(profile_name='default')\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "role_name = SAGEMAKER_ROLE_ARN.split(\"/\")[-1]\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "# Specify ARNs for resources needed to run an text classification job.\n",
    "ac_arn_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\"\n",
    "}\n",
    "# PreHumanTaskLambdaArn for text classification(single)\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-PassThrough\".format(\n",
    "    REGION, ac_arn_map[REGION]\n",
    ")\n",
    "\n",
    "# AnnotationConsolidationConfig for text classification(single)\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-PassThrough\".format(REGION, ac_arn_map[REGION])\n",
    "\n",
    "print(prehuman_arn)\n",
    "print(acs_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c0ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER validation helper classes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import io\n",
    "from io import StringIO\n",
    "import time\n",
    "import re\n",
    "import typing as t\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from numpy.linalg import norm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "class Util():\n",
    "    def __init__(self,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.debug = debug\n",
    "        self.wrapper = BedrockLLMWrapper(model_id='anthropic.claude-3-sonnet-20240229-v1:0', temperature=0,max_token_count=1000)\n",
    "\n",
    "    REASONING_PATTERN = r'<thinking>(.*?)</thinking>'\n",
    "    SCORE_PATTERN = r'<score>(.*?)</score>'\n",
    "    ANSWER_PATTERN = r'<question_answer>(.*?)</question_answer>'\n",
    "\n",
    "    # Strip out the portion of the response with regex.\n",
    "    def extract_with_regex(self, response, regex):\n",
    "        matches = re.search(regex, response, re.DOTALL)\n",
    "        # Extract the matched content, if any\n",
    "        return matches.group(1).strip() if matches else None\n",
    "\n",
    "    def format_results(self, grade: str, chat_conversation: list[dict]) -> dict:\n",
    "        reasoning: str = self.extract_with_regex(grade, self.REASONING_PATTERN)\n",
    "        score: str =  self.extract_with_regex(grade, self.SCORE_PATTERN)\n",
    "        \n",
    "        return {\n",
    "            'chat_conversation': chat_conversation,\n",
    "            'reasoning': reasoning,\n",
    "            'score': score\n",
    "        }\n",
    "\n",
    "    def compare_results(self, answer_results1, answer_results2):\n",
    "        \n",
    "\n",
    "        # # Function to convert 'score' column\n",
    "        def convert_score(df):\n",
    "            # df['score'] = df['score'].map({'correct': 1, 'incorrect': 0})\n",
    "            df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0).astype(int)\n",
    "            return df\n",
    "\n",
    "        # Apply the conversion to both dataframes\n",
    "        answer_results1 = convert_score(answer_results1)\n",
    "        answer_results2 = convert_score(answer_results2)\n",
    "\n",
    "        # Calculate the average values for each metric\n",
    "        metrics = ['score', 'faithfulness' ,'answer_relevancy', 'latency', 'cost']\n",
    "        avg_results1 = [answer_results1[metric].mean() for metric in metrics]\n",
    "        avg_results2 = [answer_results2[metric].mean() for metric in metrics]\n",
    "\n",
    "        # Calculate percentage change, handling divide-by-zero and infinite cases\n",
    "        def safe_percent_change(a, b):\n",
    "            if pd.isna(a) or pd.isna(b):\n",
    "                return 0\n",
    "            if a == 0 and b == 0:\n",
    "                return 0\n",
    "            elif a == 0:\n",
    "                return 100  # Arbitrarily set to 100% increase if original value was 0\n",
    "            else:\n",
    "                change = (b - a) / a * 100\n",
    "                return change if np.isfinite(change) else 0\n",
    "\n",
    "        percent_change = [safe_percent_change(a, b) for a, b in zip(avg_results1, avg_results2)]\n",
    "\n",
    "        # Set up the bar chart\n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.5\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Create the bars\n",
    "        bars = ax.bar(x, percent_change, width)\n",
    "\n",
    "        # Customize the chart\n",
    "        ax.set_ylabel('Percentage Change (%)')\n",
    "        ax.set_title('Percentage Change in Metrics (Results 2 vs Results 1)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics)\n",
    "\n",
    "        # Add a horizontal line at y=0\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "        # Add value labels on top of each bar\n",
    "        def autolabel(rects):\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                ax.annotate(f'{height:.2f}%',\n",
    "                            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                            xytext=(0, 3 if height >= 0 else -3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom' if height >= 0 else 'top')\n",
    "\n",
    "        autolabel(bars)\n",
    "\n",
    "        # Color the bars based on positive (green) or negative (red) change\n",
    "        # For latency & cost, reverse the color logic\n",
    "        for bar, change, metric in zip(bars, percent_change, metrics):\n",
    "            if metric == 'latency' or metric == 'cost':\n",
    "                bar.set_color('green' if change <= 0 else 'red')\n",
    "            else:\n",
    "                bar.set_color('green' if change >= 0 else 'red')\n",
    "            \n",
    "\n",
    "        # Adjust layout and display the chart\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_faithfulness(self, answer, context, similarity_threshold=0.3):\n",
    "        # Tokenize the answer into sentences (potential claims)\n",
    "        claims = sent_tokenize(answer)\n",
    "        \n",
    "        # Get stop words as a list\n",
    "        stop_words = list(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "        \n",
    "        # Fit the vectorizer on the context and claims\n",
    "        all_text = [context] + claims\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "        \n",
    "        # Get the context vector (first row of the matrix)\n",
    "        context_vector = tfidf_matrix[0]\n",
    "        \n",
    "        # Initialize counters\n",
    "        total_claims = len(claims)\n",
    "        # print(f'total_claims: {total_claims}')\n",
    "        supported_claims = 0\n",
    "        \n",
    "        # Calculate similarity between each claim and the context\n",
    "        for i, claim in enumerate(claims, start=1):\n",
    "            claim_vector = tfidf_matrix[i]\n",
    "            similarity = cosine_similarity(context_vector, claim_vector)[0][0]\n",
    "            # print(f'similarity: {similarity}')\n",
    "            # If similarity is above a threshold, consider it supported\n",
    "            if similarity > similarity_threshold:  # You can adjust this threshold\n",
    "                supported_claims += 1\n",
    "        \n",
    "        # Calculate faithfulness score\n",
    "        faithfulness_score = supported_claims / total_claims if total_claims > 0 else 0\n",
    "        rounded_score = round(faithfulness_score, 3)\n",
    "        return rounded_score\n",
    "\n",
    "    def calculate_answer_relevance(self, actual_question, generated_answer, n=3):\n",
    "\n",
    "        # Step 1: Generate 'n' variants of the question using Amazon Bedrock\n",
    "        prompt = f\"\"\"Human: \n",
    "        Given the following answer, generate {n} possible questions that could have led to this answer:\n",
    "\n",
    "        Answer: {generated_answer}\n",
    "\n",
    "        Generate {n} different questions.\n",
    "        Assistant:\n",
    "        \"\"\"\n",
    "        response = self.wrapper.generate(prompt)\n",
    "        \n",
    "        generated_questions = response[0]\n",
    "        # mean_cosine_similarity = self.cosine_similarity(actual_question, generated_questions)\n",
    "\n",
    "        # Step 2: Vectorize the actual question and generated questions\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        all_questions = [actual_question] + generated_questions.split('\\n')\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "\n",
    "        # Step 3: Calculate cosine similarity between actual question and each generated question\n",
    "        actual_question_vector = tfidf_matrix[0]\n",
    "        generated_questions_vectors = tfidf_matrix[1:]\n",
    "        similarities = cosine_similarity(actual_question_vector, generated_questions_vectors)\n",
    "\n",
    "        # Step 4: Calculate the mean cosine similarity\n",
    "        mean_cosine_similarity = similarities.mean()\n",
    "        rounded_score = round(mean_cosine_similarity, 3)\n",
    "        return rounded_score\n",
    "            \n",
    "    \n",
    "\n",
    "    def calculate_cost(self, usage, model_id):\n",
    "        '''\n",
    "        Takes the usage tokens returned by Bedrock in input and output, and coverts to cost in dollars.\n",
    "        '''\n",
    "        \n",
    "        input_token_haiku = 0.25/1000000\n",
    "        output_token_haiku = 1.25/1000000\n",
    "        input_token_sonnet = 3.00/1000000\n",
    "        output_token_sonnet = 15.00/1000000\n",
    "        input_token_opus = 15.00/1000000\n",
    "        output_token_opus = 75.00/1000000\n",
    "        \n",
    "        input_token_titan_embeddingv1 = 0.1/1000000\n",
    "        input_token_titan_embeddingv2 = 0.02/1000000\n",
    "        input_token_titan_embeddingmultimodal = 0.8/1000000\n",
    "        input_token_titan_premier = 0.5/1000000\n",
    "        output_token_titan_premier = 1.5/1000000\n",
    "        input_token_titan_lite = 0.15/1000000\n",
    "        output_token_titan_lite = 0.2/1000000\n",
    "        input_token_titan_express = 0.2/1000000\n",
    "        output_token_titan_express = 0.6/1000000\n",
    "       \n",
    "        input_token_cohere_command = 0.15/1000000\n",
    "        output_token_cohere_command = 2/1000000\n",
    "        input_token_cohere_commandlight = 0.3/1000000\n",
    "        output_token_cohere_commandlight = 0.6/1000000\n",
    "        input_token_cohere_commandrplus = 3/1000000\n",
    "        output_token_cohere_commandrplus = 15/1000000\n",
    "        input_token_cohere_commandr = 5/1000000\n",
    "        output_token_cohere_commandr = 1.5/1000000\n",
    "        input_token_cohere_embedenglish = 0.1/1000000\n",
    "        input_token_cohere_embedmultilang = 0.1/1000000\n",
    "\n",
    "        input_token_llama3_8b = 0.4/1000000\n",
    "        output_token_llama3_8b = 0.6/1000000\n",
    "        input_token_llama3_70b = 2.6/1000000\n",
    "        output_token_llama3_70b = 3.5/1000000\n",
    "\n",
    "        input_token_mistral_8b = 0.15/1000000\n",
    "        output_token_mistral_8b = 0.2/1000000\n",
    "        input_token_mistral_large = 4/1000000\n",
    "        output_token_mistral_large = 12/1000000\n",
    "\n",
    "        cost = 0\n",
    "\n",
    "        if 'haiku' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_haiku\n",
    "            cost+= usage['outputTokens']*output_token_haiku\n",
    "        if 'sonnet' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_sonnet\n",
    "            cost+= usage['outputTokens']*output_token_sonnet\n",
    "        if 'opus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_opus\n",
    "            cost+= usage['outputTokens']*output_token_opus\n",
    "        if 'amazon.titan-embed-text-v1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv1\n",
    "        if 'amazon.titan-embed-text-v2' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv2\n",
    "        if 'cohere.embed-multilingual' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedmultilang\n",
    "        if 'cohere.embed-english' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedenglish \n",
    "        if 'meta.llama3-8b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_8b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_8b\n",
    "        if 'meta.llama3-70b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_70b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_70b\n",
    "        if 'cohere.command-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_command\n",
    "            cost+= usage['outputTokens']*output_token_cohere_command\n",
    "        if 'cohere.command-light-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandlight\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandlight\n",
    "        if 'cohere.command-r-plus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandrplus\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandrplus\n",
    "        if 'cohere.command-r' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandr\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandr\n",
    "        if 'amazon.titan-text-express' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_express\n",
    "            cost+= usage['outputTokens']*output_token_titan_express\n",
    "        if 'amazon.titan-text-lite' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_lite\n",
    "            cost+= usage['outputTokens']*output_token_titan_lite\n",
    "        if 'amazon.titan-text-premier' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_premier\n",
    "            cost+= usage['outputTokens']*output_token_titan_premier\n",
    "        if 'mistral.mixtral-8x7b-instruct-v0:1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_mistral_8b\n",
    "            cost+= usage['outputTokens']*output_token_mistral_8b\n",
    "\n",
    "        return cost\n",
    "\n",
    "class BedrockLLMWrapper():\n",
    "    def __init__(self,\n",
    "        model_id: str = 'anthropic.claude-3-haiku-20240307-v1:0',\n",
    "        top_k: int = 5,\n",
    "        top_p: int = 0.7,\n",
    "        temperature: float = 0.0,\n",
    "        max_token_count: int = 4000,\n",
    "        max_attempts: int = 3,\n",
    "        debug: bool = False,\n",
    "        region: str ='us-east-1'\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.region = region\n",
    "        self.debug = debug\n",
    "        config = Config(\n",
    "            retries = {\n",
    "                'max_attempts': 10,\n",
    "                'mode': 'standard'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", config=config, region_name=self.region)\n",
    "    \n",
    "    def get_embedding(self, body, modelId, accept, contentType):\n",
    "        response = self.bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding = response_body.get('embedding')\n",
    "        return embedding\n",
    "    \n",
    "    def generate(self,prompt):\n",
    "        if self.debug: \n",
    "            print('entered BedrockLLMWrapper generate')\n",
    "        attempt = 1\n",
    "\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "        messages = []\n",
    "        messages.append(message)\n",
    "        \n",
    "        # model specific inference parameters to use.\n",
    "        if \"anthropic\" in self.model_id.lower():\n",
    "            # system_prompts = [{\"text\": \"You are a helpful AI Assistant.\"}]\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                                \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "                                \"topP\": self.top_p,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "        else:\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "\n",
    "        if self.debug: \n",
    "            print(\"Sending:\\nSystem:\\n\",str(system_prompts),\"\\nMessages:\\n\",str(messages))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Send the message.\n",
    "                response = self.bedrock_runtime.converse(\n",
    "                    modelId=self.model_id,\n",
    "                    messages=messages,\n",
    "                    system=system_prompts,\n",
    "                    inferenceConfig=inference_config,\n",
    "                    additionalModelRequestFields=additional_model_fields\n",
    "                )\n",
    "\n",
    "                # Log token usage.\n",
    "                text = response['output'].get('message').get('content')[0].get('text')\n",
    "                usage = response['usage']\n",
    "                latency = response['metrics'].get('latencyMs')\n",
    "\n",
    "                if self.debug: \n",
    "                    print(f'text: {text} ; and token usage: {usage} ; and query_time: {latency}')    \n",
    "                \n",
    "                break\n",
    "               \n",
    "            except Exception as e:\n",
    "                print(\"Error with calling Bedrock: \"+str(e))\n",
    "                attempt+=1\n",
    "                if attempt>self.max_attempts:\n",
    "                    print(\"Max attempts reached!\")\n",
    "                    result_text = str(e)\n",
    "                    break\n",
    "                else:#retry in 10 seconds\n",
    "                    print(\"retry\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "        # return result_text\n",
    "        return [text,usage,latency]\n",
    "\n",
    "    # Threaded function for queue processing.\n",
    "    def thread_request(self, q, results):\n",
    "        while True:\n",
    "            try:\n",
    "                index, prompt = q.get(block=False)\n",
    "                data = self.generate(prompt)\n",
    "                results[index] = data\n",
    "            except Queue.Empty:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'Error with prompt: {str(e)}')\n",
    "                results[index] = str(e)\n",
    "            finally:\n",
    "                q.task_done()\n",
    "\n",
    "    def generate_threaded(self, prompts, max_workers=15):\n",
    "        results = [None] * len(prompts)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_index = {executor.submit(self.generate, prompt): i for i, prompt in enumerate(prompts)}\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {exc}')\n",
    "                    results[index] = str(exc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class AnswerTaskRunner:\n",
    "    def __init__(self, eval_df: pd.DataFrame, \n",
    "                 model_id:str = 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "                 eval_model_id:str = 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "                 temperature: float = 0.0,\n",
    "                 max_token_count: int = 2000,\n",
    "                 max_attempts: int = 3, \n",
    "                 prompt_template: str = '',\n",
    "                 prompt_eval_template: str = ''):\n",
    "        self.eval_df = eval_df\n",
    "        self.model_id = model_id\n",
    "        self.eval_model_id = eval_model_id\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.prompt_template = prompt_template\n",
    "        self.prompt_eval_template = prompt_eval_template\n",
    "        self.wrapper = BedrockLLMWrapper(model_id=self.model_id, \n",
    "                                         max_token_count=self.max_token_count,\n",
    "                                         temperature=self.temperature\n",
    "                                         )\n",
    "        self.eval_wrapper = BedrockLLMWrapper(model_id=self.eval_model_id, \n",
    "                                         max_token_count=self.max_token_count,\n",
    "                                         temperature=self.temperature\n",
    "                                         )\n",
    "\n",
    "    def get_prompt(self, question, context_list):\n",
    "        context = context_list\n",
    "        prompt = self.prompt_template.format(question=question, context=context)\n",
    "        return prompt\n",
    "\n",
    "    def build_grader_prompt(self, original_query: str, llm_system_response: str, ground_truth_answer:str, context_list, answer_relevancy:str):\n",
    "        context = context_list\n",
    "\n",
    "        prompt = self.prompt_eval_template.format(\n",
    "                        original_query= original_query,\n",
    "                        llm_system_response= llm_system_response,\n",
    "                        ground_truth_answer=ground_truth_answer,\n",
    "                        context=context,\n",
    "                        answer_relevancy = answer_relevancy,\n",
    "                        \n",
    "                    ) \n",
    "        return prompt\n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(self.eval_df)\n",
    "        util = Util()\n",
    "        \n",
    "        # Prepare prompts for answer generation\n",
    "        answer_prompts = [self.get_prompt(row['query_text'], row['context']) for _, row in df.iterrows()]\n",
    "        \n",
    "        # Generate answers in parallel\n",
    "        answer_responses = self.wrapper.generate_threaded(answer_prompts)\n",
    "        \n",
    "        eval_prompts = []\n",
    "        for (_, row), response in zip(df.iterrows(), answer_responses):\n",
    "            query = row['query_text']\n",
    "            groundtruth_answer = row['question_answer']\n",
    "            retrieved_chunks = row['context']\n",
    "            \n",
    "            generated_answer = util.extract_with_regex(str(response[0]), util.ANSWER_PATTERN)\n",
    "            \n",
    "            answer_relevancy = util.calculate_answer_relevance(query, generated_answer) if generated_answer else 0\n",
    "            \n",
    "            eval_prompts.append(self.build_grader_prompt(query, generated_answer, groundtruth_answer, retrieved_chunks, answer_relevancy))\n",
    "        \n",
    "        # Generate evaluations in parallel\n",
    "        eval_responses = self.eval_wrapper.generate_threaded(eval_prompts)\n",
    "        \n",
    "        results = []\n",
    "        for (_, row), answer_response, eval_response in zip(df.iterrows(), answer_responses, eval_responses):\n",
    "            query = row['query_text']\n",
    "            groundtruth_answer = row['question_answer']\n",
    "            retrieved_chunks = row['context']\n",
    "            \n",
    "            generated_answer = util.extract_with_regex(str(answer_response[0]), util.ANSWER_PATTERN)\n",
    "            \n",
    "            reasoning = util.extract_with_regex(str(eval_response[0]), util.REASONING_PATTERN)\n",
    "            score = util.extract_with_regex(str(eval_response[0]), util.SCORE_PATTERN)\n",
    "            \n",
    "            answer_relevancy = util.calculate_answer_relevance(query, generated_answer) if generated_answer else 0\n",
    "            faithfulness = util.calculate_faithfulness(generated_answer, retrieved_chunks) if generated_answer else 0\n",
    "            \n",
    "            cost = util.calculate_cost(answer_response[1], self.model_id)\n",
    "            \n",
    "            result = {\n",
    "                'query_text': query,\n",
    "                'groundtruth_answer': groundtruth_answer,\n",
    "                'retrieved_chunks': json.dumps(retrieved_chunks),\n",
    "                'generated_answer': generated_answer,\n",
    "                'usage': json.dumps(answer_response[1]),\n",
    "                'latency': answer_response[2],\n",
    "                'reasoning': str(reasoning),\n",
    "                'score': score,\n",
    "                'faithfulness': faithfulness,\n",
    "                'answer_relevancy': answer_relevancy,\n",
    "                'cost': cost,\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a395ade",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b915cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Evaluation Template\n",
    "## this template could be improved by adding eval examples.\n",
    "\n",
    "prompt_eval_template = \"\"\"You are an expert judge evaluating the Retrieval Augmented Generation applications.\n",
    "                   Your task is to evaluate a given answer based on a context and question using the criteria provided below.\n",
    " \n",
    "                    Evaluation Criteria (Additive Score, 0-3):\n",
    "                    1. Context: Award 1 point if the answer uses only information provided in the context, without introducing external or fabricated details.\n",
    "                    2. Completeness: Add 1 point if the answer addresses all key elements of the question based on the available context, without omissions.\n",
    "                    3. Conciseness: Add a final point if the answer uses the fewest words possible to address the question and avoids redundancy.\n",
    "                    \n",
    "                    Evaluation Steps:\n",
    "                    1. Read provided context, question and answer carefully.\n",
    "                    2. Go through each evaluation criterion one by one and assess whether the answer meets the criteria.\n",
    "                    3. Compose your reasoning for each critera, explaining why you did or did not award a point. You can only award full points. \n",
    "                    4. Calculate the total score by summing the points awarded.\n",
    "                    5. Think through the evaluation criteria inside <thinking></thinking> tags. \n",
    "                    Then, output the total score inside <score></score> tags.\n",
    "                    Review your formatted response. It needs to be valid XML.\n",
    "\n",
    "                    Now, please evaluate the following:\n",
    "\n",
    "                    Question:\n",
    "                    <original_query>\n",
    "                    {original_query}\n",
    "                    </original_query>\n",
    "\n",
    "                    Generated answer:\n",
    "                    <llm_system_response>\n",
    "                    {llm_system_response}\n",
    "                    </llm_system_response>\n",
    "\n",
    "                    Ground truth answer:\n",
    "                    <ground_truth_answer>\n",
    "                    {ground_truth_answer}\n",
    "                    </ground_truth_answer>\n",
    "\n",
    "                    Context:\n",
    "                    <context>\n",
    "                    {context}\n",
    "                    </context>\n",
    "\n",
    "                    Here is the answer_relevancy score based of the original question and generated questions based of the generated answer.\n",
    "                    <answer_relevancy>\n",
    "                    {answer_relevancy}\n",
    "                    </answer_relevancy>\n",
    "                    The answer_relevancy score range between 0 and 1 where higher scores indicate better relevancy.\n",
    "                \n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c63ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_PATH = '../data/eval-datasets/4_answer_validation_opensearch.jsonl'\n",
    "answer_eval_df = pd.read_json(EVAL_PATH,lines=True)\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "\n",
    "# Template 1: Long prompt with XML tags context chunks, and step by step instructions, and reiterating how the response should be provided in the end.\n",
    "prompt_template_claude_1 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest research assistant, dedicated to providing valuable and accurate information.\n",
    "        You will be provided with a report extract between <report></report> XML tags, please read it and analyse the content.\n",
    "        Please answer the following question: \n",
    "        {question} \n",
    "        \n",
    "        The answer must only be based on the information from the context below.\n",
    "\n",
    "        If a particular bit of information is not present, return \"There is not enough information available to answer this question\" inside the XML tags.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The context will be given between <context></context> XML tags.\n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "answer_results1: pd.DataFrame = AnswerTaskRunner(answer_eval_df,model_id=MODEL_ID, prompt_template=prompt_template_claude_1, prompt_eval_template=prompt_eval_template).run()\n",
    "answer_results1.to_json('../data/eval-datasets/4_answer_validation_opensearch_graded1.jsonl', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c2f5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template 2: InContextLearning prompt with example interaction between Human and Assistant\n",
    "prompt_template_claude_2 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest research assistant, dedicated to providing valuable and accurate information.\n",
    "\n",
    "        Assistant:\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        Human:\n",
    "        If you don't see answer in the context just reply \"There is not enough information available to answer this question\" in XML tags.\n",
    "\n",
    "        Assistant:\n",
    "        Noted. I will respond with \"There is not enough information available to answer this question\" if the information is not available in the context.\n",
    "\n",
    "        Human:\n",
    "        Return the answer inside <question_answer></question_answer> XML tags. \n",
    "\n",
    "        Assistant:\n",
    "        I will provide the answer inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        Human:\n",
    "        Now read and analyse the context provided in <context></context> XML tags carefully.\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "        Then answer the below question:\n",
    "        {question}\n",
    "\n",
    "        Assistant:\n",
    "        \"\"\"\n",
    "answer_results2: pd.DataFrame = AnswerTaskRunner(answer_eval_df,model_id=MODEL_ID, prompt_template=prompt_template_claude_2, prompt_eval_template=prompt_eval_template).run()\n",
    "answer_results2.to_json('../data/eval-datasets/4_answer_validation_opensearch_graded2.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734304de",
   "metadata": {},
   "source": [
    "### Review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31aa863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util = Util()\n",
    "util.compare_results(answer_results1, answer_results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d4a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_results1 already exists in memory.\n",
      "answer_results2 already exists in memory.\n",
      "Percentage correct for answer_results1: 79.17%\n",
      "Percentage correct for answer_results2: 41.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>query_text</th>\n",
       "      <th>groundtruth_answer</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>usage</th>\n",
       "      <th>latency</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>score</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?</td>\n",
       "      <td>&lt;question_answer&gt;\\nThe context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.\\n&lt;/question_answer&gt;</td>\n",
       "      <td>The context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by a nested field and calculating metrics on a parent field in the same query.</td>\n",
       "      <td>{\"inputTokens\": 894, \"outputTokens\": 91, \"totalTokens\": 985}</td>\n",
       "      <td>2290</td>\n",
       "      <td>Context:\\n1. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent. However, it does not specifically cover how to group data by a nested field while calculating metrics on a parent-level field in the same query. Therefore, I will not award a point for this criterion.\\n\\nCompleteness:\\n2. The generated answer correctly states that the context does not provide enough information to answer the original question about grouping by a nested field and calculating parent-level metrics in the same query. Since the answer addresses all key elements of the question based on the available context, I will award 1 point for completeness.\\n\\nConciseness:\\n3. The generated answer is concise and uses the fewest words possible to address the question without redundancy. Therefore, I will award 1 point for conciseness.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?</td>\n",
       "      <td>&lt;question_answer&gt;\\nThe specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.\\n&lt;/question_answer&gt;</td>\n",
       "      <td>The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\\n\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n\\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.</td>\n",
       "      <td>{\"inputTokens\": 5220, \"outputTokens\": 165, \"totalTokens\": 5385}</td>\n",
       "      <td>5039</td>\n",
       "      <td>Context:\\n1. The context does not provide any information about the specific Java permissions required to register and unregister MBeans when installing the anomaly detection plugin. Therefore, I cannot award a point for this criterion.\\n\\nCompleteness:\\n2. The generated answer provides the exact Java permissions required to register and unregister MBeans for the org.apache.commons.pool2.impl.GenericObjectPool object when installing the anomaly detection plugin. It addresses all key elements of the question based on the available context. I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The generated answer is concise and uses the fewest words possible to address the question without redundancy. I will award 1 point for this criterion.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.018135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I have a model and some ML nodes, how do I boost it's performance?</td>\n",
       "      <td>&lt;question_answer&gt;\\nTo boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.\\n&lt;/question_answer&gt;</td>\n",
       "      <td>There is not enough information available to answer this question. The context provides details on how to use GPU acceleration and upload custom models to OpenSearch, but does not mention how to boost the performance of an existing model and ML nodes.</td>\n",
       "      <td>{\"inputTokens\": 9100, \"outputTokens\": 61, \"totalTokens\": 9161}</td>\n",
       "      <td>4713</td>\n",
       "      <td>Context:\\n1. Award 1 point if the answer uses only information provided in the context, without introducing external or fabricated details.\\nThe generated answer does not introduce any external or fabricated details, and only uses information from the provided context. Therefore, I will award 1 point for this criterion.\\n\\n2. Add 1 point if the answer addresses all key elements of the question based on the available context, without omissions.\\nThe question asks how to boost the performance of an ML model and some ML nodes. The answer correctly identifies that the context provides details on GPU acceleration for ML nodes, which can help boost performance. However, it does not address boosting performance of the model itself. Therefore, I will not award a point for this criterion.\\n\\n3. Add a final point if the answer uses the fewest words possible to address the question and avoids redundancy.\\nThe answer is concise and does not contain redundant information, using the fewest words necessary to convey the relevant information from the context. I will award 1 point for this criterion.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.028215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>query_text</th>\n",
       "      <th>groundtruth_answer</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>usage</th>\n",
       "      <th>latency</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>score</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?</td>\n",
       "      <td>&lt;question_answer&gt;\\nTo update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/&lt;config_id&gt; endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT /_plugins/_notifications/configs/&lt;config_id&gt;\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/new-webhook-url\"\\n    }\\n  }\\n}\\n&lt;/question_answer&gt;</td>\n",
       "      <td>To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the /_plugins/_notifications/configs/&lt;config_id&gt; endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\\n\\nPUT /_plugins/_notifications/configs/&lt;config_id&gt;\\n{\\n  \"config\": {\\n    \"name\": \"Slack Channel\",\\n    \"description\": \"This is an updated channel configuration\",\\n    \"config_type\": \"slack\",\\n    \"is_enabled\": true,\\n    \"slack\": {\\n      \"url\": \"https://hooks.slack.com/sample-url\"\\n    }\\n  }\\n}</td>\n",
       "      <td>{\"inputTokens\": 3560, \"outputTokens\": 172, \"totalTokens\": 3732}</td>\n",
       "      <td>4187</td>\n",
       "      <td>Context:\\n1. The generated answer provides the correct steps to update an existing Slack channel configuration using the Notifications API, including sending a PUT request to the /_plugins/_notifications/configs/&lt;config_id&gt; endpoint with the updated webhook URL and description in the request body. This aligns with the information provided in the context, so I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The generated answer addresses all key elements of the question by specifying how to update the webhook URL and description for an existing Slack channel configuration. No important details are omitted based on the context provided, so I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The generated answer provides a concise example request with only the necessary details to update the webhook URL and description. It does not include any redundant or extraneous information, so I will award 1 point for this criterion.</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.013260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you explain compound queries to me?</td>\n",
       "      <td>&lt;question_answer&gt;\\nCompound queries are used to combine multiple leaf or compound queries together using Boolean logic or other operations. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Wraps a query and assigns a constant relevance score to all matching documents\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for each document\\n&lt;/question_answer&gt;</td>\n",
       "      <td>Compound queries are used to combine multiple leaf or compound queries together using Boolean logic or other operations. Some examples of compound queries include:\\n\\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\\n- constant_score: Wraps a query and assigns a constant relevance score to all matching documents\\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\\n- function_score: Recalculates the relevance score of documents using a custom function\\n- hybrid: Combines relevance scores from multiple queries into one score for each document</td>\n",
       "      <td>{\"inputTokens\": 730, \"outputTokens\": 174, \"totalTokens\": 904}</td>\n",
       "      <td>2979</td>\n",
       "      <td>Context:\\n1. The context provides a detailed explanation of compound queries and lists different types of compound queries like bool, boosting, constant_score, dis_max, function_score, and hybrid. The generated answer covers all these types of compound queries mentioned in the context, so it gets 1 point for using only information from the context.\\n\\nCompleteness:\\n2. The question asks to explain compound queries. The generated answer provides a good overview of what compound queries are and lists different types with brief descriptions. It addresses all key elements of the question based on the context, so it gets 1 point for completeness.\\n\\nConciseness:\\n3. The generated answer is concise and uses the fewest words possible to explain compound queries and list the different types. It avoids redundancy, so it gets 1 point for conciseness.\\n\\nTotal score: 3 (1 + 1 + 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?</td>\n",
       "      <td>&lt;question_answer&gt;\\nTo set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\\n- Configure cache expiration policies to evict stale entries.\\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.\\n&lt;/question_answer&gt;</td>\n",
       "      <td>To set up a tiered cache with an on-heap and disk tier for the index request cache:\\n\\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\\n\\nTo prevent the tiered cache from consuming excessive resources, you can:\\n- Limit the on-heap cache size using `indices.request.cache.opensearch_onheap.size`\\n- Limit the disk cache size using `indices.request.cache.ehcache_disk.max_size_in_bytes`\\n- Set appropriate expiration times for cached entries using `indices.request.cache.opensearch_onheap.expire` and `indices.request.cache.ehcache_disk.expire_after_access`</td>\n",
       "      <td>{\"inputTokens\": 3352, \"outputTokens\": 340, \"totalTokens\": 3692}</td>\n",
       "      <td>9161</td>\n",
       "      <td>Context:\\n1. The generated answer provides detailed steps on how to set up a tiered cache with an on-heap and disk tier for the index request cache. It covers enabling the pluggable caching feature, installing the required plugin, setting the cache store name and tiers, and configuring the on-heap and disk store settings. This information is present in the context, so I will award 1 point for this criterion.\\n\\nCompleteness:\\n2. The generated answer addresses all key elements of the question, including setting up the tiered cache and tweaking settings to prevent excessive resource consumption. It provides specific settings to limit the cache sizes and configure expiration policies. This covers the question comprehensively based on the available context, so I will award 1 point for this criterion.\\n\\nConciseness:\\n3. The generated answer is concise and focused, providing the necessary information without redundancy or extraneous details. It uses a clear and structured format to convey the steps and settings. Therefore, I will award 1 point for this criterion.\\n\\nTotal score: 3 (1 + 1 + 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.015156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial review of grades from LLM-as-a-Judge\n",
    "\n",
    "# Check if answer_results1 exists in the current namespace\n",
    "if 'answer_results1' not in globals():\n",
    "    # If it doesn't exist, try to load it from a JSONL file\n",
    "    file_path = '../data/eval-datasets/4_answer_validation_opensearch_graded1.jsonl'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Load the dataframe from the JSONL file\n",
    "        df1_graded = pd.read_json(file_path, lines=True)\n",
    "        print(\"answer_results1 loaded from JSONL file.\")\n",
    "    else:\n",
    "        print(f\"Error: JSONL file not found at {file_path}\")\n",
    "else:\n",
    "    print(\"answer_results1 already exists in memory.\")\n",
    "    \n",
    "# Check if answer_results2 exists in the current namespace\n",
    "if 'answer_results2' not in globals():\n",
    "    # If it doesn't exist, try to load it from a JSONL file\n",
    "    file_path = '../data/eval-datasets/4_answer_validation_opensearch_graded2.jsonl'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Load the dataframe from the JSONL file\n",
    "        df1_graded = pd.read_json(file_path, lines=True)\n",
    "        print(\"answer_results2 loaded from JSONL file.\")\n",
    "    else:\n",
    "        print(f\"Error: JSONL file not found at {file_path}\")\n",
    "else:\n",
    "    print(\"answer_results2 already exists in memory.\")\n",
    "\n",
    "\n",
    "answer_results1['score'] = answer_results1['score'].astype('int64')\n",
    "answer_results2['score'] = answer_results2['score'].astype('int64')\n",
    "\n",
    "def calculate_percentage_correct(df, threshold=2):\n",
    "    total_count = len(df)\n",
    "    correct_count = len(df[df['score'] > threshold])\n",
    "    return (correct_count / total_count) * 100\n",
    "\n",
    "percentage_correct1 = calculate_percentage_correct(answer_results1)\n",
    "print(f\"Percentage correct for answer_results1: {percentage_correct1:.2f}%\")\n",
    "\n",
    "percentage_correct2 = calculate_percentage_correct(answer_results2)\n",
    "print(f\"Percentage correct for answer_results2: {percentage_correct2:.2f}%\")\n",
    "\n",
    "# sample a subsection of 3 incorrect responses for answer_results1\n",
    "from IPython.display import display, HTML\n",
    "sample_size1 = 3 if len(answer_results1[(answer_results1['score'] <= 2)]) > 3 else len(answer_results1[(answer_results1['score'] <= 2)])\n",
    "incorrect_rows = answer_results1[(answer_results1['score'] <= 2)].sample(n=sample_size1)\n",
    "incorrect_rows_no_context = incorrect_rows.drop(columns=['retrieved_chunks'])\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = incorrect_rows_no_context.to_html(index=False, classes='table table-striped')\n",
    "display(HTML(table_html))\n",
    "\n",
    "# sample a subsection of 3 correct responses for answer_results1\n",
    "sample_size2 = 3 if len(answer_results1[(answer_results1['score'] > 2)]) > 3 else len(answer_results1[(answer_results1['score'] > 2)])\n",
    "correct_rows = answer_results1[(answer_results1['score'] > 2)].sample(n=sample_size2)\n",
    "correct_rows_no_context = correct_rows.drop(columns=['retrieved_chunks'])\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = correct_rows_no_context.to_html(index=False, classes='table table-striped')\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f14b1e-b99d-468f-a574-5b7b44251eac",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* Iterate and ensure you have a sufficient # of ground truth data \n",
    "* Ensure you run a sufficient # of evaluation runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb0999",
   "metadata": {},
   "source": [
    "# Things to improve\n",
    "* Using human-labeled few-shot examples allows you to align your LLM Judge with a human expert at almost 0 cost.\n",
    "* Dynamically inject in relevant examples from VectorDB into prompt\n",
    "\n",
    "# Sources\n",
    "* 'LLM Evaluation doesn't need to be complicated' Blog: https://www.philschmid.de/llm-evaluation\n",
    "* RAG System evaluation with RAGAS: https://github.com/fhuthmacher/aws-rag-system-eval/tree/main & https://www.youtube.com/watch?v=gNY06JM6m8U\n",
    "* RAGChecker: A Fine-grained Framework For Diagnosing RAG: https://github.com/amazon-science/RAGChecker\n",
    "* Observability and Evaluation Custom Solution for Amazon Bedrock Applications: https://github.com/aws-samples/amazon-bedrock-samples/tree/main/evaluation-and-observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d0a4a",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4554140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_job: arn:aws:bedrock:us-east-1:026459568683:evaluation-job/7dlb2yforjp0\n"
     ]
    }
   ],
   "source": [
    "# Bedrock LLM eval example\n",
    "\n",
    "import boto3\n",
    "import secrets\n",
    "import string\n",
    "\n",
    "bedrock_client = boto3.client('bedrock',region_name=REGION)\n",
    "sagemaker_client = boto3.client('sagemaker',region_name=REGION)\n",
    "\n",
    "# Generate the random string\n",
    "random_string = ''.join(secrets.choice(string.ascii_letters + string.digits) for _ in range(6)).lower()\n",
    "\n",
    "create_flow_definition_response = sagemaker_client.create_flow_definition(\n",
    "    FlowDefinitionName= f'human-eval-task-{random_string}',\n",
    "    HumanLoopRequestSource={\n",
    "        'AwsManagedHumanLoopRequestSource': 'AWS/Bedrock/Evaluation'\n",
    "    },\n",
    "\n",
    "    HumanLoopConfig={\n",
    "        'WorkteamArn': WORKTEAM_ARN,\n",
    "        'HumanTaskUiArn': f'arn:aws:sagemaker:{REGION}:394669845002:human-task-ui/Evaluation',\n",
    "        'TaskTitle': 'Human review tasks',\n",
    "        'TaskDescription': 'Determine best answer',\n",
    "        'TaskCount': 1,\n",
    "        'TaskAvailabilityLifetimeInSeconds': 864000,\n",
    "        'TaskTimeLimitInSeconds': 3600,\n",
    "        'TaskKeywords': [\n",
    "            'evaluation',\n",
    "        ],\n",
    "        \n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputPath': f's3://{S3_BUCKET_NAME}/eval',\n",
    "    },\n",
    "    RoleArn=SAGEMAKER_ROLE_ARN,\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'eval',\n",
    "            'Value': 'human-eval'\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "flow_definition = create_flow_definition_response.get('FlowDefinitionArn')\n",
    "\n",
    "\n",
    "\n",
    "create_evaluation_job_response = bedrock_client.create_evaluation_job(\n",
    "    jobName=f'human-eval-{random_string}',\n",
    "    jobDescription=\"Evaluate answer generated with different prompt templates\",\n",
    "    roleArn=SAGEMAKER_ROLE_ARN,\n",
    "    inferenceConfig={\n",
    "        # array of models to be evaluated\n",
    "        \"models\": [\n",
    "            {\n",
    "                \"bedrockModel\": {\n",
    "                    \"modelIdentifier\":f\"arn:aws:bedrock:{REGION}::foundation-model/mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "                    \"inferenceParams\":\"{\\\"temperature\\\":\\\"0.0\\\", \\\"topP\\\":\\\"1\\\", \\\"maxTokenCount\\\":\\\"512\\\"}\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            # {\n",
    "            #     \"bedrockModel\": {\n",
    "            #         \"modelIdentifier\":f\"arn:aws:bedrock:{region}::foundation-model/amazon.titan-text-lite-v1\",\n",
    "            #         \"inferenceParams\":\"{\\\"temperature\\\":\\\"0.25\\\", \\\"topP\\\":\\\"1\\\", \\\"maxTokenCount\\\":\\\"256\\\"}\"\n",
    "            #     }\n",
    "\n",
    "            # },\n",
    "            \n",
    "        ]\n",
    "\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\":f\"s3://{S3_BUCKET_NAME}/evalresults/\"\n",
    "    },\n",
    "    evaluationConfig={\n",
    "        \"human\": {\n",
    "        \"humanWorkflowConfig\": {\n",
    "            \"flowDefinitionArn\": f\"{flow_definition}\",\n",
    "            \"instructions\": \"Review the generated answers\"\n",
    "        },\n",
    "        \"customMetrics\": [\n",
    "            {\n",
    "                \"name\": \"HumanPreference\",\n",
    "                \"description\": \"human preference\",\n",
    "                \"ratingMethod\": \"IndividualLikertScale\"\n",
    "            }\n",
    "        ],\n",
    "        \"datasetMetricConfigs\": [\n",
    "            {\n",
    "                \"taskType\": \"Generation\",\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"Custom_Dataset1\",\n",
    "                    \"datasetLocation\": {\n",
    "                        \"s3Uri\": f\"s3://{S3_BUCKET_NAME}/eval/data.jsonl\"\n",
    "                    }\n",
    "                },\n",
    "                \"metricNames\": [\n",
    "                  \"HumanPreference\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "      }\n",
    "\n",
    "    }\n",
    ")\n",
    "eval_job = create_evaluation_job_response.get('jobArn')\n",
    "print(f'eval_job: {eval_job}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
