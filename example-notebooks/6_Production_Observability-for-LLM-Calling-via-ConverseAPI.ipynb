{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00307a86",
   "metadata": {},
   "source": [
    "# Observability in Production PLACEHOLDER\n",
    "\n",
    "TBD\n",
    "\n",
    "In this notebook, we'll demonstrates how to implement observability for the Amazon Bedrock Converse API in production using the custom observability solution. We'll use the `BedrockLogs` class from the `observability` module to track and log API calls, responses, and user feedback.\n",
    "\n",
    "### Prerequisite\n",
    "After successfully setting up the backend resources required using the provided `CloudFormation template` to gather necessary data on user requests, your custom metadata like latency, time to first token, tags, model responses, citations, and any other custom identifiers you would like to add (e.g., user_id/customer_id), you can now test if your observability architecture is working as expected and determine the latency introduced by adding this additional component to your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b62f8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ruamel.yaml in /Users/huthmac/.pyenv/versions/3.11.7/envs/llm-system-eval/lib/python3.11/site-packages (0.18.6)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /Users/huthmac/.pyenv/versions/3.11.7/envs/llm-system-eval/lib/python3.11/site-packages (from ruamel.yaml) (0.2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992e2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy prerequisites \n",
    "# IAM role, S3 bucket, Firehose\n",
    "\n",
    "import boto3\n",
    "import datetime\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "from io import StringIO\n",
    "\n",
    "yaml = YAML()\n",
    "with open('../cf/prod-observability.yaml', 'r') as file:\n",
    "    yaml_data = yaml.load(file)\n",
    "\n",
    "cloudformation_client = boto3.client('cloudformation')\n",
    "\n",
    "stream = StringIO()\n",
    "yaml.dump(yaml_data, stream)\n",
    "\n",
    "template_body = stream.getvalue()\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "date_string = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "stack_name = 'observability-stack' + date_string\n",
    "\n",
    "response = cloudformation_client.create_stack(\n",
    "    StackName=stack_name,\n",
    "    TemplateBody=template_body,\n",
    "    Parameters=[\n",
    "        {\n",
    "            'ParameterKey': 'DateString',\n",
    "            'ParameterValue': date_string\n",
    "        }\n",
    "    ],\n",
    "    Capabilities=['CAPABILITY_IAM', 'CAPABILITY_NAMED_IAM']\n",
    ")\n",
    "\n",
    "cloudformation_client.get_waiter('stack_create_complete').wait(StackName=stack_name)\n",
    "\n",
    "response = cloudformation_client.describe_stacks(StackName=stack_name)\n",
    "output_dict = {}\n",
    "\n",
    "for output in response['Stacks'][0]['Outputs']:\n",
    "    output_dict[output['OutputKey']] = output['OutputValue']\n",
    "\n",
    "output_dict_select = {\"FIREHOSE_NAME\": output_dict[\"FirehoseDeliveryStreamName\"], \n",
    "                      \"CRAWLER_NAME\": output_dict[\"GlueCrawlerName\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ea2a3",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a02c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 'CRAWLER_NAME',\n",
       " 'observability-026459568683-glue-crawler-20240828225635')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import string\n",
    "import random\n",
    "from uuid import uuid4\n",
    "\n",
    "# Custom observability module\n",
    "from observability import BedrockLogs\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import os\n",
    "\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'llm-system-eval.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "REGION = os.environ['REGION']\n",
    "os.environ['GUARDRAIL_ID'] = os.getenv('GUARDRAIL_ID')\n",
    "GUARDRAIL_ID = os.environ['GUARDRAIL_ID']\n",
    "os.environ['GUARDRAIL_VERSION'] = os.getenv('GUARDRAIL_VERSION')\n",
    "GUARDRAIL_VERSION = os.environ['GUARDRAIL_VERSION']\n",
    "os.environ['CUSTOM_TAG'] = os.getenv('CUSTOM_TAG')\n",
    "CUSTOM_TAG = os.environ['CUSTOM_TAG']\n",
    "os.environ['EXPERIMENT_ID'] = os.getenv('EXPERIMENT_ID')\n",
    "EXPERIMENT_ID = os.environ['EXPERIMENT_ID']\n",
    "\n",
    "FIREHOSE_NAME = output_dict[\"FirehoseDeliveryStreamName\"]\n",
    "CRAWLER_NAME = output_dict[\"GlueCrawlerName\"]\n",
    "\n",
    "# Update environment variables with the resource references we created in the prerequisites\n",
    "os.environ['FIREHOSE_NAME'] = FIREHOSE_NAME\n",
    "os.environ['CRAWLER_NAME'] = CRAWLER_NAME\n",
    "\n",
    "set_key(local_env_filename, 'FIREHOSE_NAME', FIREHOSE_NAME)\n",
    "set_key(local_env_filename, 'CRAWLER_NAME', CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0446c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observability and Evaluation Custom Solution for Amazon Bedrock Applications\n",
    "import pytz\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from uuid import uuid4\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "class BedrockLogs:\n",
    "    VALID_FEATURE_NAMES = [\"None\", \"Agent\", \"KB\", \"InvokeModel\"]\n",
    "\n",
    "    def __init__(self, delivery_stream_name: str = None, \n",
    "                 experiment_id: str = None, \n",
    "                 default_call_type: str = 'LLM', \n",
    "                 feature_name: str = None, \n",
    "                 feedback_variables: bool = False\n",
    "                ):\n",
    "        self.delivery_stream_name = delivery_stream_name\n",
    "        self.experiment_id = experiment_id\n",
    "        self.default_call_type = default_call_type\n",
    "        self.feedback_variables = feedback_variables\n",
    "\n",
    "        if feature_name is not None:\n",
    "            if feature_name not in BedrockLogs.VALID_FEATURE_NAMES:\n",
    "                raise ValueError(f\"Invalid feature_name '{feature_name}'. Valid values are: {', '.join(BedrockLogs.VALID_FEATURE_NAMES)}\")\n",
    "        self.feature_name = feature_name\n",
    "        self.step_counter = 0\n",
    "\n",
    "        if self.delivery_stream_name is None:\n",
    "            raise ValueError(\"delivery_stream_name must be provided or set equals to 'local' example: delivery_stream_name='local'.\")\n",
    "\n",
    "        if self.delivery_stream_name == 'local':\n",
    "            self.firehose_client = None\n",
    "        else:\n",
    "            self.firehose_client = boto3.client('firehose')\n",
    "\n",
    "    @staticmethod\n",
    "    def find_keys(dictionary, key, path=[]):\n",
    "        \"\"\"\n",
    "        Recursive function to find all keys in a nested dictionary and their paths.\n",
    "\n",
    "        Args:\n",
    "            dictionary (dict): The dictionary to search.\n",
    "            key (str): The key to search for.\n",
    "            path (list, optional): The path of keys to the current dictionary. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tuples containing the key's path and value.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        if isinstance(dictionary, dict):\n",
    "            for k, v in dictionary.items():\n",
    "                new_path = path + [k]\n",
    "                if k == key:\n",
    "                    results.append((new_path, v))\n",
    "                else:\n",
    "                    results.extend(BedrockLogs.find_keys(v, key, new_path))\n",
    "        elif isinstance(dictionary, list):\n",
    "            for i, item in enumerate(dictionary):\n",
    "                new_path = path + [i]\n",
    "                results.extend(BedrockLogs.find_keys(item, key, new_path))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def extract_session_id(self, log_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the session ID from the log data. If the session ID is not available,\n",
    "        it generates a new UUID for the run ID.\n",
    "\n",
    "        Args:\n",
    "            log_data (Dict[str, Any]): The log data dictionary.\n",
    "\n",
    "        Returns:\n",
    "            str: The session ID or a newly generated UUID if the session ID is not available.\n",
    "        \"\"\"\n",
    "        if self.feature_name == \"Agent\":\n",
    "            session_id_paths = self.find_keys(log_data, 'x-amz-bedrock-agent-session-id')\n",
    "        else:\n",
    "            session_id_paths = self.find_keys(log_data, 'sessionId')\n",
    "\n",
    "        if session_id_paths:\n",
    "            path, session_id = session_id_paths[0]\n",
    "            return session_id\n",
    "        else:\n",
    "            return str(uuid4())\n",
    "\n",
    "    def handle_agent_feature(self, output_data, request_start_time):\n",
    "        \"\"\"\n",
    "        Handles the logic for the 'Agent' feature, including step counting and latency calculation.\n",
    "\n",
    "        Args:\n",
    "            output_data (Any): The output data from the function call.\n",
    "            request_start_time (float): The start time of the request.\n",
    "\n",
    "        Returns:\n",
    "            Any: The updated output data with step numbers and latency information.\n",
    "        \"\"\"\n",
    "        self.session_id = None\n",
    "        prev_trace_time = None\n",
    "        for data in output_data:\n",
    "            if isinstance(data, dict) and 'trace' in data:\n",
    "                trace = data['trace']\n",
    "                if 'start_trace_time' in trace:\n",
    "                    # Check if 'start_trace_time' is defined correctly\n",
    "                    if not isinstance(trace['start_trace_time'], float):\n",
    "                        raise ValueError(\"The key 'start_trace_time' should be present and should be a time.time() object.\")\n",
    "\n",
    "                    # Calculate the latency between traces\n",
    "                    if prev_trace_time is None:\n",
    "                        trace['latency'] = trace['start_trace_time'] - request_start_time\n",
    "                    else:\n",
    "                        trace['latency'] = trace['start_trace_time'] - prev_trace_time\n",
    "\n",
    "                    prev_trace_time = trace['start_trace_time']\n",
    "                    trace['step_number'] = self.step_counter\n",
    "                    self.step_counter += 1\n",
    "                    data['trace'] = trace  # Update the 'trace' dictionary in the original data\n",
    "\n",
    "            elif isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if isinstance(item, dict) and 'start_trace_time' in item:\n",
    "                        # Check if 'start_trace_time' is defined correctly\n",
    "                        if not isinstance(item['start_trace_time'], float):\n",
    "                            raise ValueError(\"The key 'start_trace_time' should be present and should be a time.time() object.\")\n",
    "\n",
    "                        # Calculate the latency between traces\n",
    "                        if prev_trace_time is None:\n",
    "                            item['latency'] = item['start_trace_time'] - request_start_time\n",
    "                        else:\n",
    "                            item['latency'] = item['start_trace_time'] - prev_trace_time\n",
    "\n",
    "                        prev_trace_time = item['start_trace_time']\n",
    "                        item['step_number'] = self.step_counter\n",
    "                        self.step_counter += 1\n",
    "\n",
    "                    elif isinstance(item, dict) and 'trace' in item:\n",
    "                        trace = item['trace']\n",
    "                        if 'start_trace_time' in trace:\n",
    "                            # Check if 'start_trace_time' is defined correctly\n",
    "                            if not isinstance(trace['start_trace_time'], float):\n",
    "                                raise ValueError(\"The key 'start_trace_time' should be present and should be a time.time() object.\")\n",
    "\n",
    "                            # Calculate the latency between traces\n",
    "                            if prev_trace_time is None:\n",
    "                                trace['latency'] = trace['start_trace_time'] - request_start_time\n",
    "                            else:\n",
    "                                trace['latency'] = trace['start_trace_time'] - prev_trace_time\n",
    "\n",
    "                            prev_trace_time = trace['start_trace_time']\n",
    "                            trace['step_number'] = self.step_counter\n",
    "                            self.step_counter += 1\n",
    "                            item['trace'] = trace  # Update the 'trace' dictionary in the original item\n",
    "\n",
    "        return output_data\n",
    "\n",
    "    def watch(self, capture_input: bool = True, capture_output: bool = True, call_type: Optional[str] = None):\n",
    "        def wrapper(func):\n",
    "            def inner(*args, **kwargs):\n",
    "                # For Latency Calculation:\n",
    "                self.request_start_time = time.time()\n",
    "\n",
    "                # Get the function name\n",
    "                function_name = func.__name__\n",
    "\n",
    "                # Capture input if requested\n",
    "                input_data = args if capture_input else None\n",
    "                input_log = None\n",
    "                if input_data:\n",
    "                    input_log = input_data[0]\n",
    "                    \n",
    "                # Generate observation_id\n",
    "                observation_id = str(uuid4())\n",
    "                obs_timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "                # Get the start time\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Calls the function to be executed\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # Capture output if requested\n",
    "                output_data = result if capture_output else None\n",
    "\n",
    "                # Get the end time\n",
    "                end_time = time.time()\n",
    "\n",
    "                # Calculate the duration\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                # Begin Logging Time:\n",
    "                logging_start_time = time.time()\n",
    "\n",
    "                # Handle the 'Agent' feature case\n",
    "                if self.feature_name == \"Agent\":\n",
    "                    if output_data is not None:\n",
    "                        output_data = self.handle_agent_feature(output_data, self.request_start_time)\n",
    "                        run_id = self.extract_session_id(output_data[0])\n",
    "                    else:\n",
    "                        run_id = self.extract_session_id(input_log)\n",
    "                else:\n",
    "                    # Extract the session ID from the log or generate a new one\n",
    "                    run_id = self.extract_session_id(input_log)\n",
    "\n",
    "                # Prepare the metadata\n",
    "                metadata = {\n",
    "                    'experiment_id': self.experiment_id,\n",
    "                    'run_id': run_id,\n",
    "                    'observation_id': observation_id,\n",
    "                    'obs_timestamp': obs_timestamp,\n",
    "                    'start_time': datetime.fromtimestamp(start_time, tz=pytz.utc).isoformat(),\n",
    "                    'end_time': datetime.fromtimestamp(end_time, tz=pytz.utc).isoformat(),\n",
    "                    'duration': duration,\n",
    "                    'input_log': input_log,\n",
    "                    'output_log': output_data,\n",
    "                    'call_type': call_type or self.default_call_type,\n",
    "                    'feature_name': self.feature_name,\n",
    "                    'feedback_enabled': self.feedback_variables\n",
    "                }\n",
    "\n",
    "                # Update the metadata with additional_metadata if provided\n",
    "                additional_metadata = kwargs.get('additional_metadata', {})\n",
    "                if additional_metadata:\n",
    "                    metadata.update(additional_metadata)\n",
    "\n",
    "                input_data = kwargs.get('user_prompt', {})\n",
    "                if input_data:\n",
    "                    metadata.update(input_data)\n",
    "                    \n",
    "                # Get the end time\n",
    "                logging_end_time = time.time()\n",
    "\n",
    "                # Calculate the duration\n",
    "                logging_duration = logging_end_time - logging_start_time\n",
    "                metadata['logging_duration'] = logging_duration\n",
    "\n",
    "                # Send the metadata to Amazon Kinesis Data Firehose or return it locally for testing:\n",
    "                if self.delivery_stream_name == 'local':\n",
    "                    if self.feedback_variables:\n",
    "                        print(\"Logs in local mode-with feedback:\")\n",
    "                        return result, metadata, run_id, observation_id\n",
    "                    else:\n",
    "                        print(\"Logs in local mode-without feedback:\")\n",
    "                        return result, metadata\n",
    "                # log to firehose\n",
    "                else:\n",
    "                    firehose_response = self.firehose_client.put_record(\n",
    "                        DeliveryStreamName=self.delivery_stream_name,\n",
    "                        Record={\n",
    "                            'Data': json.dumps(metadata)\n",
    "                        }\n",
    "                    )\n",
    "                    if self.feedback_variables:\n",
    "                        print(\"Logs in S3-with feedback:\")\n",
    "                        return result, run_id, observation_id\n",
    "                    else:\n",
    "                        print(\"Logs in S3-without feedback:\")\n",
    "                        return result\n",
    "\n",
    "            return inner\n",
    "        return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8404aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BedrockLogs in Local mode with feedback variables\n",
    "bedrock_logs = BedrockLogs(delivery_stream_name='local', feedback_variables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a137191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AWS clients\n",
    "boto3_session = boto3.session.Session()\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1e39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate a random session ID\n",
    "def generate_web_session_id(length=16):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choices(characters, k=length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da34765",
   "metadata": {},
   "source": [
    "### Main Function with Observability\n",
    "\n",
    "Let's create our main function that uses the Converse API and is decorated with our observability logger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3da63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bedrock_logs.watch(call_type='Converse-API')\n",
    "def converse_with_model(application_metadata):\n",
    "    model_id = application_metadata['model_arn']\n",
    "    \n",
    "    system_prompts = [{\"text\": \"You are a helpful AI Assistant for Amazon OpenSearch's documentation.\"}]\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": application_metadata['question']}]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inference_config = {\n",
    "        \"temperature\": 0,\n",
    "        \"maxTokens\": 500,\n",
    "        \"topP\": 0.7\n",
    "    }\n",
    "    \n",
    "    guardrail_config = {\n",
    "        \"guardrailIdentifier\": GUARDRAIL_ID,\n",
    "        \"guardrailVersion\": GUARDRAIL_VERSION,\n",
    "        \"trace\": \"enabled\"\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        guardrailConfig=guardrail_config\n",
    "    )\n",
    "    \n",
    "    application_metadata['model_response'] = response\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f33359",
   "metadata": {},
   "source": [
    "### Running the Conversation\n",
    "\n",
    "Now, let's run a conversation with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "975900af",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87367cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs in local mode-with feedback:\n",
      "Model response: OpenSearch 2.1 does not support zstd compression out of the box. The zstd compression codec was introduced in OpenSearch 2.2.\n",
      "\n",
      "In OpenSearch versions prior to 2.2, the available compression codecs are:\n",
      "\n",
      "- deflate\n",
      "- lz4 \n",
      "- none (no compression)\n",
      "\n",
      "To use zstd compression, you need to upgrade to OpenSearch 2.2 or later. The zstd codec provides better compression ratios compared to deflate while maintaining high decompression speeds.\n",
      "\n",
      "After upgrading to 2.2+, you can configure zstd compression for specific indices by setting the \"codec\" parameter when creating an index:\n",
      "\n",
      "```\n",
      "PUT my_index\n",
      "{\n",
      "  \"settings\": {\n",
      "    \"index.codec\": \"zstd\" \n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Or update an existing index's codec:\n",
      "\n",
      "```\n",
      "PUT my_index/_settings\n",
      "{\n",
      "  \"index.codec\": \"zstd\"\n",
      "}\n",
      "```\n",
      "\n",
      "The zstd compression level can also be configured from 1 (fastest) to 22 (maximum compression) using the \"index.codec.zstd.level\" setting.\n",
      "run_id: 267f8eea-d221-4257-9135-c6fb8e309ddd\n",
      "observation_id: 5f263047-7b51-4428-9e72-13a22553b2cc\n"
     ]
    }
   ],
   "source": [
    "application_metadata = {\n",
    "    'webSessionId': generate_web_session_id(),\n",
    "    'userID': 'User-1',\n",
    "    'customTags': CUSTOM_TAG,\n",
    "    'request_time': datetime.now(pytz.utc).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "    'model_arn': 'anthropic.claude-3-sonnet-20240229-v1:0', #'anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    'question': question\n",
    "}\n",
    "\n",
    "response, log, run_id, observation_id = converse_with_model(application_metadata)\n",
    "\n",
    "print(f\"Model response: {response}\")\n",
    "print(f\"run_id: {run_id}\")\n",
    "print(f\"observation_id: {observation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037402d",
   "metadata": {},
   "source": [
    "### Collecting Feedback\n",
    "\n",
    "We'll define two functions for collecting feedback at the observation and session levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c42a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs in local mode-with feedback:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " {'experiment_id': None,\n",
       "  'run_id': '05f1dd74-96be-4b99-874c-21fc18aaf146',\n",
       "  'observation_id': '6c34ae23-5cf1-46bd-a59e-56524bcf069b',\n",
       "  'obs_timestamp': '2024-08-29T03:09:50.018834+00:00',\n",
       "  'start_time': '2024-08-29T03:09:50.018843+00:00',\n",
       "  'end_time': '2024-08-29T03:09:50.018844+00:00',\n",
       "  'duration': 9.5367431640625e-07,\n",
       "  'input_log': {'user_id': 'User-1',\n",
       "   'f_run_id': '267f8eea-d221-4257-9135-c6fb8e309ddd',\n",
       "   'f_observation_id': '5f263047-7b51-4428-9e72-13a22553b2cc',\n",
       "   'actual_feedback': 'Thumbs-up'},\n",
       "  'output_log': None,\n",
       "  'call_type': 'observation-feedback',\n",
       "  'feature_name': None,\n",
       "  'feedback_enabled': True,\n",
       "  'logging_duration': 2.4080276489257812e-05},\n",
       " '05f1dd74-96be-4b99-874c-21fc18aaf146',\n",
       " '6c34ae23-5cf1-46bd-a59e-56524bcf069b')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observation level feedback\n",
    "\n",
    "@bedrock_logs.watch(call_type='observation-feedback')\n",
    "def observation_level_feedback(feedback):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "user_feedback = 'Thumbs-up'\n",
    "observation_feedback_from_front_end = {\n",
    "    'user_id': 'User-1',\n",
    "    'f_run_id': run_id,\n",
    "    'f_observation_id': observation_id,\n",
    "    'actual_feedback': user_feedback\n",
    "}\n",
    "observation_level_feedback(observation_feedback_from_front_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "878e5a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs in local mode-with feedback:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " {'experiment_id': None,\n",
       "  'run_id': '06196278-9196-4cea-bf0b-9ffeb0af92b5',\n",
       "  'observation_id': '1819d959-0f34-4006-a57e-1476d61e5ad0',\n",
       "  'obs_timestamp': '2024-08-29T03:09:53.727146+00:00',\n",
       "  'start_time': '2024-08-29T03:09:53.727151+00:00',\n",
       "  'end_time': '2024-08-29T03:09:53.727152+00:00',\n",
       "  'duration': 9.5367431640625e-07,\n",
       "  'input_log': {'user_id': 'User-1',\n",
       "   'f_run_id': '267f8eea-d221-4257-9135-c6fb8e309ddd',\n",
       "   'actual_feedback': 'Amazing - this is fast and an awesome way to help the customers!'},\n",
       "  'output_log': None,\n",
       "  'call_type': 'session-feedback',\n",
       "  'feature_name': None,\n",
       "  'feedback_enabled': True,\n",
       "  'logging_duration': 2.384185791015625e-05},\n",
       " '06196278-9196-4cea-bf0b-9ffeb0af92b5',\n",
       " '1819d959-0f34-4006-a57e-1476d61e5ad0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session level feedback\n",
    "\n",
    "@bedrock_logs.watch(call_type='session-feedback')\n",
    "def session_level_feedback(feedback):\n",
    "    pass\n",
    "\n",
    "user_feedback = 'Amazing - this is fast and an awesome way to help the customers!'\n",
    "session_feedback_from_front_end = {\n",
    "    'user_id': 'User-1',\n",
    "    'f_run_id': run_id,\n",
    "    'actual_feedback': user_feedback\n",
    "}\n",
    "session_level_feedback(session_feedback_from_front_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36f8a6",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "This notebook demonstrated how to use the customized `observability` module with the AWS Bedrock Converse API. We've shown how to:\n",
    "\n",
    "1. Set up the observability environment \n",
    "2. Initialize the BedrockLogs class\n",
    "2. Create a conversation function with observability\n",
    "3. Run a conversation and collect the response\n",
    "4. Implement feedback collection at both the observation and session level\n",
    "\n",
    "The collected data can be used for analysis, troubleshooting, and improving your application's performance and user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ced5a",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* https://github.com/aws-samples/amazon-bedrock-samples/tree/main/evaluation-and-observability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-system-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
