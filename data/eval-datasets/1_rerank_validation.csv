query_text,relevant_doc_ids,retrieved_doc_ids,retrieved_chunks,precision@1,recall@1,ndcg@1,precision@3,recall@3,ndcg@3,precision@5,recall@5,ndcg@5,precision@10,recall@10,ndcg@10
I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working?,"[""_im-plugin/index-codecs.md"", ""_tuning-your-cluster/performance.md""]","[""_upgrade-to/upgrade-to.md"", ""_benchmark/quickstart.md"", ""_clients/go.md"", ""documentation-website/_benchmark/user-guide/installing-benchmark.md""]","[{""relative_path"": ""_upgrade-to/upgrade-to.md"", ""chunk"": ""Check the `_nodes` summary to verify that all nodes are available and running the expected version:\n\n   ```bash\n   # Security plugin disabled\n   curl -XGET 'localhost:9200/_nodes/_all?pretty=true'\n   # Security plugin enabled\n   curl -XGET -k -u 'admin:' 'https://localhost:9200/_nodes/_all?pretty=true'\n   ```\n\n   Specifically, check the `nodes..version` portion of the response. Also check `_cat/indices?v` for a green status on all indexes.\n\n1. (Rolling) Repeat steps 2--5 until all nodes are using OpenSearch.\n\n1. After all nodes are using the new version, re-enable shard allocation:\n\n   ```json\n   PUT _cluster/settings\n   {\n     \""persistent\"": {\n       \""cluster.routing.allocation.enable\"": \""all\""\n     }\n   }\n   ```""}, {""relative_path"": ""_benchmark/quickstart.md"", ""chunk"": ""For instructions on how to install OpenSearch with Tar, see Installing OpenSearch > Tarball.\n\nOpenSearch Benchmark has not been tested with the Window's distribution of OpenSearch.\n{: .note}\n\nAfter installation, you can verify OpenSearch is running by going to `localhost:9200`. If you're running your cluster with the Security plugin enabled, OpenSearch will expect SSL connections with the username \""admin\"" and password \""admin\"".  However, since the localhost address is not a unique public address, no certificate authority will issue an SSL certificate for it, so certificate checking will need to be disabled using the `-k` option.\n\nUse the following command to verify OpenSearch is running with SSL certificate checks disabled:\n\n```bash\ncurl -k -u admin: https://localhost:9200\t\t\t# the \""-k\"" option skips SSL certificate checks\n\n{\n  \""name\"" : \""147ddae31bf8.opensearch.org\"",\n  \""cluster_name\"" : \""opensearch\"",""}, {""relative_path"": ""_upgrade-to/upgrade-to.md"", ""chunk"": ""(Optional) Add your certificates to your `config` directory, add them to `opensearch.yml`, and initialize the Security plugin.\n\n1. Start OpenSearch on the node (rolling) or all nodes (cluster restart).\n\n   For the tarball, run `./bin/opensearch -d`.\n\n1. Wait for the OpenSearch node to rejoin the cluster (rolling) or for the cluster to start (cluster restart). Check the `_nodes` summary to verify that all nodes are available and running the expected version:\n\n   ```bash\n   # Security plugin disabled\n   curl -XGET 'localhost:9200/_nodes/_all?pretty=true'\n   # Security plugin enabled\n   curl -XGET -k -u 'admin:' 'https://localhost:9200/_nodes/_all?pretty=true'\n   ```\n\n   Specifically, check the `nodes..version` portion of the response. Also check `_cat/indices?v` for a green status on all indexes.\n\n1.""}, {""relative_path"": ""_clients/go.md"", ""chunk"": ""client, err := opensearch.NewClient(opensearch.Config{\n        Transport: &http.Transport{\n            TLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n        },\n        Addresses: []string{\""https://localhost:9200\""},\n        Username:  \""admin\"", // For testing only. Don't store credentials in code.\n        Password:  \""admin\"",\n    })\n    if err != nil {\n        fmt.Println(\""cannot initialize\"", err)\n        os.Exit(1)\n    }\n\n    // Print OpenSearch version information on console.\n    fmt.Println(client.Info())\n\n    // Define index settings.\n    settings := strings.NewReader(`{\n     'settings': {\n       'index': {\n            'number_of_shards': 1,\n            'number_of_replicas': 2\n            }\n          }\n     }`)\n\n    // Create an index with non-default settings.""}, {""relative_path"": ""documentation-website/_benchmark/user-guide/installing-benchmark.md"", ""chunk"": ""Provisioning an OpenSearch cluster with a test\n\nOpenSearch Benchmark is compatible with JDK versions 17, 16, 15, 14, 13, 12, 11, and 8.\n{: .note}\n\nIf you installed OpenSearch with PyPi, you can also provision a new OpenSearch cluster by specifying a `distribution-version` in the `execute-test` command.\n\nIf you plan on having Benchmark provision a cluster, you'll need to inform Benchmark of the location of the `JAVA_HOME` path for the Benchmark cluster. To set the `JAVA_HOME` path and provision a cluster:\n\n1. Find the `JAVA_HOME` path you're currently using. Open a terminal and enter `/usr/libexec/java_home`.\n\n2. Set your corresponding JDK version environment variable by entering the path from the previous step. Enter `export JAVA17_HOME=`.\n\n3. Run the `execute-test` command and indicate the distribution version of OpenSearch you want to use: \n\n  ```bash\n  opensearch-benchmark execute-test --distribution-version=2.3.0 --workload=geonames --test-mode \n  ```""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
"I'm trying to set up this new aggregate view thing for saved objects in OpenSearch Dashboards, but I'm worried about messing up our existing multi-tenancy setup. The docs mention something about tenant indexes and a kibana_server role. How do I make sure I don't break anything when I turn this feature on? And what's the deal with not being able to turn it off once it's enabled","[""_security/multi-tenancy/dynamic-config.md"", ""_security/multi-tenancy/mt-agg-view.md"", ""_security/multi-tenancy/multi-tenancy-config.md""]","[""documentation-website/_security/multi-tenancy/multi-tenancy-config.md"", ""_security/multi-tenancy/multi-tenancy-config.md"", ""_security/multi-tenancy/mt-agg-view.md"", ""_security/configuration/best-practices.md"", ""documentation-website/_security/configuration/best-practices.md""]","[{""relative_path"": ""documentation-website/_security/multi-tenancy/multi-tenancy-config.md"", ""chunk"": ""Multi-tenancy configuration\n\nMulti-tenancy is enabled in OpenSearch Dashboards by default. If you need to disable or change settings related to multi-tenancy, see the `kibana` settings in `config/opensearch-security/config.yml`, as shown in the following example:\n\n```yml\nconfig:\n  dynamic:\n    kibana:\n      multitenancy_enabled: true\n      private_tenant_enabled: true\n      default_tenant: global tenant\n      server_username: kibanaserver\n      index: '.kibana'\n    do_not_fail_on_forbidden: false\n```\n\n| Setting | Description |\n| :--- | :--- |\n| `multitenancy_enabled` | Enable or disable multi-tenancy. Default is `true`. |\n| `private_tenant_enabled` | Enable or disable the private tenant. Default is `true`. |\n| `default_tenant` | Use to set the tenant that is available when users log in. |\n| `server_username` | Must match the name of the OpenSearch Dashboards server user in `opensearch_dashboards.yml`. Default is `kibanaserver`. If a different user is configured, then make sure that user is mapped to the `kibana_server` role through the `role_mappings.yml` file in order to give them the appropriate permissions listed in kibana_server role details. |\n| `index` | Must match the name of the OpenSearch Dashboards index from `opensearch_dashboards.yml`. Default is `.kibana`. |\n| `do_not_fail_on_forbidden` | When `true`, the Security plugin removes any content that a user is not allowed to see from the search results. When `false`, the plugin returns a security exception. Default is `false`. |\n\nThe `opensearch_dashboards.yml` file includes additional settings:\n\n```yml\nopensearch.username: kibanaserver\nopensearch.password: kibanaserver\nopensearch.requestHeadersAllowlist: [\""securitytenant\"",\""Authorization\""]\nopensearch_security.multitenancy.enabled: true\nopensearch_security.multitenancy.tenants.enable_global: true\nopensearch_security.multitenancy.tenants.enable_private: true\nopensearch_security.multitenancy.tenants.preferred: [\""Private\"", \""Global\""]\nopensearch_security.multitenancy.enable_filter: false\n```\n\n| Setting | Description |\n| :--- | :--- |\n| `opensearch.requestHeadersAllowlist` | OpenSearch Dashboards requires that you add all HTTP headers to the allow list so that the headers pass to OpenSearch. Multi-tenancy uses a specific header, `securitytenant`, that must be present with the standard `Authorization` header. If the `securitytenant` header is not on the allow list, OpenSearch Dashboards starts with a red status.\n| `opensearch_security.multitenancy.enabled` | Enables or disables multi-tenancy in OpenSearch Dashboards. Default is `true`. |\n| `opensearch_security.multitenancy.tenants.enable_global` | Enables or disables the global tenant. Default is `true`. |\n| `opensearch_security.multitenancy.tenants.enable_private` | Enables or disables private tenants. Default is `true`. |\n| `opensearch_security.multitenancy.tenants.preferred` | Lets you change ordering in the **Tenants** tab of OpenSearch Dashboards. By default, the list starts with Global and Private (if enabled) and then proceeds alphabetically. You can add tenants here to move them to the top of the list. |\n| `opensearch_security.multitenancy.enable_filter` | If you have many tenants, you can add a search bar to the top of the list. Default is `false`. |""}, {""relative_path"": ""_security/multi-tenancy/multi-tenancy-config.md"", ""chunk"": ""Multi-tenancy configuration\n\nMulti-tenancy is enabled in OpenSearch Dashboards by default. If you need to disable or change settings related to multi-tenancy, see the `kibana` settings in `config/opensearch-security/config.yml`, as shown in the following example:\n\n```yml\nconfig:\n  dynamic:\n    kibana:\n      multitenancy_enabled: true\n      private_tenant_enabled: true\n      default_tenant: global tenant\n      server_username: kibanaserver\n      index: '.kibana'\n    do_not_fail_on_forbidden: false\n```\n\n| Setting | Description |\n| :--- | :--- |\n| `multitenancy_enabled` | Enable or disable multi-tenancy. Default is `true`. |\n| `private_tenant_enabled` | Enable or disable the private tenant. Default is `true`. |\n| `default_tenant` | Use to set the tenant that is available when users log in.""}, {""relative_path"": ""_security/multi-tenancy/mt-agg-view.md"", ""chunk"": ""OpenSearch Dashboards multi-tenancy aggregate view for saved objects\n\nThis is an experimental feature released in OpenSearch 2.4 and is not recommended for use in a production environment. For updates on the progress of the feature or if you want to leave feedback, see the Dashboards object sharing GitHub issue. For a more comprehensive view of the proposed future development of multi-tenancy, see the Dashboards object sharing issue.\n{: .warning}\n\nAggregate view for saved objects allows a user who has access to multiple tenants to see all saved objects associated with those tenants in a single view without having to switch between tenants to do so. This includes both tenants created by the user and tenants shared with the user. Aggregate view introduces a Tenant dropdown menu and column in the Saved Objects table that gives the user the option to filter by tenants and make visible their associated saved objects.\n\nOnce you identify a saved object of interest, you can then switch to that tenant to work with the object.""}, {""relative_path"": ""_security/configuration/best-practices.md"", ""chunk"": ""7. Consider disabling the private tenant\n\nIn many cases, the use of private tenants is unnecessary, although this feature is enabled by default. As a result, every OpenSearch Dashboards user is provided with their own private tenant and a corresponding new index in which to save objects. This can lead to a large number of unnecessary indexes. Evaluate whether private tenants are needed in your cluster. If private tenants are not needed, disable the feature by adding the following configuration to the `config.yml` file:\n\n```yaml\nconfig:\n  dynamic:\n    kibana:\n      multitenancy_enabled: true\n      private_tenant_enabled: false\n```\n{% include copy.html %}""}, {""relative_path"": ""documentation-website/_security/configuration/best-practices.md"", ""chunk"": ""7. Consider disabling the private tenant\n\nIn many cases, the use of private tenants is unnecessary, although this feature is enabled by default. As a result, every OpenSearch Dashboards user is provided with their own private tenant and a corresponding new index in which to save objects. This can lead to a large number of unnecessary indexes. Evaluate whether private tenants are needed in your cluster. If private tenants are not needed, disable the feature by adding the following configuration to the `config.yml` file:\n\n```yaml\nconfig:\n  dynamic:\n    kibana:\n      multitenancy_enabled: true\n      private_tenant_enabled: false\n```\n{% include copy.html %}""}]",0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.5307212739772434,0.4,0.6666666666666666,0.5307212739772434,0.2,0.6666666666666666,0.5307212739772434
What software license does Opensearch use?,"[""README.md"",""_about/index.md""]","[""_getting-started/index.md"", ""documentation-website/_getting-started/index.md"", ""TERMS.md"", ""_about/index.md"", ""documentation-website/_about/index.md""]","[{""relative_path"": ""_getting-started/index.md"", ""chunk"": ""Getting started\n\nOpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indexes, boost fields, rank results by score, sort results by field, and aggregate results.\n\nUnsurprisingly, builders often use a search engine like OpenSearch as the backend for a search application---think Wikipedia or an online store. It offers excellent performance and can scale up or down as the needs of the application grow or shrink.\n\nAn equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours.""}, {""relative_path"": ""documentation-website/_getting-started/index.md"", ""chunk"": ""Getting started\n\nOpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indexes, boost fields, rank results by score, sort results by field, and aggregate results.\n\nUnsurprisingly, builders often use a search engine like OpenSearch as the backend for a search application---think Wikipedia or an online store. It offers excellent performance and can scale up or down as the needs of the application grow or shrink.\n\nAn equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours. You can use OpenSearch Dashboards to build these sorts of visualizations from data in OpenSearch.""}, {""relative_path"": ""TERMS.md"", ""chunk"": ""O\n\n**onsite**\n\n**OpenSearch**\n\nOpenSearch is a community-driven, open-source search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 and Kibana 7.10.2. It consists of a search engine daemon, OpenSearch, and a visualization and user interface, OpenSearch Dashboards.\n\n**OpenSearch Dashboards**\n\nThe default visualization tool for data in OpenSearch. On first appearance, use the full name. *Dashboards* may be used on subsequent appearances.\n\nopen source (n.), open-source (adj.)\n\nUse _open source_ as a noun (for example, \""The code used throughout this tutorial is open source and can be freely modified\""). Use _open-source_ as an adjective _(open-source software)_.\n\n**OpenSearch Playground**\n\nDo not precede with _the_. OpenSearch Playground provides a central location for existing and evaluating users to explore features in OpenSearch and OpenSearch Dashboards without downloading or installing any OpenSearch components locally.""}, {""relative_path"": ""_about/index.md"", ""chunk"": ""Get involved\n\nOpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub.\nThe project welcomes GitHub issues, bug fixes, features, plugins, documentation---anything at all. To get involved, see Contributing on the OpenSearch website.\n\n---\n\nOpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V.""}, {""relative_path"": ""documentation-website/_about/index.md"", ""chunk"": ""Get involved\n\nOpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub.\nThe project welcomes GitHub issues, bug fixes, features, plugins, documentation---anything at all. To get involved, see Contributing on the OpenSearch website.\n\n---\n\nOpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V.""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.5,0.2640681225725909,0.1,0.5,0.2640681225725909
Does GPU accelerated nodes support Pytorch?,"[""_ml-commons-plugin/gpu-acceleration.md""]","[""_ml-commons-plugin/gpu-acceleration.md"", ""documentation-website/_ml-commons-plugin/gpu-acceleration.md""]","[{""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""PyTorch\n\nGPU-accelerated ML nodes require PyTorch 1.12.1 work with ML models.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""PyTorch\n\nGPU-accelerated ML nodes require PyTorch 1.12.1 work with ML models.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Setting up a GPU-accelerated ML node\n\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Setting up a GPU-accelerated ML node\n\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Supported GPUs\n\nCurrently, ML nodes support the following GPU instances:\n\n- NVIDIA instances with CUDA 11.6\n- AWS Inferentia\n\nIf you need GPU power, you can provision GPU instances through Amazon Elastic Compute Cloud (Amazon EC2). For more information on how to provision a GPU instance, see Recommended GPU Instances.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
Does opensearch support hugging face models? If so which ones?,"[""_ml-commons-plugin/pretrained-models.md""]","[""_ml-commons-plugin/pretrained-models.md"", ""documentation-website/_ml-commons-plugin/pretrained-models.md"", ""_ml-commons-plugin/custom-local-models.md"", ""documentation-website/_ml-commons-plugin/custom-local-models.md""]","[{""relative_path"": ""_ml-commons-plugin/pretrained-models.md"", ""chunk"": ""Supported pretrained models\n\nOpenSearch supports the following models, categorized by type. Text embedding models are sourced from Hugging Face. Sparse encoding models are trained by OpenSearch. Although models with the same type will have similar use cases, each model has a different model size and will perform differently depending on your cluster setup. For a performance comparison of some pretrained models, see the SBERT documentation.\n\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\n{: .important}""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/pretrained-models.md"", ""chunk"": ""Supported pretrained models\n\nOpenSearch supports the following models, categorized by type. Text embedding models are sourced from Hugging Face. Sparse encoding models are trained by OpenSearch. Although models with the same type will have similar use cases, each model has a different model size and will perform differently depending on your cluster setup. For a performance comparison of some pretrained models, see the SBERT documentation.\n\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\n{: .important}""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Model support\n\nAs of OpenSearch 2.6, OpenSearch supports local text embedding models.\n\nAs of OpenSearch 2.11, OpenSearch supports local sparse encoding models.\n\nAs of OpenSearch 2.12, OpenSearch supports local cross-encoder models.\n\nAs of OpenSearch 2.13, OpenSearch supports local question answering models.\n\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\n{: .important}""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Model support\n\nAs of OpenSearch 2.6, OpenSearch supports local text embedding models.\n\nAs of OpenSearch 2.11, OpenSearch supports local sparse encoding models.\n\nAs of OpenSearch 2.12, OpenSearch supports local cross-encoder models.\n\nAs of OpenSearch 2.13, OpenSearch supports local question answering models.\n\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\n{: .important}""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Model format\n\nTo use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the TorchScript and ONNX formats.\n\nYou must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript model file.\n\nAdditionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:\n\n```bash\nshasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip\n```""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
"I have a custom model, can I run it in Opensearch?","[""_ml-commons-plugin/custom-local-models.md"",""_ml-commons-plugin/api/model-apis/register-model.md""]","[""_ml-commons-plugin/api/model-apis/register-model.md"", ""documentation-website/_ml-commons-plugin/api/model-apis/register-model.md"", ""_ml-commons-plugin/custom-local-models.md"", ""documentation-website/_ml-commons-plugin/custom-local-models.md"", ""_ml-commons-plugin/integrating-ml-models.md""]","[{""relative_path"": ""_ml-commons-plugin/api/model-apis/register-model.md"", ""chunk"": ""Register a custom model\n\nTo use a custom model locally within the OpenSearch cluster, you need to provide a URL and a config object for that model. For more information, see Custom local models.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/api/model-apis/register-model.md"", ""chunk"": ""Register a custom model\n\nTo use a custom model locally within the OpenSearch cluster, you need to provide a URL and a config object for that model. For more information, see Custom local models.""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Prerequisites\n\nTo upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from Hugging Face, or train a new model in accordance with your needs.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Prerequisites\n\nTo upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from Hugging Face, or train a new model in accordance with your needs.""}, {""relative_path"": ""_ml-commons-plugin/integrating-ml-models.md"", ""chunk"": ""Choosing a model\n\nTo integrate an ML model into your search workflow, choose one of the following options:\n\n1. **Local model**: Upload a model to the OpenSearch cluster and use it locally. This option allows you to serve the model in your OpenSearch cluster but may require significant system resources.\n\n    1. **Pretrained model provided by OpenSearch**: This option requires minimal setup and avoids the time and effort required to train a custom model.\n\n        For a list of supported models and information about using a pretrained model provided by OpenSearch, see Pretrained models. \n\n    1. **Custom model**: This option offers customization for your specific use case.\n\n        For information about uploading your model, see Using ML models within OpenSearch.\n\n1. **Externally hosted model**: This option allows you to connect to a model hosted on a third-party platform. It requires more setup but allows the use of models that are already hosted on a service other than OpenSearch.""}]",1.0,0.5,1.0,0.6666666666666666,1.0,0.9197207891481876,0.4,1.0,0.9197207891481876,0.2,1.0,0.9197207891481876
"I have a model and some ML nodes, how do I boost it's performance?","[""_ml-commons-plugin/gpu-acceleration.md"", ""_ml-commons-plugin/custom-local-models.md"", ""_ml-commons-plugin/using-ml-models.md""]","[""_ml-commons-plugin/using-ml-models.md"", ""documentation-website/_ml-commons-plugin/using-ml-models.md"", ""_ml-commons-plugin/gpu-acceleration.md"", ""documentation-website/_ml-commons-plugin/gpu-acceleration.md""]","[{""relative_path"": ""_ml-commons-plugin/using-ml-models.md"", ""chunk"": ""GPU acceleration\n\nFor better performance, you can take advantage of GPU acceleration on your ML node. For more information, see GPU acceleration.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/using-ml-models.md"", ""chunk"": ""GPU acceleration\n\nFor better performance, you can take advantage of GPU acceleration on your ML node. For more information, see GPU acceleration.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Setting up a GPU-accelerated ML node\n\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Setting up a GPU-accelerated ML node\n\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""PyTorch\n\nGPU-accelerated ML nodes require PyTorch 1.12.1 work with ML models.""}]",1.0,0.3333333333333333,1.0,0.6666666666666666,0.6666666666666666,0.7039180890341347,0.4,0.6666666666666666,0.7039180890341347,0.2,0.6666666666666666,0.7039180890341347
Can you show me an example of how to use lat/long coordinates?,"[""_field-types/supported-field-types/geo-point.md""]","[""_dashboards/visualize/viz-index.md"", ""documentation-website/_dashboards/visualize/viz-index.md"", ""_field-types/supported-field-types/geo-point.md"", ""_dashboards/visualize/geojson-regionmaps.md"", ""documentation-website/_dashboards/visualize/geojson-regionmaps.md""]","[{""relative_path"": ""_dashboards/visualize/viz-index.md"", ""chunk"": ""Coordinate maps\n\nCoordinate maps show location-based data on a map. Use coordinate maps to visualize GPS data (latitude and longitude coordinates) on a map. For information about OpenSearch-supported coordinate field types, see Geographic field types and Cartesian field types.""}, {""relative_path"": ""documentation-website/_dashboards/visualize/viz-index.md"", ""chunk"": ""Coordinate maps\n\nCoordinate maps show location-based data on a map. Use coordinate maps to visualize GPS data (latitude and longitude coordinates) on a map. For information about OpenSearch-supported coordinate field types, see Geographic field types and Cartesian field types.""}, {""relative_path"": ""_field-types/supported-field-types/geo-point.md"", ""chunk"": ""`longitude`\"" format\n\n```json\nPUT testindex1/_doc/2\n{\n  \""point\"": \""40.71,74.00\"" \n}\n```\n{% include copy-curl.html %}\n\n- A geohash\n\n```json\nPUT testindex1/_doc/3\n{\n  \""point\"": \""txhxegj0uyp3\""\n}\n```\n{% include copy-curl.html %}\n\n- An array in the [`longitude`, `latitude`] format\n\n```json\nPUT testindex1/_doc/4\n{\n  \""point\"": [74.00, 40.71] \n}\n```\n{% include copy-curl.html %}\n\n- A Well-Known Text POINT in the \""POINT(`longitude` `latitude`)\"" format\n\n```json\nPUT testindex1/_doc/5\n{\n  \""point\"": \""POINT (74.00 40.71)\""\n}\n```\n{% include copy-curl.html %}\n\n- GeoJSON format,""}, {""relative_path"": ""_dashboards/visualize/geojson-regionmaps.md"", ""chunk"": ""Example GeoJSON file\n\nThe following example GeoJSON file provides coordinates for two US counties.\n\n```json\n{\n  \""type\"": \""FeatureCollection\"",\n  \""name\"": \""usa counties\"",\n  \""features\"": [\n    { \""type\"": \""Feature\"", \""properties\"": { \""iso2\"": \""US\"", \""iso3\"": \""LA-CA\"", \""name\"": \""Los Angeles County\"", \""country\"": \""US\"", \""county\"": \""LA\"" }, \""geometry\"": { \""type\"": \""Polygon\"", \""coordinates\"":[[[-118.71826171875,34.07086232376631],[-118.69628906249999,34.03445260967645],[-118.56994628906249,34.02990029603907],[-118.487548828125,33.957030069982316],[-118.37219238281249,33.86129311351553],[-118.45458984375,33.75631505992707],[-118.""}, {""relative_path"": ""documentation-website/_dashboards/visualize/geojson-regionmaps.md"", ""chunk"": ""Example GeoJSON file\n\nThe following example GeoJSON file provides coordinates for two US counties.\n\n```json\n{\n  \""type\"": \""FeatureCollection\"",\n  \""name\"": \""usa counties\"",\n  \""features\"": [\n    { \""type\"": \""Feature\"", \""properties\"": { \""iso2\"": \""US\"", \""iso3\"": \""LA-CA\"", \""name\"": \""Los Angeles County\"", \""country\"": \""US\"", \""county\"": \""LA\"" }, \""geometry\"": { \""type\"": \""Polygon\"", \""coordinates\"":[[[-118.71826171875,34.07086232376631],[-118.69628906249999,34.03445260967645],[-118.56994628906249,34.02990029603907],[-118.487548828125,33.957030069982316],[-118.37219238281249,33.86129311351553],[-118.45458984375,33.75631505992707],[-118.33923339843749,33.715201644740844],[-118.22937011718749,33.75631505992707],[-118.1414794921875,33.678639851675555],[-117.9107666015625,33.578014746143985],[-117.75146484375,33.4955977448657],[-117.55920410156249,33.55512901742288],[-117.3065185546875,33.5963189611327],[-117.0703125,33.67406853374198],[-116.69677734375,34.06176136129718],[-116.9439697265625,34.28445325435288],[-117.18017578125,34.42956713470528],[-117.3779296875,34.542762387234845],[-117.62512207031251,34.56990638085636],[-118.048095703125,34.615126683462194],[-118.44909667968749,34.542762387234845],[-118.61938476562499,34.38877925439021],[-118.740234375,34.21180215769026],[-118.71826171875,34.07086232376631]]] } },\n    { \""type\"": \""Feature\"", \""properties\"": { \""iso2\"": \""US\"", \""iso3\"": \""SD-CA\"", \""name\"": \""San Diego County\"", \""country\"": \""US\"", \""county\"": \""SD\"" }, \""geometry\"": { \""type\"": \""Polygon\"", \""coordinates\"":[[[-117.23510742187501,32.861132322810946],[-117.2406005859375,32.75494243654723],[-117.1636962890625,32.68099643258195],[-117.14172363281251,32.58384932565662],[-117.09228515624999,32.46342595776104],[-117.0538330078125,32.29177633471201],[-116.96044921875,32.194208672875384],[-116.85607910156249,32.16631295696736],[-116.6748046875,32.20350534542368],[-116.3671875,32.319633552035214],[-116.1474609375,32.55144352864431],[-116.1639404296875,32.80574473290688],[-116.4111328125,33.073130945006625],[-116.72973632812499,33.08233672856376],[-117.09228515624999,32.99484290420988],[-117.2515869140625,32.96258644191747], [-117.23510742187501,32.861132322810946]]] } }\n  ]\n}\n\n```""}]",0.0,0.0,0.0,0.3333333333333333,1.0,0.5,0.2,1.0,0.5,0.1,1.0,0.5
How do I use vector search?,"[""_search-plugins/vector-search.md"",""_search-plugins/knn/index.md"", ""_search-plugins/knn/knn-index.md"", ""_field-types/supported-field-types/knn-vector.md""]","[""_search-plugins/vector-search.md"", ""documentation-website/_search-plugins/vector-search.md"", ""_ml-commons-plugin/remote-models/index.md"", ""documentation-website/_ml-commons-plugin/remote-models/index.md"", ""_ml-commons-plugin/custom-local-models.md""]","[{""relative_path"": ""_search-plugins/vector-search.md"", ""chunk"": ""Vector search with filtering\n\nFor information about vector search with filtering, see k-NN search with filters.""}, {""relative_path"": ""documentation-website/_search-plugins/vector-search.md"", ""chunk"": ""Vector search with filtering\n\nFor information about vector search with filtering, see k-NN search with filters.""}, {""relative_path"": ""_ml-commons-plugin/remote-models/index.md"", ""chunk"": ""Step 7: Use the model for search\n\nTo learn how to use the model for vector search, see Using an ML model for neural search.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/remote-models/index.md"", ""chunk"": ""Step 7: Use the model for search\n\nTo learn how to use the model for vector search, see Using an ML model for neural search.""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Step 5: Use the model for search\n\nTo learn how to use the model for vector search, see Using an ML model for neural search.""}]",1.0,0.25,1.0,0.3333333333333333,0.25,0.46927872602275644,0.2,0.25,0.3903800499921017,0.1,0.25,0.3903800499921017
How do I understand the memory requirements for using hnsw?,"[""_search-plugins/knn/knn-vector-quantization.md""]","[""_search-plugins/knn/knn-index.md"", ""documentation-website/_search-plugins/knn/knn-index.md"", ""_search-plugins/knn/knn-vector-quantization.md"", ""documentation-website/_search-plugins/knn/knn-vector-quantization.md""]","[{""relative_path"": ""_search-plugins/knn/knn-index.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for HNSW is estimated to be `1.1 * (4 * dimension + 8 * M)` bytes/vector.\n\nAs an example, assume you have a million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\n\n```\n1.1 * (4 * 256 + 8 * 16) * 1,000,000 ~= 1.267 GB\n```""}, {""relative_path"": ""documentation-website/_search-plugins/knn/knn-index.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for HNSW is estimated to be `1.1 * (4 * dimension + 8 * M)` bytes/vector.\n\nAs an example, assume you have a million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\n\n```\n1.1 * (4 * 256 + 8 * 16) * 1,000,000 ~= 1.267 GB\n```""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.\n\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\n\n```r\n1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB\n```""}, {""relative_path"": ""documentation-website/_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.\n\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\n\n```r\n1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB\n```""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.\n\nAs an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:\n\n```r\n1.1 * ((8 / 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB\n```""}]",0.0,0.0,0.0,0.3333333333333333,1.0,0.5,0.2,1.0,0.5,0.1,1.0,0.5
Can you show me some different examples of using different quantization methods for vectors?,"[""_search-plugins/knn/knn-vector-quantization.md""]","[""_aggregations/metric/index.md"", ""documentation-website/_aggregations/metric/index.md"", ""_monitoring-your-cluster/metrics/getting-started.md""]","[{""relative_path"": ""_aggregations/metric/index.md"", ""chunk"": ""Metric aggregations\n\nMetric aggregations let you perform simple calculations such as finding the minimum, maximum, and average values of a field.""}, {""relative_path"": ""documentation-website/_aggregations/metric/index.md"", ""chunk"": ""Metric aggregations\n\nMetric aggregations let you perform simple calculations such as finding the minimum, maximum, and average values of a field.""}, {""relative_path"": ""_monitoring-your-cluster/metrics/getting-started.md"", ""chunk"": ""2. **UpDown counters:** UpDown counters can be incremented with positive values or decremented with negative values. UpDown counters are well suited for tracking metrics like open connections, active requests, and other fluctuating quantities.\n3. **Histograms:** Histograms are valuable tools for visualizing the distribution of continuous data. Histograms offer insight into the central tendency, spread, skewness, and potential outliers that might exist in your metrics. Patterns such as normal distribution, skewed distribution, or bimodal distribution can be readily identified, making histograms ideal for analyzing latency metrics and assessing percentiles.\n4. **Asynchronous Gauges:** Asynchronous gauges capture the current value at the moment a metric is read. These metrics are non-additive and are commonly used to measure CPU utilization on a per-minute basis, memory utilization, and other real-time values.""}, {""relative_path"": ""_aggregations/metric/index.md"", ""chunk"": ""Types of metric aggregations\n\nThere are two types of metric aggregations: single-value metric aggregations and multi-value metric aggregations.""}, {""relative_path"": ""documentation-website/_aggregations/metric/index.md"", ""chunk"": ""Types of metric aggregations\n\nThere are two types of metric aggregations: single-value metric aggregations and multi-value metric aggregations.""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
"Would you recommend I use ReRank? If so, what type of model would you recommend for ReRank?","[""_search-plugins/search-relevance/reranking-search-results.md""]","[""_ml-commons-plugin/integrating-ml-models.md"", ""documentation-website/_ml-commons-plugin/integrating-ml-models.md"", ""_search-plugins/neural-search-tutorial.md"", ""documentation-website/_search-plugins/neural-search-tutorial.md"", ""documentation-website/_search-plugins/vector-search.md""]","[{""relative_path"": ""_ml-commons-plugin/integrating-ml-models.md"", ""chunk"": ""Choosing a model\n\nTo integrate an ML model into your search workflow, choose one of the following options:\n\n1. **Local model**: Upload a model to the OpenSearch cluster and use it locally. This option allows you to serve the model in your OpenSearch cluster but may require significant system resources.\n\n    1. **Pretrained model provided by OpenSearch**: This option requires minimal setup and avoids the time and effort required to train a custom model.\n\n        For a list of supported models and information about using a pretrained model provided by OpenSearch, see Pretrained models. \n\n    1. **Custom model**: This option offers customization for your specific use case.\n\n        For information about uploading your model, see Using ML models within OpenSearch.\n\n1. **Externally hosted model**: This option allows you to connect to a model hosted on a third-party platform. It requires more setup but allows the use of models that are already hosted on a service other than OpenSearch.""}, {""relative_path"": ""documentation-website/_ml-commons-plugin/integrating-ml-models.md"", ""chunk"": ""Choosing a model\n\nTo integrate an ML model into your search workflow, choose one of the following options:\n\n1. **Local model**: Upload a model to the OpenSearch cluster and use it locally. This option allows you to serve the model in your OpenSearch cluster but may require significant system resources.\n\n    1. **Pretrained model provided by OpenSearch**: This option requires minimal setup and avoids the time and effort required to train a custom model.\n\n        For a list of supported models and information about using a pretrained model provided by OpenSearch, see Pretrained models. \n\n    1. **Custom model**: This option offers customization for your specific use case.\n\n        For information about uploading your model, see Using ML models within OpenSearch.\n\n1. **Externally hosted model**: This option allows you to connect to a model hosted on a third-party platform. It requires more setup but allows the use of models that are already hosted on a service other than OpenSearch.     \n    \n    To connect to an externally hosted model, you need to set up a connector:  \n\n    - For a walkthrough with detailed steps, see Connecting to externally hosted models.\n    - For more information about supported connectors, see Connectors.\n    - For information about creating your own connector, see Connector blueprints.\n\nIn OpenSearch version 2.9 and later, you can integrate local and external models simultaneously within a single cluster.\n{: .note}""}, {""relative_path"": ""_search-plugins/neural-search-tutorial.md"", ""chunk"": ""Advanced: Using a different model\n\nAlternatively, you can choose one of the following options for your model:\n\n- Use any other pretrained model provided by OpenSearch. For more information, see OpenSearch-provided pretrained models.\n\n- Upload your own model to OpenSearch. For more information, see Custom local models.\n\n- Connect to a foundation model hosted on an external platform. For more information, see Connecting to remote models.\n\nFor information about choosing a model, see Further reading.""}, {""relative_path"": ""documentation-website/_search-plugins/neural-search-tutorial.md"", ""chunk"": ""Advanced: Using a different model\n\nAlternatively, you can choose one of the following options for your model:\n\n- Use any other pretrained model provided by OpenSearch. For more information, see OpenSearch-provided pretrained models.\n\n- Upload your own model to OpenSearch. For more information, see Custom local models.\n\n- Connect to a foundation model hosted on an external platform. For more information, see Connecting to remote models.\n\nFor information about choosing a model, see Further reading.""}, {""relative_path"": ""documentation-website/_search-plugins/vector-search.md"", ""chunk"": ""Choosing a model\n\nThe first step in setting up neural search is choosing a model. You can upload a model to your OpenSearch cluster, use one of the pretrained models provided by OpenSearch, or connect to an externally hosted model. For more information, see Integrating ML models.""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?,"[""_search-plugins/caching/tiered-cache.md"",""_search-plugins/caching/request-cache.md""]","[""_search-plugins/caching/index.md"", ""_search-plugins/caching/tiered-cache.md"", ""documentation-website/_search-plugins/caching/tiered-cache.md""]","[{""relative_path"": ""_search-plugins/caching/index.md"", ""chunk"": ""Use a disk cache to cache much larger datasets, provided that the disk's latency is within an acceptable range.\n- **Tiered cache**: A multi-level cache in which each tier has its own characteristics and performance levels. For example, a tiered cache can contain both on-heap and disk tiers. By combining different tiers, you can achieve a balance between cache performance and size. To learn more, see Tiered cache.\n\nIn OpenSearch 2.14, the request cache is integrated with cache plugins. You can use a tiered or disk cache as a request-level cache.\n{: .note}""}, {""relative_path"": ""_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Setting on-heap and disk store tiers\n\nSet the on-heap and disk store tiers to `opensearch_onheap` and `ehcache_disk`, as shown in the following example:\n\n```yaml\nindices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap\nindices.request.cache.tiered_spillover.disk.store.name: ehcache_disk\n```\nThe `opensearch_onheap` setting uses the built-in on-heap cache available in OpenSearch. \n\nThe `ehcache_disk` setting is the disk cache implementation from Ehcache and requires installing the `cache-ehcache` plugin.\n\n{% include copy.html %}""}, {""relative_path"": ""documentation-website/_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Setting on-heap and disk store tiers\n\nSet the on-heap and disk store tiers to `opensearch_onheap` and `ehcache_disk`, as shown in the following example:\n\n```yaml\nindices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap\nindices.request.cache.tiered_spillover.disk.store.name: ehcache_disk\n```\nThe `opensearch_onheap` setting uses the built-in on-heap cache available in OpenSearch. \n\nThe `ehcache_disk` setting is the disk cache implementation from Ehcache and requires installing the `cache-ehcache` plugin.\n\n{% include copy.html %}""}, {""relative_path"": ""_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Types of tiered caches\n\nOpenSearch provides an implementation of a `_tiered` spillover `cache_`. This implementation spills any items removed from the upper tiers to the lower tiers of cache. The upper tier, such as the on-heap tier, is smaller in size but offers better latency. The lower tier, such as the disk cache, is larger in size but slower in terms of latency. OpenSearch offers both on-heap and disk tiers.""}, {""relative_path"": ""documentation-website/_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Types of tiered caches\n\nOpenSearch provides an implementation of a `_tiered` spillover `cache_`. This implementation spills any items removed from the upper tiers to the lower tiers of cache. The upper tier, such as the on-heap tier, is smaller in size but offers better latency. The lower tier, such as the disk cache, is larger in size but slower in terms of latency. OpenSearch offers both on-heap and disk tiers.""}]",0.0,0.0,0.0,0.3333333333333333,0.5,0.38685280723454163,0.2,0.5,0.38685280723454163,0.1,0.5,0.38685280723454163
I need to combine relevance scores from different types of searches.,"[""_query-dsl/compound/hybrid.md""]","[""_query-dsl/compound/hybrid.md"", ""documentation-website/_query-dsl/compound/hybrid.md"", ""_search-plugins/search-pipelines/normalization-processor.md"", ""documentation-website/_search-plugins/search-pipelines/normalization-processor.md"", ""documentation-website/_query-dsl/specialized/script-score.md""]","[{""relative_path"": ""_query-dsl/compound/hybrid.md"", ""chunk"": ""Hybrid query\n\nYou can use a hybrid query to combine relevance scores from multiple queries into one score for a given document. A hybrid query contains a list of one or more queries and independently calculates document scores at the shard level for each subquery. The subquery rewriting is performed at the coordinating node level in order to avoid duplicate computations.""}, {""relative_path"": ""documentation-website/_query-dsl/compound/hybrid.md"", ""chunk"": ""Hybrid query\n\nYou can use a hybrid query to combine relevance scores from multiple queries into one score for a given document. A hybrid query contains a list of one or more queries and independently calculates document scores at the shard level for each subquery. The subquery rewriting is performed at the coordinating node level in order to avoid duplicate computations.""}, {""relative_path"": ""_search-plugins/search-pipelines/normalization-processor.md"", ""chunk"": ""Score normalization and combination\n\nMany applications require both keyword matching and semantic understanding. For example, BM25 accurately provides relevant search results for a query containing keywords, and neural networks perform well when a query requires natural language understanding. Thus, you might want to combine BM25 search results with the results of a k-NN or neural search. However, BM25 and k-NN search use different scales to calculate relevance scores for the matching documents. Before combining the scores from multiple queries, it is beneficial to normalize them so that they are on the same scale, as shown by experimental data. For further reading about score normalization and combination, including benchmarks and various techniques, see this semantic search blog post.""}, {""relative_path"": ""documentation-website/_search-plugins/search-pipelines/normalization-processor.md"", ""chunk"": ""Score normalization and combination\n\nMany applications require both keyword matching and semantic understanding. For example, BM25 accurately provides relevant search results for a query containing keywords, and neural networks perform well when a query requires natural language understanding. Thus, you might want to combine BM25 search results with the results of a k-NN or neural search. However, BM25 and k-NN search use different scales to calculate relevance scores for the matching documents. Before combining the scores from multiple queries, it is beneficial to normalize them so that they are on the same scale, as shown by experimental data. For further reading about score normalization and combination, including benchmarks and various techniques, see this semantic search blog post.""}, {""relative_path"": ""documentation-website/_query-dsl/specialized/script-score.md"", ""chunk"": ""Example\n\nThe following example query searches for the text `neural search` in the `articles` index. It combines the original document relevance score with the `article_rank` value, which is first transformed with a saturation function:\n\n```json\nGET articles/_search\n{\n  \""query\"": {\n    \""script_score\"": {\n      \""query\"": {\n        \""match\"": { \""article_name\"": \""neural search\"" }\n      },\n      \""script\"" : {\n        \""source\"" : \""_score + saturation(doc['article_rank'].value, 11)\""\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
What metrics does query metrics give me out of the box?,"[""_observing-your-data/query-insights/query-metrics.md""]","[""_observing-your-data/query-insights/query-metrics.md"", ""documentation-website/_observing-your-data/query-insights/query-metrics.md"", ""documentation-website/_observing-your-data/query-insights/index.md""]","[{""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Metrics\n\nQuery metrics provide the following measurements:\n\n- The number of queries per query type (for example, the number of `match` or `regex` queries)\n- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)\n- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)\n- Histograms of `latency` for each query type, aggregation type, and sort order\n- Histograms of `cpu` for each query type, aggregation type, and sort order\n- Histograms of `memory` for each query type, aggregation type, and sort order""}, {""relative_path"": ""documentation-website/_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Metrics\n\nQuery metrics provide the following measurements:\n\n- The number of queries per query type (for example, the number of `match` or `regex` queries)\n- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)\n- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)\n- Histograms of `latency` for each query type, aggregation type, and sort order\n- Histograms of `cpu` for each query type, aggregation type, and sort order\n- Histograms of `memory` for each query type, aggregation type, and sort order""}, {""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Query metrics\n\nKey query metrics, such as aggregation types, query types, latency, and resource usage per query type, are captured along the search path by using the OpenTelemetry (OTel) instrumentation framework. The telemetry data can be consumed using OTel metrics exporters.""}, {""relative_path"": ""documentation-website/_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Query metrics\n\nKey query metrics, such as aggregation types, query types, latency, and resource usage per query type, are captured along the search path by using the OpenTelemetry (OTel) instrumentation framework. The telemetry data can be consumed using OTel metrics exporters.""}, {""relative_path"": ""documentation-website/_observing-your-data/query-insights/index.md"", ""chunk"": ""Query Insights settings\n\nYou can obtain the following information using Query Insights:\n\n- Top n queries\n- Query metrics""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
Does opensearch work with open telemetry?,"[""_observing-your-data/prometheusmetrics.md""]","[""_benchmark/reference/commands/execute-test.md"", ""documentation-website/_benchmark/reference/commands/execute-test.md"", ""_dashboards/search-telemetry.md"", ""documentation-website/_dashboards/search-telemetry.md"", ""documentation-website/_observing-your-data/trace/ta-dashboards.md""]","[{""relative_path"": ""_benchmark/reference/commands/execute-test.md"", ""chunk"": ""Telemetry\n\nThe following options enable telemetry devices on OpenSearch Benchmark: \n \n- `--telemetry`: Enables the provided telemetry devices when the devices are provided using a comma-separated list. You can find a list of possible telemetry devices by using `opensearch-benchmark list telemetry`.\n- `--telemetry-params`: Defines a comma-separated list of key-value pairs that are injected verbatim into the telemetry devices as parameters.""}, {""relative_path"": ""documentation-website/_benchmark/reference/commands/execute-test.md"", ""chunk"": ""Telemetry\n\nThe following options enable telemetry devices on OpenSearch Benchmark: \n \n- `--telemetry`: Enables the provided telemetry devices when the devices are provided using a comma-separated list. You can find a list of possible telemetry devices by using `opensearch-benchmark list telemetry`.\n- `--telemetry-params`: Defines a comma-separated list of key-value pairs that are injected verbatim into the telemetry devices as parameters.""}, {""relative_path"": ""_dashboards/search-telemetry.md"", ""chunk"": ""Sample opensearch_dashboards.yml with telemetry enabled\n\n This OpenSearch Dashboards YAML file excerpt shows the telemetry setting set to `true` to turn on search telemetry:\n\n ```json""}, {""relative_path"": ""documentation-website/_dashboards/search-telemetry.md"", ""chunk"": ""Sample opensearch_dashboards.yml with telemetry enabled\n\n This OpenSearch Dashboards YAML file excerpt shows the telemetry setting set to `true` to turn on search telemetry:\n\n ```json""}, {""relative_path"": ""documentation-website/_observing-your-data/trace/ta-dashboards.md"", ""chunk"": ""Setting up the OpenTelemetry Demo\n\nThe OpenTelemetry Demo with OpenSearch simulates a distributed application generating real-time telemetry data, providing you with a practical environment in which to explore features available with the Trace Analytics plugin before implementing it in your environment.\n\n\n**Step 1: Set up the OpenTelemetry Demo**\n  \n  - Clone the OpenTelemetry Demo with OpenSearch repository: `git clone https://github.com/opensearch-project/opentelemetry-demo`.\n  - Follow the Getting Started instructions to deploy the demo application using Docker, which runs multiple microservices generating telemetry data.\n\n**Step 2: Ingest telemetry data**\n\n  - Configure the OTel collectors to send telemetry data (traces, metrics, logs) to your OpenSearch cluster, using the preexisting setup.\n  - Confirm that Data Prepper is set up to process the incoming data, handle trace analytics and service map pipelines, submit data to required indexes, and perform preaggregated calculations.\n\n**Step 3: Explore Trace Analytics in OpenSearch Dashboards**\n\nThe **Trace Analytics** application includes two options: **Services** and **Traces**:\n\n  - **Services** lists all services in the application, plus an interactive map that shows how the various services connect to each other. In contrast to the dashboard (which helps identify problems by operation), the **Service map** helps you identify problems by service based on error rates and latency. To access this option, go to **Trace Analytics** > **Services**.\n  - **Traces** groups traces together by HTTP method and path so that you can see the average latency, error rate, and trends associated with a particular operation. For a more focused view, try filtering by trace group name. To access this option, go to **Trace Analytics** > **Traces**. From the **Trace Groups** panel, you can review the traces that comprise a trace group. From the **Traces** panel you can analyze individual traces for a detailed summary.\n\n **Step 4: Perform correlation analysis**\n  - Select **Services correlation** to display connections between various telemetry signals. This allows you to navigate from the logical service level to the associated metrics and logs for that specific service.\n\n---""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?,"[""_observing-your-data/notifications/api.md""]","[""_observing-your-data/notifications/api.md"", ""documentation-website/_observing-your-data/notifications/api.md""]","[{""relative_path"": ""_observing-your-data/notifications/api.md"", ""chunk"": ""Example request\n\n```json\nPUT _plugins/_notifications/configs/\n{\n  \""config\"": {\n    \""name\"": \""Slack Channel\"",\n    \""description\"": \""This is an updated channel configuration\"",\n    \""config_type\"": \""slack\"",\n    \""is_enabled\"": true,\n    \""slack\"": {\n      \""url\"": \""https://hooks.slack.com/sample-url\""\n    }\n  }\n}\n```""}, {""relative_path"": ""documentation-website/_observing-your-data/notifications/api.md"", ""chunk"": ""Example request\n\n```json\nPUT _plugins/_notifications/configs/\n{\n  \""config\"": {\n    \""name\"": \""Slack Channel\"",\n    \""description\"": \""This is an updated channel configuration\"",\n    \""config_type\"": \""slack\"",\n    \""is_enabled\"": true,\n    \""slack\"": {\n      \""url\"": \""https://hooks.slack.com/sample-url\""\n    }\n  }\n}\n```""}, {""relative_path"": ""_observing-your-data/notifications/api.md"", ""chunk"": ""Example request\n\n```json\nPOST /_plugins/_notifications/configs/\n{\n  \""config_id\"": \""sample-id\"",\n  \""name\"": \""sample-name\"",\n  \""config\"": {\n    \""name\"": \""Sample Slack Channel\"",\n    \""description\"": \""This is a Slack channel\"",\n    \""config_type\"": \""slack\"",\n    \""is_enabled\"": true,\n    \""slack\"": {\n      \""url\"": \""https://sample-slack-webhook\""\n    }\n  }\n}\n```\n\nThe create channel API operation accepts the following fields in its request body:\n\nField |\tData type |\tDescription |\tRequired\n:--- | :--- | :--- | :---\nconfig_id | String | The configuration's custom ID. | No\nconfig | Object |\tContains all relevant information, such as channel name, configuration type, and plugin source. |\tYes\nname | String |\tName of the channel. | Yes\ndescription |\tString | The channel's description. | No\nconfig_type |\tString | The destination of your notification.""}, {""relative_path"": ""documentation-website/_observing-your-data/notifications/api.md"", ""chunk"": ""Example request\n\n```json\nPOST /_plugins/_notifications/configs/\n{\n  \""config_id\"": \""sample-id\"",\n  \""name\"": \""sample-name\"",\n  \""config\"": {\n    \""name\"": \""Sample Slack Channel\"",\n    \""description\"": \""This is a Slack channel\"",\n    \""config_type\"": \""slack\"",\n    \""is_enabled\"": true,\n    \""slack\"": {\n      \""url\"": \""https://sample-slack-webhook\""\n    }\n  }\n}\n```\n\nThe create channel API operation accepts the following fields in its request body:\n\nField |\tData type |\tDescription |\tRequired\n:--- | :--- | :--- | :---\nconfig_id | String | The configuration's custom ID. | No\nconfig | Object |\tContains all relevant information, such as channel name, configuration type, and plugin source. |\tYes\nname | String |\tName of the channel. | Yes\ndescription |\tString | The channel's description. | No\nconfig_type |\tString | The destination of your notification. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`. | Yes\nis_enabled | Boolean | Indicates whether the channel is enabled for sending and receiving notifications. Default is `true`.\t| No\n\nThe create channel operation accepts multiple `config_types` as possible notification destinations, so follow the format for your preferred `config_type`.\n\n```json\n\""sns\"": {\n  \""topic_arn\"": \""\"",\n  \""role_arn\"": \""\"" //optional\n}\n\""slack\"": {\n  \""url\"": \""https://sample-chime-webhoook\""\n}\n\""chime\"": {\n  \""url\"": \""https://sample-amazon-chime-webhoook\""\n}\n\""webhook\"": {\n      \""url\"": \""https://custom-webhook-test-url.com:8888/test-path?params1=value1&params2=value2\""\n}\n\""smtp_account\"": {\n  \""host\"": \""test-host.com\"",\n  \""port\"": 123,\n  \""method\"": \""start_tls\"",\n  \""from_address\"": \""test@email.com\""\n}\n\""ses_account\"": {\n  \""region\"": \""us-east-1\"",\n  \""role_arn\"": \""arn:aws:iam::012345678912:role/NotificationsSESRole\"",\n  \""from_address\"": \""test@email.com\""\n}\n\""email_group\"": { //Email recipient group\n  \""recipient_list\"": [\n    {\n      \""recipient\"": \""test-email1@test.com\""\n    },\n    {\n      \""recipient\"": \""test-email2@test.com\""\n    }\n  ]\n}\n\""email\"": { //The channel that sends emails\n  \""email_account_id\"": \""\"",\n  \""recipient_list\"": [\n    {\n      \""recipient\"": \""custom.email@test.com\""\n    }\n  ],\n  \""email_group_id_list\"": []\n}\n```\n\nThe following example demonstrates how to create a channel using email as a `config_type`:\n\n```json\nPOST /_plugins/_notifications/configs/\n{\n  \""id\"": \""sample-email-id\"",\n  \""name\"": \""sample-name\"",\n  \""config\"": {\n    \""name\"": \""Sample Email Channel\"",\n    \""description\"": \""Sample email description\"",\n    \""config_type\"": \""email\"",\n    \""is_enabled\"": true,\n    \""email\"": {\n      \""email_account_id\"": \""\"",\n      \""recipient_list\"": [\n        \""sample@email.com\""\n      ]\n    }\n  }\n}\n```""}, {""relative_path"": ""documentation-website/_observing-your-data/notifications/api.md"", ""chunk"": ""Update channel configuration\n\nTo update a channel configuration, send a POST request to the `configs` resource and specify the channel's `config_id` as a path parameter. Specify the new configuration details in the request body.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
"What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?","[""_install-and-configure/plugins.md""]","[""_install-and-configure/plugins.md""]","[{""relative_path"": ""_install-and-configure/plugins.md"", ""chunk"": ""commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\n* javax.management.MBeanServerPermission createMBeanServer\n* javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y/N]y\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\n```""}, {""relative_path"": ""_install-and-configure/plugins.md"", ""chunk"": ""commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\n* javax.management.MBeanServerPermission createMBeanServer\n* javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y/N]y\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\n```\n\n\nRestart your OpenSearch node after installing a plugin.\n{: .note}""}, {""relative_path"": ""_install-and-configure/plugins.md"", ""chunk"": ""commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\n* javax.management.MBeanServerPermission createMBeanServer\n* javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y/N]y\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\n\n# Zip file in a local directory.\n$ sudo ./opensearch-plugin install file:/home/user/opensearch-anomaly-detection-2.2.0.0.""}, {""relative_path"": ""_install-and-configure/plugins.md"", ""chunk"": ""management.MBeanServerPermission createMBeanServer\n* javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y/N]y\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\n\n# Zip file in a local directory.\n$ sudo ./opensearch-plugin install file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip\n-> Installing file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip\n-> Downloading file:/home/user/opensearch-anomaly-detection-2.2.0.0.""}, {""relative_path"": ""_install-and-configure/plugins.md"", ""chunk"": ""plugin:opensearch-anomaly-detection:2.2.0.0 from maven central\n[=================================================] 100%   \n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@     WARNING: plugin requires additional permissions     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n* java.lang.RuntimePermission accessClassInPackage.sun.misc\n* java.lang.RuntimePermission accessDeclaredMembers\n* java.lang.RuntimePermission getClassLoader\n* java.lang.RuntimePermission setContextClassLoader\n* java.lang.reflect.ReflectPermission suppressAccessChecks\n* java.net.SocketPermission * connect,resolve\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
What environment variable and YAML structure must be used to customize the admin password in a helm chart?,"[""_install-and-configure/install-opensearch/helm.md""]","[""_install-and-configure/install-dashboards/helm.md"", ""_install-and-configure/install-opensearch/helm.md"", ""_security/configuration/security-admin.md"", ""_security/configuration/yaml.md"", ""documentation-website/_security/configuration/yaml.md""]","[{""relative_path"": ""_install-and-configure/install-dashboards/helm.md"", ""chunk"": ""`values.yaml` |  Default configuration values for the chart.\n`templates` |  Templates that combine with values to generate the Kubernetes manifest files.\n\nThe specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC).\n\nFor information about the default configuration, steps to configure security, and configurable parameters, see the\nREADME.\n\nThe instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the Kubernetes documentation for steps to configure a Kubernetes cluster and the Helm documentation to install Helm.\n{: .note }""}, {""relative_path"": ""_install-and-configure/install-opensearch/helm.md"", ""chunk"": ""`values.yaml` |  Default configuration values for the chart.\n`templates` |  Templates that combine with values to generate the Kubernetes manifest files.\n\nThe specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC).\n\nFor information about the default configuration, steps to configure security, and configurable parameters, see the\nREADME.\n\nThe instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the Kubernetes documentation for steps to configure a Kubernetes cluster and the Helm documentation to install Helm.\n{: .note }""}, {""relative_path"": ""_security/configuration/security-admin.md"", ""chunk"": ""For example, if you create ten new roles, you can safely load `internal_users.yml` into the index without losing your roles; only the internal users get overwritten.\n\n```bash\n./securityadmin.sh -f ../../../config/opensearch-security/internal_users.yml \\\n  -t internalusers \\\n  -icl \\\n  -nhnv \\\n  -cacert ../../../config/root-ca.pem \\\n  -cert ../../../config/kirk.pem \\\n  -key ../../../config/kirk-key.pem\n```\n\nTo resolve all environment variables before applying the security configurations, use the `-rev` parameter.\n\n```bash\n./securityadmin.sh -cd ../../../config/opensearch-security/ \\\n -rev \\\n -cacert ../../../root-ca.pem \\\n -cert ../../../kirk.pem \\\n -key ../../../kirk.key.pem\n```\n\nThe following example shows an environment variable in the `config.yml` file:\n\n```yml\npassword: ${env.LDAP_PASSWORD}\n```""}, {""relative_path"": ""_security/configuration/yaml.md"", ""chunk"": ""Modifying the YAML files\n\nThe Security installation provides a number of YAML configuration files that are used to store the necessary settings that define the way the Security plugin manages users, roles, and activity within the cluster. These settings range from configurations for authentication backends to lists of allowed endpoints and HTTP requests. \n\nBefore running `securityadmin.sh` to load the settings into the `.opendistro_security` index, perform an initial configuration of the YAML files. The files can be found in the `config/opensearch-security` directory. It's also good practice to back up these files so that you can reuse them for other clusters.\n\nThe approach we recommend for using the YAML files is to first configure reserved and hidden resources, such as the `admin` and `kibanaserver` users. Thereafter you can create other users, roles, mappings, action groups, and tenants using OpenSearch Dashboards or the REST API.""}, {""relative_path"": ""documentation-website/_security/configuration/yaml.md"", ""chunk"": ""Modifying the YAML files\n\nThe Security installation provides a number of YAML configuration files that are used to store the necessary settings that define the way the Security plugin manages users, roles, and activity within the cluster. These settings range from configurations for authentication backends to lists of allowed endpoints and HTTP requests. \n\nBefore running `securityadmin.sh` to load the settings into the `.opendistro_security` index, perform an initial configuration of the YAML files. The files can be found in the `config/opensearch-security` directory. It's also good practice to back up these files so that you can reuse them for other clusters.\n\nThe approach we recommend for using the YAML files is to first configure reserved and hidden resources, such as the `admin` and `kibanaserver` users. Thereafter you can create other users, roles, mappings, action groups, and tenants using OpenSearch Dashboards or the REST API.""}]",0.0,0.0,0.0,0.3333333333333333,1.0,0.6309297535714575,0.2,1.0,0.6309297535714575,0.1,1.0,0.6309297535714575
How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?,"[""_aggregations/bucket/reverse-nested.md""]","[""_observing-your-data/query-insights/query-metrics.md"", ""documentation-website/_observing-your-data/query-insights/query-metrics.md"", ""documentation-website/_api-reference/nodes-apis/nodes-info.md""]","[{""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Metrics\n\nQuery metrics provide the following measurements:\n\n- The number of queries per query type (for example, the number of `match` or `regex` queries)\n- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)\n- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)\n- Histograms of `latency` for each query type, aggregation type, and sort order\n- Histograms of `cpu` for each query type, aggregation type, and sort order\n- Histograms of `memory` for each query type, aggregation type, and sort order""}, {""relative_path"": ""documentation-website/_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Metrics\n\nQuery metrics provide the following measurements:\n\n- The number of queries per query type (for example, the number of `match` or `regex` queries)\n- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)\n- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)\n- Histograms of `latency` for each query type, aggregation type, and sort order\n- Histograms of `cpu` for each query type, aggregation type, and sort order\n- Histograms of `memory` for each query type, aggregation type, and sort order""}, {""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""---\nlayout: default\ntitle: Query metrics\nparent: Query insights\nnav_order: 20\n---""}, {""relative_path"": ""documentation-website/_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""---\nlayout: default\ntitle: Query metrics\nparent: Query insights\nnav_order: 20\n---""}, {""relative_path"": ""documentation-website/_api-reference/nodes-apis/nodes-info.md"", ""chunk"": ""Example response\n\nThe response contains the metric groups specified in the `` request parameter (in this case, `process` and `transport`):\n\n```json\n{\n  \""_nodes\"": {\n    \""total\"": 1,\n    \""successful\"": 1,\n    \""failed\"": 0\n  },\n  \""cluster_name\"": \""opensearch\"",\n  \""nodes\"": {\n    \""VC0d4RgbTM6kLDwuud2XZQ\"": {\n      \""name\"": \""node-m1-23\"",\n      \""transport_address\"": \""127.0.0.1:9300\"",\n      \""host\"": \""127.0.0.1\"",\n      \""ip\"": \""127.0.0.1\"",\n      \""version\"": \""1.3.1\"",\n      \""build_type\"": \""tar\"",\n      \""build_hash\"": \""c4c0672877bf0f787ca857c7c37b775967f93d81\"",\n      \""roles\"": [\n        \""data\"",\n        \""ingest\"",\n        \""master\"",\n        \""remote_cluster_client\""\n      ],\n      \""attributes\"": {\n        \""shard_indexing_pressure_enabled\"": \""true\""\n      },\n      \""process\"" : {\n        \""refresh_interval_in_millis\"": 1000,\n        \""id\"": 44584,\n        \""mlockall\"": false\n      },\n      \""transport\"": {\n        \""bound_address\"": [\n          \""[::1]:9300\"",\n          \""127.0.0.1:9300\""\n        ],\n        \""publish_address\"": \""127.0.0.1:9300\"",\n        \""profiles\"": { }\n      }\n    }\n  }\n}\n```""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Make it simpler like what happens when I have missing values in an agg?,"[""_aggregations/bucket/missing.md""]","[""_aggregations/bucket/missing.md"", ""documentation-website/_aggregations/bucket/missing.md"", ""documentation-website/_aggregations/metric/weighted-avg.md"", ""_aggregations/metric/median-absolute-deviation.md"", ""documentation-website/_aggregations/metric/median-absolute-deviation.md""]","[{""relative_path"": ""_aggregations/bucket/missing.md"", ""chunk"": ""Missing aggregations\n\nIf you have documents in your index that don\u2019t contain the aggregating field at all or the aggregating field has a value of NULL, use the `missing` parameter to specify the name of the bucket such documents should be placed in.\n\nThe following example adds any missing values to a bucket named \""N/A\"":\n\n```json\nGET opensearch_dashboards_sample_data_logs/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""response_codes\"": {\n      \""terms\"": {\n        \""field\"": \""response.keyword\"",\n        \""size\"": 10,\n        \""missing\"": \""N/A\""\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nBecause the default value for the `min_doc_count` parameter is 1, the `missing` parameter doesn't return any buckets in its response.""}, {""relative_path"": ""documentation-website/_aggregations/bucket/missing.md"", ""chunk"": ""Missing aggregations\n\nIf you have documents in your index that don\u2019t contain the aggregating field at all or the aggregating field has a value of NULL, use the `missing` parameter to specify the name of the bucket such documents should be placed in.\n\nThe following example adds any missing values to a bucket named \""N/A\"":\n\n```json\nGET opensearch_dashboards_sample_data_logs/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""response_codes\"": {\n      \""terms\"": {\n        \""field\"": \""response.keyword\"",\n        \""size\"": 10,\n        \""missing\"": \""N/A\""\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nBecause the default value for the `min_doc_count` parameter is 1, the `missing` parameter doesn't return any buckets in its response. Set `min_doc_count` parameter to 0 to see the \""N/A\"" bucket in the response:\n\n```json\nGET opensearch_dashboards_sample_data_logs/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""response_codes\"": {\n      \""terms\"": {\n        \""field\"": \""response.keyword\"",\n        \""size\"": 10,\n        \""missing\"": \""N/A\"",\n        \""min_doc_count\"": 0\n      }\n    }\n  }\n}\n```""}, {""relative_path"": ""documentation-website/_aggregations/metric/weighted-avg.md"", ""chunk"": ""Handling missing values\n\nThe `missing` parameter allows you to specify default values for documents missing the `value` field or the `weight` field instead of excluding them from the calculation.\n\nThe following is an example of this behavior. First, create an index and add sample documents. This example includes five documents with different combinations of missing values for the `rating` and `num_reviews` fields: \n\n```json\nPUT /products\n{\n  \""mappings\"": {\n    \""properties\"": {\n      \""name\"": {\n        \""type\"": \""text\""\n      },\n      \""rating\"": {\n        \""type\"": \""double\""\n      },\n      \""num_reviews\"": {\n        \""type\"": \""integer\""\n      }\n    }\n  }\n}\n\nPOST /_bulk\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product A\"", \""rating\"": 4.5, \""num_reviews\"": 100 }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product B\"", \""rating\"": 3.8, \""num_reviews\"": 50 }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product C\"", \""rating\"": null, \""num_reviews\"": 20 }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product D\"", \""rating\"": 4.2, \""num_reviews\"": null }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product E\"", \""rating\"": null, \""num_reviews\"": null }\n```\n{% include copy-curl.html %}\n\nNext, run the following `weighted_avg` aggregation:\n\n```json\nGET /products/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""weighted_rating\"": {\n      \""weighted_avg\"": {\n        \""value\"": {\n          \""field\"": \""rating\""\n        },\n        \""weight\"": {\n          \""field\"": \""num_reviews\""\n        }\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nIn the response, you can see that the missing values for `Product E` were completely ignored in the calculation.""}, {""relative_path"": ""_aggregations/metric/median-absolute-deviation.md"", ""chunk"": ""Missing\n\nBy default, if a field is missing or has a null value in a document, it is ignored during computation. However, you can specify a value to be used for those missing or null fields by using the `missing` parameter, as shown in the following request:\n\n```json\nGET opensearch_dashboards_sample_data_flights/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""median_absolute_deviation_distanceMiles\"": {\n      \""median_absolute_deviation\"": {\n        \""field\"": \""DistanceMiles\"",\n        \""missing\"": 1000\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}, {""relative_path"": ""documentation-website/_aggregations/metric/median-absolute-deviation.md"", ""chunk"": ""Missing\n\nBy default, if a field is missing or has a null value in a document, it is ignored during computation. However, you can specify a value to be used for those missing or null fields by using the `missing` parameter, as shown in the following request:\n\n```json\nGET opensearch_dashboards_sample_data_flights/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""median_absolute_deviation_distanceMiles\"": {\n      \""median_absolute_deviation\"": {\n        \""field\"": \""DistanceMiles\"",\n        \""missing\"": 1000\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
Can you explain compound queries to me?,"[""_query-dsl/compound/index.md""]","[""_query-dsl/compound/index.md"", ""documentation-website/_query-dsl/compound/index.md"", ""_getting-started/search-data.md"", ""documentation-website/_getting-started/search-data.md"", ""_query-dsl/index.md""]","[{""relative_path"": ""_query-dsl/compound/index.md"", ""chunk"": ""Compound queries\n\nCompound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. \n\nThe following table lists all compound query types.\n\nQuery type | Description\n:--- | :---\n`bool` (Boolean)| Combines multiple query clauses with Boolean logic. \n`boosting` | Changes the relevance score of documents without removing them from the search results. Returns documents that match a `positive` query, but downgrades the relevance of documents in the results that match a `negative` query.\n`constant_score` | Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the `boost` value.\n`dis_max` (disjunction max) | Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value.""}, {""relative_path"": ""documentation-website/_query-dsl/compound/index.md"", ""chunk"": ""Compound queries\n\nCompound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. \n\nThe following table lists all compound query types.\n\nQuery type | Description\n:--- | :---\n`bool` (Boolean)| Combines multiple query clauses with Boolean logic. \n`boosting` | Changes the relevance score of documents without removing them from the search results. Returns documents that match a `positive` query, but downgrades the relevance of documents in the results that match a `negative` query.\n`constant_score` | Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the `boost` value.\n`dis_max` (disjunction max) | Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value.\n`function_score` | Recalculates the relevance score of documents that are returned by a query using a function that you define.\n`hybrid` | Combines relevance scores from multiple queries into one score for a given document.""}, {""relative_path"": ""_getting-started/search-data.md"", ""chunk"": ""Compound queries\n\nA compound query lets you combine multiple query or filter clauses. A Boolean query is an example of a compound query.\n\nFor example, to search for students whose name matches `doe` and filter by graduation year and GPA, use the following request:\n\n```json\nGET students/_search\n{\n  \""query\"": {\n    \""bool\"": {\n      \""must\"": [\n        {\n          \""match\"": {\n            \""name\"": \""doe\""\n          }\n        },\n        { \""range\"": { \""gpa\"": { \""gte\"": 3.6, \""lte\"": 3.9 } } },\n        { \""term\"":  { \""grad_year\"": 2022 }}\n      ]\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nFor more information about Boolean and other compound queries, see Compound queries.""}, {""relative_path"": ""documentation-website/_getting-started/search-data.md"", ""chunk"": ""Compound queries\n\nA compound query lets you combine multiple query or filter clauses. A Boolean query is an example of a compound query.\n\nFor example, to search for students whose name matches `doe` and filter by graduation year and GPA, use the following request:\n\n```json\nGET students/_search\n{\n  \""query\"": {\n    \""bool\"": {\n      \""must\"": [\n        {\n          \""match\"": {\n            \""name\"": \""doe\""\n          }\n        },\n        { \""range\"": { \""gpa\"": { \""gte\"": 3.6, \""lte\"": 3.9 } } },\n        { \""term\"":  { \""grad_year\"": 2022 }}\n      ]\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nFor more information about Boolean and other compound queries, see Compound queries.""}, {""relative_path"": ""_query-dsl/index.md"", ""chunk"": ""You can combine query clauses to produce complex queries. \n\nBroadly, you can classify queries into two categories---*leaf queries* and *compound queries*:\n\n- **Leaf queries**: Leaf queries search for a specified value in a certain field or fields. You can use leaf queries on their own. They include the following query types:\n\n    - Full-text queries: Use full-text queries to search text documents. For an analyzed text field search, full-text queries split the query string into terms using the same analyzer that was used when the field was indexed. For an exact value search, full-text queries look for the specified value without applying text analysis. \n\n    - Term-level queries: Use term-level queries to search documents for an exact term, such as an ID or value range. Term-level queries do not analyze search terms or sort results by relevance score.\n\n    - Geographic and xy queries: Use geographic queries to search documents that include geographic data. Use xy queries to search documents that include points and shapes in a two-dimensional coordinate system.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
What is the difference between vector search and full text search?,"[""_query-dsl/term-vs-full-text.md"", ""search-plugins/vector-search.md""]","[""documentation-website/_query-dsl/term-vs-full-text.md"", ""_query-dsl/term-vs-full-text.md"", ""documentation-website/_search-plugins/vector-search.md""]","[{""relative_path"": ""documentation-website/_query-dsl/term-vs-full-text.md"", ""chunk"": ""Term-level and full-text queries compared\n\nYou can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries analyze the query string. The following table summarizes the differences between term-level and full-text queries.\n\n| | Term-level queries | Full-text queries\n:--- | :--- | :---\n*Description* | Term-level queries answer which documents match a query. | Full-text queries answer how well the documents match a query.\n*Analyzer* | The search term isn't analyzed. This means that the term query searches for your search term as it is.  | The search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document's field.\n*Relevance* | Term-level queries simply return documents that match without sorting them based on the relevance score. They still calculate the relevance score, but this score is the same for all the documents that are returned. | Full-text queries calculate a relevance score for each match and sort the results by decreasing order of relevance.\n*Use Case* | Use term-level queries when you want to match exact values such as numbers, dates, or tags and don't need the matches to be sorted by relevance. | Use full-text queries to match text fields and sort by relevance after taking into account factors like casing and stemming variants.\n\nOpenSearch uses the BM25 ranking algorithm to calculate relevance scores. To learn more, see Okapi BM25.\n{: .note }""}, {""relative_path"": ""_query-dsl/term-vs-full-text.md"", ""chunk"": ""Term-level and full-text queries compared\n\nYou can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries analyze the query string. The following table summarizes the differences between term-level and full-text queries.\n\n| | Term-level queries | Full-text queries\n:--- | :--- | :---\n*Description* | Term-level queries answer which documents match a query. | Full-text queries answer how well the documents match a query.\n*Analyzer* | The search term isn't analyzed. This means that the term query searches for your search term as it is.  | The search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document's field.""}, {""relative_path"": ""_query-dsl/term-vs-full-text.md"", ""chunk"": ""Should I use a full-text or a term-level query?\n\nTo clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster.""}, {""relative_path"": ""documentation-website/_query-dsl/term-vs-full-text.md"", ""chunk"": ""Should I use a full-text or a term-level query?\n\nTo clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster.""}, {""relative_path"": ""documentation-website/_search-plugins/vector-search.md"", ""chunk"": ""Vector search with filtering\n\nFor information about vector search with filtering, see k-NN search with filters.""}]",0.0,0.0,0.0,0.3333333333333333,0.5,0.38685280723454163,0.2,0.5,0.38685280723454163,0.1,0.5,0.38685280723454163
"Why can't my coworker see my async searches, but I can see and delete theirs? This role thing is messing everything up!","[""_search-plugins/async/security.md""]","[""_search-plugins/async/security.md"", ""documentation-website/_search-plugins/async/security.md""]","[{""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""\""hosts\"": [],\n  \""users\"": [\""judy\"",\""elon\""]\n}\n```\n\n`jack` has read access to asynchronous search results:\n\n```json\nPUT _plugins/_security/api/rolesmapping/async_read_access\n{\n  \""backend_roles\"": [],\n  \""hosts\"": [],\n  \""users\"": [\""jack\""]\n}\n```\n\nBecause none of the users have backend roles, they will be able to see each other's asynchronous searches. So, if `judy` submits an asynchronous search, `elon`, who has full access, will be able to see that search. `jack`, who has read access, will also be able to see `judy`'s asynchronous search.""}, {""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""This means that `judy` can perform GET and DELETE operations on asynchronous searches submitted by `elon`, but not the reverse.\n\nIf none of the users have any backend roles, all three will be able to see the others' searches.\n\nFor example, consider three users: `judy`, `elon`, and `jack`.\n\n`judy`, `elon`, and `jack` have no backend roles set up:\n\n```json\nPUT _plugins/_security/api/internalusers/judy\n{\n  \""password\"": \""judy\"",\n  \""backend_roles\"": [],\n  \""attributes\"": {}\n}\n```\n\n```json\nPUT _plugins/_security/api/internalusers/elon\n{\n  \""password\"": \""elon\"",\n  \""backend_roles\"": [],\n  \""attributes\"": {}\n}\n```\n\n```json\nPUT _plugins/_security/api/internalusers/jack\n{\n  \""password\"": \""jack\"",\n  \""backend_roles\"": [],""}, {""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""For example, users of different departments in an organization can view asynchronous searches owned by their own department.\n\nFirst, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually.\n\nNow when users view asynchronous search resources in OpenSearch Dashboards (or make REST API calls), they only see asynchronous searches submitted by users who have a subset of the backend role.\nFor example, consider two users: `judy` and `elon`.\n\n`judy` has an IT backend role:\n\n```json\nPUT _plugins/_security/api/internalusers/judy\n{\n  \""password\"": \""judy\"",\n  \""backend_roles\"": [\n    \""IT\""\n  ],\n  \""attributes\"": {}\n}\n```\n\n`elon` has an admin backend role:\n\n```json\nPUT _plugins/_security/api/internalusers/elon\n{\n  \""password\"": \""elon\"",\n  \""backend_roles\"": [\n    \""admin\""\n  ],\n  \""attributes\"": {}\n}\n```\n\nBoth `judy` and `elon` have full access to asynchronous search:\n\n```json\nPUT _plugins/_security/api/rolesmapping/async_full_access\n{\n  \""backend_roles\"": [],\n  \""hosts\"": [],\n  \""users\"": [\n    \""judy\"",\n    \""elon\""\n  ]\n}\n```\n\nBecause they have different backend roles, an asynchronous search submitted by `judy` will not be visible to `elon` and vice versa.""}, {""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""Basic permissions\n\nAs an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see Asynchronous search.\n\nThe Security plugin has two built-in roles that cover most asynchronous search use cases: `asynchronous_search_full_access` and `asynchronous_search_read_access`. For descriptions of each, see Predefined roles.\n\nIf these roles don\u2019t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the `cluster:admin/opensearch/asynchronous_search/delete` permission lets you delete a previously submitted asynchronous search.""}, {""relative_path"": ""documentation-website/_search-plugins/async/security.md"", ""chunk"": ""Basic permissions\n\nAs an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see Asynchronous search.\n\nThe Security plugin has two built-in roles that cover most asynchronous search use cases: `asynchronous_search_full_access` and `asynchronous_search_read_access`. For descriptions of each, see Predefined roles.\n\nIf these roles don\u2019t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the `cluster:admin/opensearch/asynchronous_search/delete` permission lets you delete a previously submitted asynchronous search.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
