query_text,relevant_doc_ids,retrieved_doc_ids,retrieved_chunks,precision@1,recall@1,ndcg@1,precision@3,recall@3,ndcg@3,precision@5,recall@5,ndcg@5,precision@10,recall@10,ndcg@10
I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working?,"[""_im-plugin/index-codecs.md"", ""_tuning-your-cluster/performance.md""]","[""_tuning-your-cluster/performance.md"", ""_im-plugin/index-codecs.md"", ""_ml-commons-plugin/custom-local-models.md""]","[{""relative_path"": ""_tuning-your-cluster/performance.md"", ""chunk"": ""Compression codecs\n\nIn OpenSearch 2.9 and later, there are two new codecs for compression: `zstd` and `zstd_no_dict`. You can optionally specify a compression level for these in the `index.codec.compression_level` setting with values in the 1, 6] range. [Benchmark data shows that `zstd` provides a 7% better write throughput and `zstd_no_dict` provides a 14% better throughput, along with a 30% improvement in storage compared with the `default` codec. For more information about compression, see Index codecs.""}, {""relative_path"": ""_im-plugin/index-codecs.md"", ""chunk"": ""Supported codecs\n\nOpenSearch provides support for four codecs that can be used for compressing the stored fields. Each codec offers different tradeoffs between compression ratio (storage size) and indexing performance (speed): \n\n* `default` -- This codec employs the LZ4 algorithm) with a preset dictionary, which prioritizes performance over compression ratio. It offers faster indexing and search operations when compared with `best_compression` but may result in larger index/shard sizes. If no codec is provided in the index settings, then LZ4 is used as the default algorithm for compression.\n* `best_compression` -- This codec uses zlib as an underlying algorithm for compression. It achieves high compression ratios that result in smaller index sizes. However, this may incur additional CPU usage during index operations and may subsequently result in high indexing and search latencies. \n\nAs of OpenSearch 2.9, two new codecs based on the Zstandard compression algorithm are available. This algorithm provides a good balance between compression ratio and speed.\n\nIt may be challenging to change the codec setting of an existing index (see Changing an index codec), so it is important to test a representative workload in a non-production environment before using a new codec setting.\n{: .important}\n\n* `zstd` (OpenSearch 2.9 and later) -- This codec provides significant compression comparable to the `best_compression` codec with reasonable CPU usage and improved indexing and search performance compared to the `default` codec.\n* `zstd_no_dict` (OpenSearch 2.9 and later) -- This codec is similar to `zstd` but excludes the dictionary compression feature. It provides faster indexing and search operations compared to `zstd` at the expense of a slightly larger index size.\n\nAs of OpenSearch 2.10, the `zstd` and `zstd_no_dict` compression codecs cannot be used for k-NN or Security Analytics indexes.\n{: .warning}\n\nFor the `zstd` and `zstd_no_dict` codecs, you can optionally specify a compression level in the `index.codec.compression_level` setting. This setting takes integers in the [1, 6] range. A higher compression level results in a higher compression ratio (smaller storage size) with a tradeoff in speed (slower compression and decompression speeds lead to greater indexing and search latencies). \n\nWhen an index segment is created, it uses the current index codec for compression. If you update the index codec, any segment created after the update will use the new compression algorithm. For specific operation considerations, see Index codec considerations for index operations.\n{: .note}\n\nAs of OpenSearch 2.15, hardware-accelerated compression codecs for the `DEFLATE` and `LZ4` compression algorithms are available. These hardware-accelerated codecs are available on the latest 4th and 5th Gen Intel\u00ae\ufe0f Xeon\u00ae\ufe0f processors running Linux kernel 3.10 and later. For all other systems and platforms, the codecs use that platform's corresponding software implementations. \n\nThe new hardware-accelerated codecs can be used by setting one of the following `index.codec` values:\n* `qat_lz4` (OpenSearch 2.15 and later): Hardware-accelerated `LZ4`\n* `qat_deflate` (OpenSearch 2.15 and later): Hardware-accelerated `DEFLATE`\n\n`qat_deflate` offers a much better compression ratio than `qat_lz4`, with a modest drop in compression and decompression speed.\n{: .note}\n\nThe `index.codec.compression_level` setting can be used to specify the compression level for both `qat_lz4` and `qat_deflate`. \n\nThe `index.codec.qatmode` setting controls the behavior of the hardware accelerator and uses one of the following values:\n\n* `auto`: If hardware acceleration fails, then the algorithm switches to software acceleration.\n* `hardware`: Guarantees hardware-only compression. If hardware is not available, then an exception occurs until hardware exists.\n\nFor information about the `index.codec.qatmode` setting's effects on snapshots, see the Snapshots section.\n\nFor more information about hardware acceleration on Intel, see the Intel (R) QAT accelerator overview.""}, {""relative_path"": ""_im-plugin/index-codecs.md"", ""chunk"": ""Performance tuning and benchmarking\n\nDepending on your specific use case, you might need to experiment with different index codec settings to fine-tune the performance of your OpenSearch cluster. Conducting benchmark tests with different codecs and measuring the impact on indexing speed, search performance, and resource utilization can help you identify the optimal index codec setting for your workload. With the `zstd` and `zstd_no_dict` codecs, you can also fine-tune the compression level in order to identify the optimal configuration for your cluster.""}, {""relative_path"": ""_im-plugin/index-codecs.md"", ""chunk"": ""Snapshots\n\nWhen creating a snapshot, the index codec setting influences the size of the snapshot and the time required for its creation. If the codec of an index is updated, newly created snapshots will use the latest codec setting. The resulting snapshot size will reflect the compression characteristics of the latest codec setting. Existing segments included in the snapshot will retain their original compression characteristics. \n\nWhen you restore the indexes from a snapshot of a cluster to another cluster, it is important to verify that the target cluster supports the codecs of the segments in the source snapshot. For example, if the source snapshot contains segments of the `zstd` or `zstd_no_dict` codecs (introduced in OpenSearch 2.9), you won't be able to restore the snapshot to a cluster that runs on an older OpenSearch version because it doesn't support these codecs. \n\nFor hardware-accelerated compression codecs, available in OpenSearch 2.15 and later, the value of `index.codec.qatmode` affects how snapshots and restores are performed. If the value is `auto` (the default), then snapshots and restores work without issue. However, if the value is `hardware`, then it must be reset to `auto` in order for the restore process to succeed on systems lacking the hardware accelerator.\n\nYou can modify the value of `index.codec.qatmode` during the restore process by setting its value as follows: `\""index_settings\"": {\""index.codec.qatmode\"": \""auto\""}`.\n{: .note}""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Model format\n\nTo use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the TorchScript and ONNX formats.\n\nYou must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript model file.\n\nAdditionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:\n\n```bash\nshasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip\n```""}]",1.0,0.5,1.0,0.6666666666666666,1.0,1.0,0.4,1.0,1.0,0.2,1.0,1.0
"I'm trying to set up this new aggregate view thing for saved objects in OpenSearch Dashboards, but I'm worried about messing up our existing multi-tenancy setup. The docs mention something about tenant indexes and a kibana_server role. How do I make sure I don't break anything when I turn this feature on? And what's the deal with not being able to turn it off once it's enabled","[""_security/multi-tenancy/dynamic-config.md"", ""_security/multi-tenancy/mt-agg-view.md"", ""_security/multi-tenancy/multi-tenancy-config.md""]","[""_security/multi-tenancy/multi-tenancy-config.md"", ""_security/configuration/best-practices.md"", ""_security/multi-tenancy/mt-agg-view.md"", ""_security/multi-tenancy/dynamic-config.md""]","[{""relative_path"": ""_security/multi-tenancy/multi-tenancy-config.md"", ""chunk"": ""Multi-tenancy configuration\n\nMulti-tenancy is enabled in OpenSearch Dashboards by default. If you need to disable or change settings related to multi-tenancy, see the `kibana` settings in `config/opensearch-security/config.yml`, as shown in the following example:\n\n```yml\nconfig:\n  dynamic:\n    kibana:\n      multitenancy_enabled: true\n      private_tenant_enabled: true\n      default_tenant: global tenant\n      server_username: kibanaserver\n      index: '.kibana'\n    do_not_fail_on_forbidden: false\n```\n\n| Setting | Description |\n| :--- | :--- |\n| `multitenancy_enabled` | Enable or disable multi-tenancy. Default is `true`. |\n| `private_tenant_enabled` | Enable or disable the private tenant. Default is `true`. |\n| `default_tenant` | Use to set the tenant that is available when users log in. |\n| `server_username` | Must match the name of the OpenSearch Dashboards server user in `opensearch_dashboards.yml`. Default is `kibanaserver`. If a different user is configured, then make sure that user is mapped to the `kibana_server` role through the `role_mappings.yml` file in order to give them the appropriate permissions listed in kibana_server role details. |\n| `index` | Must match the name of the OpenSearch Dashboards index from `opensearch_dashboards.yml`. Default is `.kibana`. |\n| `do_not_fail_on_forbidden` | When `true`, the Security plugin removes any content that a user is not allowed to see from the search results. When `false`, the plugin returns a security exception. Default is `false`. |\n\nThe `opensearch_dashboards.yml` file includes additional settings:\n\n```yml\nopensearch.username: kibanaserver\nopensearch.password: kibanaserver\nopensearch.requestHeadersAllowlist: [\""securitytenant\"",\""Authorization\""]\nopensearch_security.multitenancy.enabled: true\nopensearch_security.multitenancy.tenants.enable_global: true\nopensearch_security.multitenancy.tenants.enable_private: true\nopensearch_security.multitenancy.tenants.preferred: [\""Private\"", \""Global\""]\nopensearch_security.multitenancy.enable_filter: false\n```\n\n| Setting | Description |\n| :--- | :--- |\n| `opensearch.requestHeadersAllowlist` | OpenSearch Dashboards requires that you add all HTTP headers to the allow list so that the headers pass to OpenSearch. Multi-tenancy uses a specific header, `securitytenant`, that must be present with the standard `Authorization` header. If the `securitytenant` header is not on the allow list, OpenSearch Dashboards starts with a red status.\n| `opensearch_security.multitenancy.enabled` | Enables or disables multi-tenancy in OpenSearch Dashboards. Default is `true`. |\n| `opensearch_security.multitenancy.tenants.enable_global` | Enables or disables the global tenant. Default is `true`. |\n| `opensearch_security.multitenancy.tenants.enable_private` | Enables or disables private tenants. Default is `true`. |\n| `opensearch_security.multitenancy.tenants.preferred` | Lets you change ordering in the **Tenants** tab of OpenSearch Dashboards. By default, the list starts with Global and Private (if enabled) and then proceeds alphabetically. You can add tenants here to move them to the top of the list. |\n| `opensearch_security.multitenancy.enable_filter` | If you have many tenants, you can add a search bar to the top of the list. Default is `false`. |""}, {""relative_path"": ""_security/configuration/best-practices.md"", ""chunk"": ""7. Consider disabling the private tenant\n\nIn many cases, the use of private tenants is unnecessary, although this feature is enabled by default. As a result, every OpenSearch Dashboards user is provided with their own private tenant and a corresponding new index in which to save objects. This can lead to a large number of unnecessary indexes. Evaluate whether private tenants are needed in your cluster. If private tenants are not needed, disable the feature by adding the following configuration to the `config.yml` file:\n\n```yaml\nconfig:\n  dynamic:\n    kibana:\n      multitenancy_enabled: true\n      private_tenant_enabled: false\n```\n{% include copy.html %}""}, {""relative_path"": ""_security/multi-tenancy/mt-agg-view.md"", ""chunk"": ""OpenSearch Dashboards multi-tenancy aggregate view for saved objects\n\nThis is an experimental feature released in OpenSearch 2.4 and is not recommended for use in a production environment. For updates on the progress of the feature or if you want to leave feedback, see the Dashboards object sharing GitHub issue. For a more comprehensive view of the proposed future development of multi-tenancy, see the Dashboards object sharing issue.\n{: .warning}\n\nAggregate view for saved objects allows a user who has access to multiple tenants to see all saved objects associated with those tenants in a single view without having to switch between tenants to do so. This includes both tenants created by the user and tenants shared with the user. Aggregate view introduces a Tenant dropdown menu and column in the Saved Objects table that gives the user the option to filter by tenants and make visible their associated saved objects.\n\nOnce you identify a saved object of interest, you can then switch to that tenant to work with the object.\n\nTo access saved objects, expand the top menu and select **Management > Dashboards Management > Saved Objects**. The Saved Objects window opens. By default, all tenants the user has permissions for are displayed along with all saved objects associated with the tenants.\n\nAs an experimental feature, aggregate view for saved objects is kept behind a feature flag and must be enabled in the `opensearch_dashboards.yml` file before the feature is made available. See Enabling aggregate view for more information.\n{: .note }""}, {""relative_path"": ""_security/multi-tenancy/dynamic-config.md"", ""chunk"": ""Configuring multi-tenancy in OpenSearch Dashboards\n\nTo configure multi-tenancy in Dashboards, follow these steps:\n\n1. Begin by selecting **Security** in the Dashboards home page menu. Then select **Tenancy** from the Security menu on the left side of the screen. The **Multi-tenancy** page is displayed. \n1. By default, the **Manage** tab is displayed. Select the **Configure** tab to display the dynamic settings for multi-tenancy.\n   * In the **Multi-tenancy** field, select the **Enable tenancy** check box to enable multi-tenancy. Clear the check box to disable the feature. The default is `true`.\n   * In the **Tenants** field, you can enable or disable private tenants for users. By default the check box is selected and the feature is enabled.\n   * In the **Default tenant** field, use the dropdown menu to select a default tenant. The menu includes Global, Private, and any other custom tenants that are available to users.\n1. After making your preferred changes, select **Save changes** in the lower right corner of the window. A pop-up window appears listing the configuration items you've changed and asks you to review your changes.\n1. Select the check boxes beside the items you want to confirm and then select **Apply changes**. The changes are implemented dynamically.""}, {""relative_path"": ""_security/multi-tenancy/mt-agg-view.md"", ""chunk"": ""Enabling aggregate view for saved objects\n\nBy default, the aggregate view in the Saved Objects table is disabled. To enable the feature, add the `opensearch_security.multitenancy.enable_aggregation_view` flag to the `opensearch_dashboards.yml` file and set it to `true`:\n\n`opensearch_security.multitenancy.enable_aggregation_view: true`\n\nAfter enabling the feature you can start the new cluster and then launch Dashboards.""}]",1.0,0.3333333333333333,1.0,0.6666666666666666,0.6666666666666666,0.7039180890341347,0.6,1.0,0.9060254355346823,0.3,1.0,0.9060254355346823
What software license does Opensearch use?,"[""README.md"",""_about/index.md""]","[""_getting-started/index.md"", ""_about/index.md"", ""_data-prepper/pipelines/configuration/sources/opensearch.md"", ""_getting-started/intro.md"", ""TERMS.md""]","[{""relative_path"": ""_getting-started/index.md"", ""chunk"": ""Getting started\n\nOpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indexes, boost fields, rank results by score, sort results by field, and aggregate results.\n\nUnsurprisingly, builders often use a search engine like OpenSearch as the backend for a search application---think Wikipedia or an online store. It offers excellent performance and can scale up or down as the needs of the application grow or shrink.\n\nAn equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours. You can use OpenSearch Dashboards to build these sorts of visualizations from data in OpenSearch.""}, {""relative_path"": ""_about/index.md"", ""chunk"": ""Get involved\n\nOpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub.\nThe project welcomes GitHub issues, bug fixes, features, plugins, documentation---anything at all. To get involved, see Contributing on the OpenSearch website.\n\n---\n\nOpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V.""}, {""relative_path"": ""_data-prepper/pipelines/configuration/sources/opensearch.md"", ""chunk"": ""opensearch\n\nThe `opensearch` source plugin is used to read indexes from an OpenSearch cluster, a legacy Elasticsearch cluster, an Amazon OpenSearch Service domain, or an Amazon OpenSearch Serverless collection.\n\nThe plugin supports OpenSearch 2.x and Elasticsearch 7.x.""}, {""relative_path"": ""_getting-started/intro.md"", ""chunk"": ""Introduction to OpenSearch\n\nOpenSearch is a distributed search and analytics engine that supports various use cases, from implementing a search box on a website to analyzing security data for threat detection. The term _distributed_ means that you can run OpenSearch on multiple computers. _Search and analytics_ means that you can search and analyze your data once you ingest it into OpenSearch. No matter your type of data, you can store and analyze it using OpenSearch.""}, {""relative_path"": ""TERMS.md"", ""chunk"": ""O\n\n**onsite**\n\n**OpenSearch**\n\nOpenSearch is a community-driven, open-source search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 and Kibana 7.10.2. It consists of a search engine daemon, OpenSearch, and a visualization and user interface, OpenSearch Dashboards.\n\n**OpenSearch Dashboards**\n\nThe default visualization tool for data in OpenSearch. On first appearance, use the full name. *Dashboards* may be used on subsequent appearances.\n\nopen source (n.), open-source (adj.)\n\nUse _open source_ as a noun (for example, \""The code used throughout this tutorial is open source and can be freely modified\""). Use _open-source_ as an adjective _(open-source software)_.\n\n**OpenSearch Playground**\n\nDo not precede with _the_. OpenSearch Playground provides a central location for existing and evaluating users to explore features in OpenSearch and OpenSearch Dashboards without downloading or installing any OpenSearch components locally.\n\n**operating system**\n\nWhen referencing operating systems in documentation, follow these guidelines:\n\n- In general, if your docs or procedures apply to both Linux and macOS, you can also include Unix.\n- Unix and UNIX aren't the same. UNIX is a trademarked name that's owned by The Open Group. In most cases, you should use Unix.\n- When referring to the Mac operating system, use macOS. Don't say Mac, Mac OS, or OS X.\n- When referring to Windows, it's not necessary to prefix with Microsoft.\n- If you need to reference multiple Unix-like operating systems, you should separate by commas and use the following order: Linux, macOS, or Unix.\n\n**or earlier, or later**\n\nOK to use with software versions.""}]",0.0,0.0,0.0,0.3333333333333333,0.5,0.38685280723454163,0.2,0.5,0.38685280723454163,0.1,0.5,0.38685280723454163
Does GPU accelerated nodes support Pytorch?,"[""_ml-commons-plugin/gpu-acceleration.md""]","[""_ml-commons-plugin/gpu-acceleration.md"", ""_ml-commons-plugin/using-ml-models.md""]","[{""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""PyTorch\n\nGPU-accelerated ML nodes require PyTorch 1.12.1 work with ML models.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Setting up a GPU-accelerated ML node\n\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Supported GPUs\n\nCurrently, ML nodes support the following GPU instances:\n\n- NVIDIA instances with CUDA 11.6\n- AWS Inferentia\n\nIf you need GPU power, you can provision GPU instances through Amazon Elastic Compute Cloud (Amazon EC2). For more information on how to provision a GPU instance, see Recommended GPU Instances.""}, {""relative_path"": ""_ml-commons-plugin/using-ml-models.md"", ""chunk"": ""GPU acceleration\n\nFor better performance, you can take advantage of GPU acceleration on your ML node. For more information, see GPU acceleration.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Supported images\n\nYou can use GPU acceleration with both Docker images with CUDA 11.6 and Amazon Machine Images (AMIs).""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
Does opensearch support hugging face models? If so which ones?,"[""_ml-commons-plugin/pretrained-models.md""]","[""_ml-commons-plugin/pretrained-models.md"", ""_ml-commons-plugin/custom-local-models.md"", ""_ml-commons-plugin/agents-tools/tools/vector-db-tool.md""]","[{""relative_path"": ""_ml-commons-plugin/pretrained-models.md"", ""chunk"": ""Supported pretrained models\n\nOpenSearch supports the following models, categorized by type. Text embedding models are sourced from Hugging Face. Sparse encoding models are trained by OpenSearch. Although models with the same type will have similar use cases, each model has a different model size and will perform differently depending on your cluster setup. For a performance comparison of some pretrained models, see the SBERT documentation.\n\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\n{: .important}""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Model support\n\nAs of OpenSearch 2.6, OpenSearch supports local text embedding models.\n\nAs of OpenSearch 2.11, OpenSearch supports local sparse encoding models.\n\nAs of OpenSearch 2.12, OpenSearch supports local cross-encoder models.\n\nAs of OpenSearch 2.13, OpenSearch supports local question answering models.\n\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\n{: .important}""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Model format\n\nTo use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the TorchScript and ONNX formats.\n\nYou must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript model file.\n\nAdditionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:\n\n```bash\nshasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip\n```""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Prerequisites\n\nTo upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from Hugging Face, or train a new model in accordance with your needs.""}, {""relative_path"": ""_ml-commons-plugin/agents-tools/tools/vector-db-tool.md"", ""chunk"": ""Step 1: Register and deploy a sparse encoding model\n\nOpenSearch supports several pretrained models. You can use one of those models, use your own custom model, or create a connector for an externally hosted model. For a list of supported pretrained models, see OpenSearch-provided pretrained models. For more information about custom models, see Custom local models. For information about integrating an externally hosted model, see Connecting to externally hosted models. \n\nIn this example, you'll use the `huggingface/sentence-transformers/all-MiniLM-L12-v2` pretrained model for both ingestion and search. To register and deploy the model to OpenSearch, send the following request:\n\n```json\nPOST /_plugins/_ml/models/_register?deploy=true\n{\n  \""name\"": \""huggingface/sentence-transformers/all-MiniLM-L12-v2\"",\n  \""version\"": \""1.0.1\"",\n  \""model_format\"": \""TORCH_SCRIPT\""\n}\n```\n{% include copy-curl.html %} \n\nOpenSearch responds with a task ID for the model registration and deployment task:\n\n```json\n{\n  \""task_id\"": \""M_9KY40Bk4MTqirc5lP8\"",\n  \""status\"": \""CREATED\""\n}\n```\n\nYou can monitor the status of the task by calling the Tasks API:\n\n```json\nGET _plugins/_ml/tasks/M_9KY40Bk4MTqirc5lP8\n```\n{% include copy-curl.html %} \n\nOnce the model is registered and deployed, the task `state` changes to `COMPLETED` and OpenSearch returns a model ID for the model:\n\n```json\n{\n  \""model_id\"": \""Hv_PY40Bk4MTqircAVmm\"",\n  \""task_type\"": \""REGISTER_MODEL\"",\n  \""function_name\"": \""TEXT_EMBEDDING\"",\n  \""state\"": \""COMPLETED\"",\n  \""worker_node\"": [\n    \""UyQSTQ3nTFa3IP6IdFKoug\""\n  ],\n  \""create_time\"": 1706767869692,\n  \""last_update_time\"": 1706767935556,\n  \""is_async\"": true\n}\n```""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
"I have a custom model, can I run it in Opensearch?","[""_ml-commons-plugin/custom-local-models.md"",""_ml-commons-plugin/api/model-apis/register-model.md""]","[""_ml-commons-plugin/api/model-apis/register-model.md"", ""_ml-commons-plugin/custom-local-models.md"", ""_ml-commons-plugin/agents-tools/tools/vector-db-tool.md"", ""_ml-commons-plugin/integrating-ml-models.md""]","[{""relative_path"": ""_ml-commons-plugin/api/model-apis/register-model.md"", ""chunk"": ""Register a custom model\n\nTo use a custom model locally within the OpenSearch cluster, you need to provide a URL and a config object for that model. For more information, see Custom local models.""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Prerequisites\n\nTo upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from Hugging Face, or train a new model in accordance with your needs.""}, {""relative_path"": ""_ml-commons-plugin/agents-tools/tools/vector-db-tool.md"", ""chunk"": ""Step 1: Register and deploy a sparse encoding model\n\nOpenSearch supports several pretrained models. You can use one of those models, use your own custom model, or create a connector for an externally hosted model. For a list of supported pretrained models, see OpenSearch-provided pretrained models. For more information about custom models, see Custom local models. For information about integrating an externally hosted model, see Connecting to externally hosted models. \n\nIn this example, you'll use the `huggingface/sentence-transformers/all-MiniLM-L12-v2` pretrained model for both ingestion and search. To register and deploy the model to OpenSearch, send the following request:\n\n```json\nPOST /_plugins/_ml/models/_register?deploy=true\n{\n  \""name\"": \""huggingface/sentence-transformers/all-MiniLM-L12-v2\"",\n  \""version\"": \""1.0.1\"",\n  \""model_format\"": \""TORCH_SCRIPT\""\n}\n```\n{% include copy-curl.html %} \n\nOpenSearch responds with a task ID for the model registration and deployment task:\n\n```json\n{\n  \""task_id\"": \""M_9KY40Bk4MTqirc5lP8\"",\n  \""status\"": \""CREATED\""\n}\n```\n\nYou can monitor the status of the task by calling the Tasks API:\n\n```json\nGET _plugins/_ml/tasks/M_9KY40Bk4MTqirc5lP8\n```\n{% include copy-curl.html %} \n\nOnce the model is registered and deployed, the task `state` changes to `COMPLETED` and OpenSearch returns a model ID for the model:\n\n```json\n{\n  \""model_id\"": \""Hv_PY40Bk4MTqircAVmm\"",\n  \""task_type\"": \""REGISTER_MODEL\"",\n  \""function_name\"": \""TEXT_EMBEDDING\"",\n  \""state\"": \""COMPLETED\"",\n  \""worker_node\"": [\n    \""UyQSTQ3nTFa3IP6IdFKoug\""\n  ],\n  \""create_time\"": 1706767869692,\n  \""last_update_time\"": 1706767935556,\n  \""is_async\"": true\n}\n```""}, {""relative_path"": ""_ml-commons-plugin/integrating-ml-models.md"", ""chunk"": ""Choosing a model\n\nTo integrate an ML model into your search workflow, choose one of the following options:\n\n1. **Local model**: Upload a model to the OpenSearch cluster and use it locally. This option allows you to serve the model in your OpenSearch cluster but may require significant system resources.\n\n    1. **Pretrained model provided by OpenSearch**: This option requires minimal setup and avoids the time and effort required to train a custom model.\n\n        For a list of supported models and information about using a pretrained model provided by OpenSearch, see Pretrained models. \n\n    1. **Custom model**: This option offers customization for your specific use case.\n\n        For information about uploading your model, see Using ML models within OpenSearch.\n\n1. **Externally hosted model**: This option allows you to connect to a model hosted on a third-party platform. It requires more setup but allows the use of models that are already hosted on a service other than OpenSearch.     \n    \n    To connect to an externally hosted model, you need to set up a connector:  \n\n    - For a walkthrough with detailed steps, see Connecting to externally hosted models.\n    - For more information about supported connectors, see Connectors.\n    - For information about creating your own connector, see Connector blueprints.\n\nIn OpenSearch version 2.9 and later, you can integrate local and external models simultaneously within a single cluster.\n{: .note}""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Model format\n\nTo use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the TorchScript and ONNX formats.\n\nYou must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript model file.\n\nAdditionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:\n\n```bash\nshasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip\n```""}]",1.0,0.5,1.0,0.6666666666666666,1.0,1.0,0.4,1.0,1.0,0.2,1.0,1.0
"I have a model and some ML nodes, how do I boost it's performance?","[""_ml-commons-plugin/gpu-acceleration.md"", ""_ml-commons-plugin/custom-local-models.md"", ""_ml-commons-plugin/using-ml-models.md""]","[""_ml-commons-plugin/using-ml-models.md"", ""_ml-commons-plugin/gpu-acceleration.md"", ""_ml-commons-plugin/custom-local-models.md"", ""_ml-commons-plugin/cluster-settings.md""]","[{""relative_path"": ""_ml-commons-plugin/using-ml-models.md"", ""chunk"": ""GPU acceleration\n\nFor better performance, you can take advantage of GPU acceleration on your ML node. For more information, see GPU acceleration.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""Setting up a GPU-accelerated ML node\n\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts.""}, {""relative_path"": ""_ml-commons-plugin/gpu-acceleration.md"", ""chunk"": ""PyTorch\n\nGPU-accelerated ML nodes require PyTorch 1.12.1 work with ML models.""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Cluster settings\n\nThis example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. \n\nOn clusters with dedicated ML nodes, specify `\""only_run_on_ml_node\"": \""true\""` for improved performance. For more information, see ML Commons cluster settings.\n\nTo ensure that this basic local setup works, specify the following cluster settings:\n\n```json\nPUT _cluster/settings\n{\n  \""persistent\"": {\n    \""plugins\"": {\n      \""ml_commons\"": {\n        \""allow_registering_model_via_url\"": \""true\"",\n        \""only_run_on_ml_node\"": \""false\"",\n        \""model_access_control_enabled\"": \""true\"",\n        \""native_memory_threshold\"": \""99\""\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}, {""relative_path"": ""_ml-commons-plugin/cluster-settings.md"", ""chunk"": ""Setting\n\n```\nplugins.ml_commons.max_model_on_node: 10\n```""}]",1.0,0.3333333333333333,1.0,1.0,1.0,1.0,0.6,1.0,1.0,0.3,1.0,1.0
Can you show me an example of how to use lat/long coordinates?,"[""_field-types/supported-field-types/geo-point.md""]","[""API_STYLE_GUIDE.md"", ""_api-reference/index-apis/segment.md"", ""_api-reference/index-apis/stats.md"", ""_api-reference/index-apis/component-template.md"", ""_ml-commons-plugin/remote-models/connectors.md""]","[{""relative_path"": ""API_STYLE_GUIDE.md"", ""chunk"": ""Path and HTTP methods\n\nFor relatively complex API calls that include path parameters, it's sometimes a good idea to provide an example so that users can visualize how the request is properly formed. This section is optional and includes examples that illustrate how the endpoint and path parameters fit together in the request. The following is an example of this section for the nodes stats API:\n\n```json\nGET /_nodes/stats\nGET /_nodes//stats\nGET /_nodes/stats/\nGET /_nodes//stats/\nGET /_nodes/stats//\nGET /_nodes//stats//\n```""}, {""relative_path"": ""_api-reference/index-apis/segment.md"", ""chunk"": ""Example requests\n\nThe following example requests show you how to use the Segment API.""}, {""relative_path"": ""_api-reference/index-apis/stats.md"", ""chunk"": ""Example requests\n\nThe following example requests show how to use the Index Stats API.""}, {""relative_path"": ""_api-reference/index-apis/component-template.md"", ""chunk"": ""Example requests\n\nThe following example requests show how to use the Component Template API.""}, {""relative_path"": ""_ml-commons-plugin/remote-models/connectors.md"", ""chunk"": ""Connector examples\n\nThe following sections contain examples of connectors for popular ML platforms. For a full list of supported connectors, see Supported connectors.""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
How do I use vector search?,"[""_search-plugins/vector-search.md"",""_search-plugins/knn/index.md"", ""_search-plugins/knn/knn-index.md"", ""_field-types/supported-field-types/knn-vector.md""]","[""_search-plugins/vector-search.md"", ""_ml-commons-plugin/remote-models/index.md"", ""_ml-commons-plugin/custom-local-models.md"", ""_automating-configurations/workflow-templates.md""]","[{""relative_path"": ""_search-plugins/vector-search.md"", ""chunk"": ""Vector search with filtering\n\nFor information about vector search with filtering, see k-NN search with filters.""}, {""relative_path"": ""_ml-commons-plugin/remote-models/index.md"", ""chunk"": ""Step 7: Use the model for search\n\nTo learn how to use the model for vector search, see Using an ML model for neural search.""}, {""relative_path"": ""_ml-commons-plugin/custom-local-models.md"", ""chunk"": ""Step 5: Use the model for search\n\nTo learn how to use the model for vector search, see Using an ML model for neural search.""}, {""relative_path"": ""_search-plugins/vector-search.md"", ""chunk"": ""Vector search\n\nOpenSearch is a comprehensive search platform that supports a variety of data types, including vectors. OpenSearch vector database functionality is seamlessly integrated with its generic database function.\n\nIn OpenSearch, you can generate vector embeddings, store those embeddings in an index, and use them for vector search. Choose one of the following options:\n\n- Generate embeddings using a library of your choice before ingesting them into OpenSearch. Once you ingest vectors into an index, you can perform a vector similarity search on the vector space. For more information, see Working with embeddings generated outside of OpenSearch. \n- Automatically generate embeddings within OpenSearch. To use embeddings for semantic search, the ingested text (the corpus) and the query need to be embedded using the same model. Neural search packages this functionality, eliminating the need to manage the internal details. For more information, see Generating vector embeddings within OpenSearch.""}, {""relative_path"": ""_automating-configurations/workflow-templates.md"", ""chunk"": ""Step 3: Perform vector search\n\nTo perform a vector search on your index, use a `neural` query clause:\n\n```json\nGET /my-nlp-index/_search\n{\n  \""_source\"": {\n    \""excludes\"": [\n      \""passage_embedding\""\n    ]\n  },\n  \""query\"": {\n    \""neural\"": {\n      \""passage_embedding\"": {\n        \""query_text\"": \""Hi world\"",\n        \""k\"": 100\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}]",1.0,0.25,1.0,0.3333333333333333,0.25,0.46927872602275644,0.2,0.25,0.3903800499921017,0.1,0.25,0.3903800499921017
How do I understand the memory requirements for using hnsw?,"[""_search-plugins/knn/knn-vector-quantization.md""]","[""_search-plugins/knn/knn-index.md"", ""_search-plugins/knn/knn-vector-quantization.md""]","[{""relative_path"": ""_search-plugins/knn/knn-index.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for HNSW is estimated to be `1.1 * (4 * dimension + 8 * M)` bytes/vector.\n\nAs an example, assume you have a million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\n\n```\n1.1 * (4 * 256 + 8 * 16) * 1,000,000 ~= 1.267 GB\n```""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.\n\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\n\n```r\n1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB\n```""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.\n\nAs an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:\n\n```r\n1.1 * ((8 / 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB\n```""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""HNSW memory estimation\n\nThe memory required for the Hierarchical Navigable Small World (HNSW) graph can be estimated as `1.1 * (dimension + 8 * M)` bytes/vector, where `M` is the maximum number of bidirectional links created for each element during the construction of the graph.\n\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\n\n```r\n1.1 * (256 + 8 * 16) * 1,000,000 ~= 0.4 GB\n```""}, {""relative_path"": ""_search-plugins/knn/knn-index.md"", ""chunk"": ""HNSW parameters\n\nParameter name | Required | Default | Updatable | Description\n:--- | :--- | :--- | :--- | :---\n`ef_construction` | false | 100 | false | The size of the dynamic list used during k-NN graph creation. Higher values result in a more accurate graph but slower indexing speed.The Lucene engine uses the proprietary term \""beam_width\"" to describe this function, which corresponds directly to \""ef_construction\"". To be consistent throughout the OpenSearch documentation, we retain the term \""ef_construction\"" for this parameter.\n`m` | false | 16 | false | The number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100.The Lucene engine uses the proprietary term \""max_connections\"" to describe this function, which corresponds directly to \""m\"". To be consistent throughout OpenSearch documentation, we retain the term \""m\"" to label this parameter.\n\nLucene HNSW implementation ignores `ef_search`  and dynamically sets it to the value of \""k\"" in the search request. Therefore, there is no need to make settings for `ef_search` when using the Lucene engine.\n{: .note}\n\nAn index created in OpenSearch version 2.11 or earlier will still use the old `ef_construction` value (`512`).\n{: .note}\n\n```json\n\""method\"": {\n    \""name\"":\""hnsw\"",\n    \""engine\"":\""lucene\"",\n    \""space_type\"": \""l2\"",\n    \""parameters\"":{\n        \""m\"":2048,\n        \""ef_construction\"": 245\n    }\n}\n```""}]",0.0,0.0,0.0,0.3333333333333333,1.0,0.6309297535714575,0.2,1.0,0.6309297535714575,0.1,1.0,0.6309297535714575
Can you show me some different examples of using different quantization methods for vectors?,"[""_search-plugins/knn/knn-vector-quantization.md""]","[""_search-plugins/knn/knn-index.md"", ""_search-plugins/knn/knn-vector-quantization.md""]","[{""relative_path"": ""_search-plugins/knn/knn-index.md"", ""chunk"": ""Choosing the right method\n\nThere are several options to choose from when building your `knn_vector` field. To determine the correct methods and parameters, you should first understand the requirements of your workload and what trade-offs you are willing to make. Factors to consider are (1) query latency, (2) query quality, (3) memory limits, and (4) indexing latency.\n\nIf memory is not a concern, HNSW offers a strong query latency/query quality trade-off.\n\nIf you want to use less memory and increase indexing speed as compared to HNSW while maintaining similar query quality, you should evaluate IVF.\n\nIf memory is a concern, consider adding a PQ encoder to your HNSW or IVF index. Because PQ is a lossy encoding, query quality will drop.\n\nYou can reduce the memory footprint by a factor of 2, with a minimal loss in search quality, by using the `fp_16` encoder. If your vector dimensions are within the -128, 127] byte range, we recommend using the [byte quantizer to reduce the memory footprint by a factor of 4. To learn more about vector quantization options, see k-NN vector quantization.""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""k-NN vector quantization\n\nBy default, the k-NN plugin supports the indexing and querying of vectors of type `float`, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping `float` vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native `nmslib` and `faiss` engines). To reduce the memory footprint, you can use vector quantization.\n\nOpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, 16-bit scalar quantization, and product quantization (PQ).""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""Using Faiss product quantization\n\nTo minimize loss in accuracy, PQ requires a _training_ step that builds a model based on the distribution of the data that will be searched.\n\nThe product quantizer is trained by running k-means clustering on a set of training vectors for each subvector space and extracts the centroids to be used for encoding. The training vectors can be either a subset of the vectors to be ingested or vectors that have the same distribution and dimension as the vectors to be ingested.\n\nIn OpenSearch, the training vectors need to be present in an index. In general, the amount of training data will depend on which ANN algorithm is used and how much data will be stored in the index. For IVF-based indexes, a recommended number of training vectors is `max(1000*nlist, 2^code_size * 1000)`. For HNSW-based indexes, a recommended number is `2^code_size*1000`. See the Faiss documentation for more information about the methodology used to calculate these figures.\n\nFor PQ, both _m_ and _code_size_ need to be selected. _m_ determines the number of subvectors into which vectors should be split for separate encoding. Consequently, the _dimension_ needs to be divisible by _m_. _code_size_ determines the number of bits used to encode each subvector. In general, we recommend a setting of `code_size = 8` and then tuning _m_ to get the desired trade-off between memory footprint and recall.\n\nFor an example of setting up an index with PQ, see the Building a k-NN index from a model tutorial.""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""Lucene scalar quantization\n\nStarting with version 2.16, the k-NN plugin supports built-in scalar quantization for the Lucene engine. Unlike the Lucene byte vector, which requires you to quantize vectors before ingesting the documents, the Lucene scalar quantizer quantizes input vectors in OpenSearch during ingestion. The Lucene scalar quantizer converts 32-bit floating-point input vectors into 7-bit integer vectors in each segment using the minimum and maximum quantiles computed based on the `confidence_interval` parameter. During search, the query vector is quantized in each segment using the segment's minimum and maximum quantiles in order to compute the distance between the query vector and the segment's quantized input vectors. \n\nQuantization can decrease the memory footprint by a factor of 4 in exchange for some loss in recall. Additionally, quantization slightly increases disk usage because it requires storing both the raw input vectors and the quantized vectors.""}, {""relative_path"": ""_search-plugins/knn/knn-vector-quantization.md"", ""chunk"": ""Using Faiss scalar quantization\n\nTo use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:\n\n```json\nPUT /test-index\n{\n  \""settings\"": {\n    \""index\"": {\n      \""knn\"": true,\n      \""knn.algo_param.ef_search\"": 100\n    }\n  },\n  \""mappings\"": {\n    \""properties\"": {\n      \""my_vector1\"": {\n        \""type\"": \""knn_vector\"",\n        \""dimension\"": 3,\n        \""method\"": {\n          \""name\"": \""hnsw\"",\n          \""engine\"": \""faiss\"",\n          \""space_type\"": \""l2\"",\n          \""parameters\"": {\n            \""encoder\"": {\n              \""name\"": \""sq\""\n            },\n            \""ef_construction\"": 256,\n            \""m\"": 8\n          }\n        }\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nOptionally, you can specify the parameters in `method.parameters.encoder`. For more information about `encoder` object parameters, see SQ parameters.\n\nThe `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the [-65504.0, 65504.0] range. To define how to handle out-of-range values, the preceding request specifies the `clip` parameter. By default, this parameter is `false`, and any vectors containing out-of-range values are rejected. \n\nWhen `clip` is set to `true` (as in the preceding request), out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is `[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.\n\nWe recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.\n{: .note}\n\nThe following example method definition specifies the Faiss SQfp16 encoder, which rejects any indexing request that contains out-of-range vector values (because the `clip` parameter is `false` by default):\n\n```json\nPUT /test-index\n{\n  \""settings\"": {\n    \""index\"": {\n      \""knn\"": true,\n      \""knn.algo_param.ef_search\"": 100\n    }\n  },\n  \""mappings\"": {\n    \""properties\"": {\n      \""my_vector1\"": {\n        \""type\"": \""knn_vector\"",\n        \""dimension\"": 3,\n        \""method\"": {\n          \""name\"": \""hnsw\"",\n          \""engine\"": \""faiss\"",\n          \""space_type\"": \""l2\"",\n          \""parameters\"": {\n            \""encoder\"": {\n              \""name\"": \""sq\"",\n              \""parameters\"": {\n                \""type\"": \""fp16\""\n              }\n            },\n            \""ef_construction\"": 256,\n            \""m\"": 8\n          }\n        }\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nDuring ingestion, make sure each vector dimension is in the supported range ([-65504.0, 65504.0]).\n\n```json\nPUT test-index/_doc/1\n{\n  \""my_vector1\"": [-65504.0, 65503.845, 55.82]\n}\n```\n{% include copy-curl.html %}\n\nDuring querying, the query vector has no range limitation:\n\n```json\nGET test-index/_search\n{\n  \""size\"": 2,\n  \""query\"": {\n    \""knn\"": {\n      \""my_vector1\"": {\n        \""vector\"": [265436.876, -120906.256, 99.84],\n        \""k\"": 2\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}]",0.0,0.0,0.0,0.3333333333333333,1.0,0.6309297535714575,0.2,1.0,0.6309297535714575,0.1,1.0,0.6309297535714575
"Would you recommend I use ReRank? If so, what type of model would you recommend for ReRank?","[""_search-plugins/search-relevance/reranking-search-results.md""]","[""_search-plugins/search-pipelines/rerank-processor.md"", ""_search-plugins/search-relevance/reranking-search-results.md"", ""_ml-commons-plugin/tutorials/reranking-cohere.md"", ""_search-plugins/search-relevance/compare-search-results.md""]","[{""relative_path"": ""_search-plugins/search-pipelines/rerank-processor.md"", ""chunk"": ""Rerank processor\nIntroduced 2.12\n{: .label .label-purple }\n\nThe `rerank` search request processor intercepts search results and passes them to a cross-encoder model to be reranked. The model reranks the results, taking into account the scoring context. Then the processor orders documents in the search results based on their new scores.""}, {""relative_path"": ""_search-plugins/search-relevance/reranking-search-results.md"", ""chunk"": ""Running a search with reranking\n\nTo run a search with reranking, follow these steps:\n\n1. Configure a search pipeline.\n1. Create an index for ingestion.\n1. Ingest documents into the index.\n1. Search using reranking.""}, {""relative_path"": ""_ml-commons-plugin/tutorials/reranking-cohere.md"", ""chunk"": ""Reranking search results using the Cohere Rerank model\n\nA reranking pipeline can rerank search results, providing a relevance score for each document in the search results with respect to the search query. The relevance score is calculated by a cross-encoder model. \n\nThis tutorial illustrates how to use the Cohere Rerank model in a reranking pipeline. \n\nReplace the placeholders beginning with the prefix `your_` with your own values.\n{: .note}""}, {""relative_path"": ""_search-plugins/search-relevance/compare-search-results.md"", ""chunk"": ""Reranking results with Amazon Kendra Intelligent Ranking for OpenSearch\n\nAn example of a reranker is **Amazon Kendra Intelligent Ranking for OpenSearch**, contributed by the Amazon Kendra team. This plugin takes search results from OpenSearch and applies Amazon Kendra\u2019s semantic relevance rankings calculated using vector embeddings and other semantic search techniques. For many applications, this provides better result rankings.\n\nTo try Amazon Kendra Intelligent Ranking, you must first set up the Amazon Kendra service. To get started, see Amazon Kendra. For detailed information, including plugin setup instructions, see Amazon Kendra Intelligent Ranking for self-managed OpenSearch.""}, {""relative_path"": ""_search-plugins/search-relevance/reranking-search-results.md"", ""chunk"": ""Reranking search results\nIntroduced 2.12\n{: .label .label-purple }\n\nYou can rerank search results using a cross-encoder reranker in order to improve search relevance. To implement reranking, you need to configure a search pipeline that runs at search time. The search pipeline intercepts search results and applies the `rerank` processor to them. The `rerank` processor evaluates the search results and sorts them based on the new scores provided by the cross-encoder model. \n\n**PREREQUISITE**\nBefore configuring a reranking pipeline, you must set up a cross-encoder model. For information about using an OpenSearch-provided model, see Cross-encoder models. For information about using a custom model, see Custom local models.\n{: .note}""}]",0.0,0.0,0.0,0.3333333333333333,1.0,0.6309297535714575,0.2,1.0,0.6309297535714575,0.1,1.0,0.6309297535714575
How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?,"[""_search-plugins/caching/tiered-cache.md"",""_search-plugins/caching/request-cache.md""]","[""_search-plugins/caching/tiered-cache.md""]","[{""relative_path"": ""_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Setting on-heap and disk store tiers\n\nSet the on-heap and disk store tiers to `opensearch_onheap` and `ehcache_disk`, as shown in the following example:\n\n```yaml\nindices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap\nindices.request.cache.tiered_spillover.disk.store.name: ehcache_disk\n```\nThe `opensearch_onheap` setting uses the built-in on-heap cache available in OpenSearch. \n\nThe `ehcache_disk` setting is the disk cache implementation from Ehcache and requires installing the `cache-ehcache` plugin.\n\n{% include copy.html %}""}, {""relative_path"": ""_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Types of tiered caches\n\nOpenSearch provides an implementation of a `_tiered` spillover `cache_`. This implementation spills any items removed from the upper tiers to the lower tiers of cache. The upper tier, such as the on-heap tier, is smaller in size but offers better latency. The lower tier, such as the disk cache, is larger in size but slower in terms of latency. OpenSearch offers both on-heap and disk tiers.""}, {""relative_path"": ""_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Tiered cache\n\nThis is an experimental feature and is not recommended for use in a production environment. For updates on the progress of the feature or if you want to leave feedback, see the associated GitHub issue.    \n{: .warning}\n\nA tiered cache is a multi-level cache in which each tier has its own characteristics and performance levels. By combining different tiers, you can achieve a balance between cache performance and size.""}, {""relative_path"": ""_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Tiered cache settings\n\nIn OpenSearch 2.14, a request cache can be used in a tiered cache. To begin, configure the following settings in the `opensearch.yml` file.""}, {""relative_path"": ""_search-plugins/caching/tiered-cache.md"", ""chunk"": ""Enabling a tiered cache\n\nTo enable a tiered cache, configure the following setting in `opensearch.yml`:\n\n```yaml\nopensearch.experimental.feature.pluggable.caching.enabled: true\n```\n{% include copy.html %}\n\nFor more information about ways to enable experimental features, see Experimental feature flags.""}]",1.0,0.5,1.0,0.3333333333333333,0.5,0.6131471927654584,0.2,0.5,0.6131471927654584,0.1,0.5,0.6131471927654584
I need to combine relevance scores from different types of searches.,"[""_query-dsl/compound/hybrid.md""]","[""_query-dsl/compound/hybrid.md"", ""_search-plugins/search-pipelines/normalization-processor.md"", ""_query-dsl/specialized/script-score.md"", ""_search-plugins/sql/sql/functions.md""]","[{""relative_path"": ""_query-dsl/compound/hybrid.md"", ""chunk"": ""Hybrid query\n\nYou can use a hybrid query to combine relevance scores from multiple queries into one score for a given document. A hybrid query contains a list of one or more queries and independently calculates document scores at the shard level for each subquery. The subquery rewriting is performed at the coordinating node level in order to avoid duplicate computations.""}, {""relative_path"": ""_search-plugins/search-pipelines/normalization-processor.md"", ""chunk"": ""Score normalization and combination\n\nMany applications require both keyword matching and semantic understanding. For example, BM25 accurately provides relevant search results for a query containing keywords, and neural networks perform well when a query requires natural language understanding. Thus, you might want to combine BM25 search results with the results of a k-NN or neural search. However, BM25 and k-NN search use different scales to calculate relevance scores for the matching documents. Before combining the scores from multiple queries, it is beneficial to normalize them so that they are on the same scale, as shown by experimental data. For further reading about score normalization and combination, including benchmarks and various techniques, see this semantic search blog post.""}, {""relative_path"": ""_query-dsl/specialized/script-score.md"", ""chunk"": ""Example\n\nThe following example query searches for the text `neural search` in the `articles` index. It combines the original document relevance score with the `article_rank` value, which is first transformed with a saturation function:\n\n```json\nGET articles/_search\n{\n  \""query\"": {\n    \""script_score\"": {\n      \""query\"": {\n        \""match\"": { \""article_name\"": \""neural search\"" }\n      },\n      \""script\"" : {\n        \""source\"" : \""_score + saturation(doc['article_rank'].value, 11)\""\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}, {""relative_path"": ""_query-dsl/specialized/script-score.md"", ""chunk"": ""Example\n\nThe following example query searches for the text `neural search` in the `articles` index. It combines the original document relevance score with the `article_rank` value, which is first transformed with a sigmoid function:\n\n```json\nGET articles/_search\n{\n  \""query\"": {\n    \""script_score\"": {\n      \""query\"": {\n        \""match\"": { \""article_name\"": \""neural search\"" }\n      },\n      \""script\"" : {\n        \""source\"" : \""_score + sigmoid(doc['article_rank'].value, 11, 2)\""\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}, {""relative_path"": ""_search-plugins/sql/sql/functions.md"", ""chunk"": ""Score query\n\nTo return a relevance score along with every matching document, use the `SCORE`, `SCOREQUERY`, or `SCORE_QUERY` functions.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
What metrics does query metrics give me out of the box?,"[""_observing-your-data/query-insights/query-metrics.md""]","[""_observing-your-data/query-insights/query-metrics.md"", ""_observing-your-data/query-insights/index.md""]","[{""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Metrics\n\nQuery metrics provide the following measurements:\n\n- The number of queries per query type (for example, the number of `match` or `regex` queries)\n- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)\n- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)\n- Histograms of `latency` for each query type, aggregation type, and sort order\n- Histograms of `cpu` for each query type, aggregation type, and sort order\n- Histograms of `memory` for each query type, aggregation type, and sort order""}, {""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Query metrics\n\nKey query metrics, such as aggregation types, query types, latency, and resource usage per query type, are captured along the search path by using the OpenTelemetry (OTel) instrumentation framework. The telemetry data can be consumed using OTel metrics exporters.""}, {""relative_path"": ""_observing-your-data/query-insights/index.md"", ""chunk"": ""Query Insights settings\n\nYou can obtain the following information using Query Insights:\n\n- Top n queries\n- Query metrics""}, {""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""---\nlayout: default\ntitle: Query metrics\nparent: Query insights\nnav_order: 20\n---""}, {""relative_path"": ""_observing-your-data/query-insights/query-metrics.md"", ""chunk"": ""Configuring query metric generation\n\nTo configure query metric generation, use the following steps.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
Does opensearch work with open telemetry?,"[""_observing-your-data/prometheusmetrics.md""]","[""_benchmark/reference/commands/execute-test.md"", ""_dashboards/search-telemetry.md"", ""_observing-your-data/trace/ta-dashboards.md"", ""_observing-your-data/trace/distributed-tracing.md"", ""_monitoring-your-cluster/metrics/getting-started.md""]","[{""relative_path"": ""_benchmark/reference/commands/execute-test.md"", ""chunk"": ""Telemetry\n\nThe following options enable telemetry devices on OpenSearch Benchmark: \n \n- `--telemetry`: Enables the provided telemetry devices when the devices are provided using a comma-separated list. You can find a list of possible telemetry devices by using `opensearch-benchmark list telemetry`.\n- `--telemetry-params`: Defines a comma-separated list of key-value pairs that are injected verbatim into the telemetry devices as parameters.""}, {""relative_path"": ""_dashboards/search-telemetry.md"", ""chunk"": ""Sample opensearch_dashboards.yml with telemetry enabled\n\n This OpenSearch Dashboards YAML file excerpt shows the telemetry setting set to `true` to turn on search telemetry:\n\n ```json""}, {""relative_path"": ""_observing-your-data/trace/ta-dashboards.md"", ""chunk"": ""Setting up the OpenTelemetry Demo\n\nThe OpenTelemetry Demo with OpenSearch simulates a distributed application generating real-time telemetry data, providing you with a practical environment in which to explore features available with the Trace Analytics plugin before implementing it in your environment.\n\n\n**Step 1: Set up the OpenTelemetry Demo**\n  \n  - Clone the OpenTelemetry Demo with OpenSearch repository: `git clone https://github.com/opensearch-project/opentelemetry-demo`.\n  - Follow the Getting Started instructions to deploy the demo application using Docker, which runs multiple microservices generating telemetry data.\n\n**Step 2: Ingest telemetry data**\n\n  - Configure the OTel collectors to send telemetry data (traces, metrics, logs) to your OpenSearch cluster, using the preexisting setup.\n  - Confirm that Data Prepper is set up to process the incoming data, handle trace analytics and service map pipelines, submit data to required indexes, and perform preaggregated calculations.\n\n**Step 3: Explore Trace Analytics in OpenSearch Dashboards**\n\nThe **Trace Analytics** application includes two options: **Services** and **Traces**:\n\n  - **Services** lists all services in the application, plus an interactive map that shows how the various services connect to each other. In contrast to the dashboard (which helps identify problems by operation), the **Service map** helps you identify problems by service based on error rates and latency. To access this option, go to **Trace Analytics** > **Services**.\n  - **Traces** groups traces together by HTTP method and path so that you can see the average latency, error rate, and trends associated with a particular operation. For a more focused view, try filtering by trace group name. To access this option, go to **Trace Analytics** > **Traces**. From the **Trace Groups** panel, you can review the traces that comprise a trace group. From the **Traces** panel you can analyze individual traces for a detailed summary.\n\n **Step 4: Perform correlation analysis**\n  - Select **Services correlation** to display connections between various telemetry signals. This allows you to navigate from the logical service level to the associated metrics and logs for that specific service.\n\n---""}, {""relative_path"": ""_observing-your-data/trace/distributed-tracing.md"", ""chunk"": ""Distributed tracing pipeline\n\nOpenSearch provides a distributed tracing pipeline that can be used to ingest, process, and visualize tracing data with query and alerting functionality. OpenTelemetry is an open-source observability framework that provides a set of APIs, libraries, agents, and collectors for generating, capturing, and exporting telemetry data. The distributed tracing pipeline consists of the following components: \n\n- **Instrumentation:** Instrumenting your application code with OpenTelemetry SDKs.\n- **Propagation:** Injecting trace context into requests as they propagate through your system.\n- **Collection:** Collecting trace data from your application.\n- **Processing:** Aggregating trace data from multiple sources and enriching it with additional metadata.\n- **Exporting:** Sending trace data to a backend for storage and analysis. \n\nOpenSearch is often chosen as the sink for storing trace data.""}, {""relative_path"": ""_monitoring-your-cluster/metrics/getting-started.md"", ""chunk"": ""Option 1: Enable the experimental feature flag in the `opensearch.yml` file\n\n1. Navigate to your OpenSearch directory using the following command:\n\n  ```bash\n  cd \\path\\to\\opensearch\n  ```\n\n2. Open your `opensearch.yaml` file.\n3. Add the following setting to `opensearch.yaml`:\n\n  ```bash\n  opensearch.experimental.feature.telemetry.enabled=true\n  ```\n  {% include copy.html %}\n\n4. Save your changes and close the file.""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?,"[""_observing-your-data/notifications/api.md""]","[""_observing-your-data/notifications/api.md"", ""_security/audit-logs/storage-types.md""]","[{""relative_path"": ""_observing-your-data/notifications/api.md"", ""chunk"": ""Example request\n\n```json\nPUT _plugins/_notifications/configs/\n{\n  \""config\"": {\n    \""name\"": \""Slack Channel\"",\n    \""description\"": \""This is an updated channel configuration\"",\n    \""config_type\"": \""slack\"",\n    \""is_enabled\"": true,\n    \""slack\"": {\n      \""url\"": \""https://hooks.slack.com/sample-url\""\n    }\n  }\n}\n```""}, {""relative_path"": ""_observing-your-data/notifications/api.md"", ""chunk"": ""Example request\n\n```json\nPOST /_plugins/_notifications/configs/\n{\n  \""config_id\"": \""sample-id\"",\n  \""name\"": \""sample-name\"",\n  \""config\"": {\n    \""name\"": \""Sample Slack Channel\"",\n    \""description\"": \""This is a Slack channel\"",\n    \""config_type\"": \""slack\"",\n    \""is_enabled\"": true,\n    \""slack\"": {\n      \""url\"": \""https://sample-slack-webhook\""\n    }\n  }\n}\n```\n\nThe create channel API operation accepts the following fields in its request body:\n\nField |\tData type |\tDescription |\tRequired\n:--- | :--- | :--- | :---\nconfig_id | String | The configuration's custom ID. | No\nconfig | Object |\tContains all relevant information, such as channel name, configuration type, and plugin source. |\tYes\nname | String |\tName of the channel. | Yes\ndescription |\tString | The channel's description. | No\nconfig_type |\tString | The destination of your notification. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`. | Yes\nis_enabled | Boolean | Indicates whether the channel is enabled for sending and receiving notifications. Default is `true`.\t| No\n\nThe create channel operation accepts multiple `config_types` as possible notification destinations, so follow the format for your preferred `config_type`.\n\n```json\n\""sns\"": {\n  \""topic_arn\"": \""\"",\n  \""role_arn\"": \""\"" //optional\n}\n\""slack\"": {\n  \""url\"": \""https://sample-chime-webhoook\""\n}\n\""chime\"": {\n  \""url\"": \""https://sample-amazon-chime-webhoook\""\n}\n\""webhook\"": {\n      \""url\"": \""https://custom-webhook-test-url.com:8888/test-path?params1=value1&params2=value2\""\n}\n\""smtp_account\"": {\n  \""host\"": \""test-host.com\"",\n  \""port\"": 123,\n  \""method\"": \""start_tls\"",\n  \""from_address\"": \""test@email.com\""\n}\n\""ses_account\"": {\n  \""region\"": \""us-east-1\"",\n  \""role_arn\"": \""arn:aws:iam::012345678912:role/NotificationsSESRole\"",\n  \""from_address\"": \""test@email.com\""\n}\n\""email_group\"": { //Email recipient group\n  \""recipient_list\"": [\n    {\n      \""recipient\"": \""test-email1@test.com\""\n    },\n    {\n      \""recipient\"": \""test-email2@test.com\""\n    }\n  ]\n}\n\""email\"": { //The channel that sends emails\n  \""email_account_id\"": \""\"",\n  \""recipient_list\"": [\n    {\n      \""recipient\"": \""custom.email@test.com\""\n    }\n  ],\n  \""email_group_id_list\"": []\n}\n```\n\nThe following example demonstrates how to create a channel using email as a `config_type`:\n\n```json\nPOST /_plugins/_notifications/configs/\n{\n  \""id\"": \""sample-email-id\"",\n  \""name\"": \""sample-name\"",\n  \""config\"": {\n    \""name\"": \""Sample Email Channel\"",\n    \""description\"": \""Sample email description\"",\n    \""config_type\"": \""email\"",\n    \""is_enabled\"": true,\n    \""email\"": {\n      \""email_account_id\"": \""\"",\n      \""recipient_list\"": [\n        \""sample@email.com\""\n      ]\n    }\n  }\n}\n```""}, {""relative_path"": ""_observing-your-data/notifications/api.md"", ""chunk"": ""Update channel configuration\n\nTo update a channel configuration, send a POST request to the `configs` resource and specify the channel's `config_id` as a path parameter. Specify the new configuration details in the request body.""}, {""relative_path"": ""_observing-your-data/notifications/api.md"", ""chunk"": ""Create channel configuration\n\nTo create a notification channel configuration, send a POST request to the `configs` resource.""}, {""relative_path"": ""_security/audit-logs/storage-types.md"", ""chunk"": ""Webhook\n\nUse the following keys to configure the `webhook` storage type.\n\nName | Data type | Description\n:--- | :--- | :---\n`plugins.security.audit.config.webhook.url` | String | The HTTP or HTTPS URL to send the logs to.\n`plugins.security.audit.config.webhook.ssl.verify` | Boolean | If true, the TLS certificate provided by the endpoint (if any) will be verified. If set to false, no verification is performed. You can disable this check if you use self-signed certificates.\n`plugins.security.audit.config.webhook.ssl.pemtrustedcas_filepath` | String | The path to the trusted certificate against which the webhook's TLS certificate is validated.\n`plugins.security.audit.config.webhook.ssl.pemtrustedcas_content` | String | Same as `plugins.security.audit.config.webhook.ssl.pemtrustedcas_content`, but you can configure the base 64 encoded certificate content directly.\n`plugins.security.audit.config.webhook.format` | String | The format in which the audit log message is logged, can be one of `URL_PARAMETER_GET`, `URL_PARAMETER_POST`, `TEXT`, `JSON`, `SLACK`. See Formats.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
"What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?","[""_install-and-configure/plugins.md""]","[""_install-and-configure/plugins.md"", ""_security/access-control/permissions.md"", ""_observing-your-data/ad/security.md"", ""_install-and-configure/configuring-opensearch/plugin-settings.md""]","[{""relative_path"": ""_install-and-configure/plugins.md"", ""chunk"": ""Example\n\n\n  \n    Select to expand the example \n  \n  {: .text-delta}\n\n```console\n$ sudo ./opensearch-plugin install org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0\n-> Installing org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0\n-> Downloading org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 from maven central\n[=================================================] 100%   \n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@     WARNING: plugin requires additional permissions     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n* java.lang.RuntimePermission accessClassInPackage.sun.misc\n* java.lang.RuntimePermission accessDeclaredMembers\n* java.lang.RuntimePermission getClassLoader\n* java.lang.RuntimePermission setContextClassLoader\n* java.lang.reflect.ReflectPermission suppressAccessChecks\n* java.net.SocketPermission * connect,resolve\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\n* javax.management.MBeanServerPermission createMBeanServer\n* javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y/N]y\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\n```\n\n\nRestart your OpenSearch node after installing a plugin.\n{: .note}""}, {""relative_path"": ""_security/access-control/permissions.md"", ""chunk"": ""Anomaly detection permissions\n\nSee Anomaly Detection API.\n\n- cluster:admin/opendistro/ad/detector/delete\n- cluster:admin/opendistro/ad/detector/info\n- cluster:admin/opendistro/ad/detector/jobmanagement\n- cluster:admin/opendistro/ad/detector/preview\n- cluster:admin/opendistro/ad/detector/run\n- cluster:admin/opendistro/ad/detector/search\n- cluster:admin/opendistro/ad/detector/stats\n- cluster:admin/opendistro/ad/detector/write\n- cluster:admin/opendistro/ad/detector/validate\n- cluster:admin/opendistro/ad/detectors/get\n- cluster:admin/opendistro/ad/result/search\n- cluster:admin/opendistro/ad/result/topAnomalies\n- cluster:admin/opendistro/ad/tasks/search""}, {""relative_path"": ""_observing-your-data/ad/security.md"", ""chunk"": ""Basic permissions\n\nAs an admin user, you can use the Security plugin to assign specific permissions to users based on which APIs they need access to. For a list of supported APIs, see Anomaly detection API.\n\nThe Security plugin has two built-in roles that cover most anomaly detection use cases: `anomaly_full_access` and `anomaly_read_access`. For descriptions of each, see Predefined roles.\n\nIf these roles don't meet your needs, mix and match individual anomaly detection permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the `cluster:admin/opensearch/ad/detector/delete` permission lets you delete detectors.""}, {""relative_path"": ""_install-and-configure/plugins.md"", ""chunk"": ""Example\n\n  \n    Select to expand the example \n  \n  {: .text-delta}\n  \n```bash\n# Zip file is hosted on a remote server - in this case, Maven central repository.\n$ sudo ./opensearch-plugin install https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip\n-> Installing https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip\n-> Downloading https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip\n[=================================================] 100%   \n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@     WARNING: plugin requires additional permissions     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n* java.lang.RuntimePermission accessClassInPackage.sun.misc\n* java.lang.RuntimePermission accessDeclaredMembers\n* java.lang.RuntimePermission getClassLoader\n* java.lang.RuntimePermission setContextClassLoader\n* java.lang.reflect.ReflectPermission suppressAccessChecks\n* java.net.SocketPermission * connect,resolve\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\n* javax.management.MBeanServerPermission createMBeanServer\n* javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y/N]y\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\n\n# Zip file in a local directory.\n$ sudo ./opensearch-plugin install file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip\n-> Installing file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip\n-> Downloading file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip\n[=================================================] 100%   \n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@     WARNING: plugin requires additional permissions     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n* java.lang.RuntimePermission accessClassInPackage.sun.misc\n* java.lang.RuntimePermission accessDeclaredMembers\n* java.lang.RuntimePermission getClassLoader\n* java.lang.RuntimePermission setContextClassLoader\n* java.lang.reflect.ReflectPermission suppressAccessChecks\n* java.net.SocketPermission * connect,resolve\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\n* javax.management.MBeanServerPermission createMBeanServer\n* javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y/N]y\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\n```""}, {""relative_path"": ""_install-and-configure/configuring-opensearch/plugin-settings.md"", ""chunk"": ""Anomaly Detection plugin settings\n\nFor information about anomaly detection settings, see Anomaly Detection settings.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
What environment variable and YAML structure must be used to customize the admin password in a helm chart?,"[""_install-and-configure/install-opensearch/helm.md""]","[""_security/configuration/demo-configuration.md"", ""_security/configuration/yaml.md"", ""_install-and-configure/install-opensearch/docker.md"", ""_security/configuration/best-practices.md""]","[{""relative_path"": ""_security/configuration/demo-configuration.md"", ""chunk"": ""Helm\n\nFor Helm charts, the demo configuration is automatically installed during the OpenSearch installation. For OpenSearch 2.12 or later, customize the admin password in `values.yaml` under `extraEnvs`:\n\n```yaml\nextraEnvs:\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\n    value: \n```""}, {""relative_path"": ""_security/configuration/demo-configuration.md"", ""chunk"": ""Setting up a custom admin password\n**Note**: For OpenSearch versions 2.12 and later, you must set the initial admin password before installation. To customize the admin password, you can take the following steps:\n\n1. Download the following sample docker-compose.yml file.\n2. Create a `.env` file.\n3. Add the variable `OPENSEARCH_INITIAL_ADMIN_PASSWORD` and set the variable with a strong password. The password must pass the following complexity requirements:\n\n   - Minimum 8 characters\n   - Must contain at least one uppercase letter [A--Z]\n   - One lowercase letter [a--z]\n   - One digit [0--9]\n   - One special character\n\n4. Make sure that Docker is running on your local machine\n5. Run `docker-compose up` from the file directory where your `docker-compose.yml` file and `.env` file are located.""}, {""relative_path"": ""_security/configuration/yaml.md"", ""chunk"": ""Modifying the YAML files\n\nThe Security installation provides a number of YAML configuration files that are used to store the necessary settings that define the way the Security plugin manages users, roles, and activity within the cluster. These settings range from configurations for authentication backends to lists of allowed endpoints and HTTP requests. \n\nBefore running `securityadmin.sh` to load the settings into the `.opendistro_security` index, perform an initial configuration of the YAML files. The files can be found in the `config/opensearch-security` directory. It's also good practice to back up these files so that you can reuse them for other clusters.\n\nThe approach we recommend for using the YAML files is to first configure reserved and hidden resources, such as the `admin` and `kibanaserver` users. Thereafter you can create other users, roles, mappings, action groups, and tenants using OpenSearch Dashboards or the REST API.""}, {""relative_path"": ""_install-and-configure/install-opensearch/docker.md"", ""chunk"": ""Setting a custom admin password\n\nStarting with OpenSearch 2.12, a custom admin password is required to set up a demo security configuration. Do one of the following:\n\n- Before running `docker-compose.yml`, set a new custom admin password using the following command:\n  ```\n  export OPENSEARCH_INITIAL_ADMIN_PASSWORD=\n  ```\n  {% include copy.html %}\n  \n- Create an `.env` file in the same folder as your `docker-compose.yml` file with the `OPENSEARCH_INITIAL_ADMIN_PASSWORD` and a strong password value.""}, {""relative_path"": ""_security/configuration/best-practices.md"", ""chunk"": ""9. Replace all default passwords\n\nWhen initializing OpenSearch with the demo configuration, many default passwords are provided for internal users in `internal_users.yml`, such as `admin`, `kibanaserver`, and `logstash`.\n\nYou should change the passwords for these users to strong, complex passwords either at startup or as soon as possible once the cluster is running. Creating password configurations is a straightforward procedure, especially when using the scripts bundled with OpenSearch, like `hash.sh` or `hash.bat`, located in the `plugin/OpenSearch security/tools` directory.\n\nThe `kibanaserver` user is a crucial component that allows OpenSearch Dashboards to communicate with the OpenSearch cluster. By default, this user is preconfigured with a default password in the demo configuration. This should be replaced with a strong, unique password in the OpenSearch configuration, and the `opensearch_dashboards.yml` file should be updated to reflect this change.""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?,"[""_aggregations/bucket/reverse-nested.md""]","[""_aggregations/index.md"", ""_aggregations/pipeline-agg.md"", ""_aggregations/bucket/nested.md"", ""_aggregations/bucket/reverse-nested.md""]","[{""relative_path"": ""_aggregations/index.md"", ""chunk"": ""Nested aggregations\n\nAggregations within aggregations are called nested or subaggregations.\n\nMetric aggregations produce simple results and can't contain nested aggregations.\n\nBucket aggregations produce buckets of documents that you can nest in other aggregations. You can perform complex analysis on your data by nesting metric and bucket aggregations within bucket aggregations.""}, {""relative_path"": ""_aggregations/pipeline-agg.md"", ""chunk"": ""Parent aggregations\n\nParent aggregations take the output of an outer aggregation and produce new buckets or new aggregations at the same level as the existing buckets.\n\nParent aggregations must have `min_doc_count` set to 0 (default for `histogram` aggregations) and the specified metric must be a numeric value. If `min_doc_count` is greater than `0`, some buckets are omitted, which might lead to incorrect results.\n\n`derivatives` and `cumulative_sum` are common parent aggregations.""}, {""relative_path"": ""_aggregations/bucket/nested.md"", ""chunk"": ""Nested aggregations\n\nThe `nested` aggregation lets you aggregate on fields inside a nested object. The `nested` type is a specialized version of the object data type that allows arrays of objects to be indexed in a way that they can be queried independently of each other\n\nWith the `object` type, all the data is stored in the same document, so matches for a search can go across sub documents. For example, imagine a `logs` index with `pages` mapped as an `object` datatype:\n\n```json\nPUT logs/_doc/0\n{\n  \""response\"": \""200\"",\n  \""pages\"": [\n    {\n      \""page\"": \""landing\"",\n      \""load_time\"": 200\n    },\n    {\n      \""page\"": \""blog\"",\n      \""load_time\"": 500\n    }\n  ]\n}\n```\n{% include copy-curl.html %}\n\nOpenSearch merges all sub-properties of the entity relations that looks something like this:\n\n```json\n{\n  \""logs\"": {\n    \""pages\"": [\""landing\"", \""blog\""],\n    \""load_time\"": [\""200\"", \""500\""]\n  }\n}\n```\n\nSo, if you wanted to search this index with `pages=landing` and `load_time=500`, this document matches the criteria even though the `load_time` value for landing is 200.\n\nIf you want to make sure such cross-object matches don\u2019t happen, map the field as a `nested` type:\n\n```json\nPUT logs\n{\n  \""mappings\"": {\n    \""properties\"": {\n      \""pages\"": {\n        \""type\"": \""nested\"",\n        \""properties\"": {\n          \""page\"": { \""type\"": \""text\"" },\n          \""load_time\"": { \""type\"": \""double\"" }\n        }\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nNested documents allow you to index the same JSON document but will keep your pages in separate Lucene documents, making only searches like `pages=landing` and `load_time=200` return the expected result. Internally, nested objects index each object in the array as a separate hidden document, meaning that each nested object can be queried independently of the others.\n\nYou have to specify a nested path relative to parent that contains the nested documents:\n\n\n```json\nGET logs/_search\n{\n  \""query\"": {\n    \""match\"": { \""response\"": \""200\"" }\n  },\n  \""aggs\"": {\n    \""pages\"": {\n      \""nested\"": {\n        \""path\"": \""pages\""\n      },\n      \""aggs\"": {\n        \""min_load_time\"": { \""min\"": { \""field\"": \""pages.load_time\"" } }\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}, {""relative_path"": ""_aggregations/bucket/reverse-nested.md"", ""chunk"": ""Reverse nested aggregations\n\nYou can aggregate values from nested documents to their parent; this aggregation is called `reverse_nested`.\nYou can use `reverse_nested` to aggregate a field from the parent document after grouping by the field from the nested object. The `reverse_nested` aggregation \""joins back\"" the root page and gets the `load_time` for each for your variations.\n\nThe `reverse_nested` aggregation is a sub-aggregation inside a nested aggregation. It accepts a single option named `path`. This option defines how many steps backwards in the document hierarchy OpenSearch takes to calculate the aggregations.\n\n```json\nGET logs/_search\n{\n  \""query\"": {\n    \""match\"": { \""response\"": \""200\"" }\n  },\n  \""aggs\"": {\n    \""pages\"": {\n      \""nested\"": {\n        \""path\"": \""pages\""\n      },\n      \""aggs\"": {\n        \""top_pages_per_load_time\"": {\n          \""terms\"": {\n            \""field\"": \""pages.load_time\""\n          },\n          \""aggs\"": {\n            \""comment_to_logs\"": {\n              \""reverse_nested\"": {},\n              \""aggs\"": {\n                \""min_load_time\"": {\n                  \""min\"": {\n                    \""field\"": \""pages.load_time\""\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}, {""relative_path"": ""_aggregations/pipeline-agg.md"", ""chunk"": ""Sibling aggregations\n\nSibling aggregations take the output of a nested aggregation and produce new buckets or new aggregations at the same level as the nested buckets.\n\nSibling aggregations must be a multi-bucket aggregation (have multiple grouped values for a certain field) and the metric must be a numeric value.\n\n`min_bucket`, `max_bucket`, `sum_bucket`, and `avg_bucket` are common sibling aggregations.""}]",0.0,0.0,0.0,0.0,0.0,0.0,0.2,1.0,0.43067655807339306,0.1,1.0,0.43067655807339306
Make it simpler like what happens when I have missing values in an agg?,"[""_aggregations/bucket/missing.md""]","[""_aggregations/bucket/missing.md"", ""_aggregations/metric/weighted-avg.md"", ""_aggregations/metric/median-absolute-deviation.md"", ""_search-plugins/searching-data/sort.md""]","[{""relative_path"": ""_aggregations/bucket/missing.md"", ""chunk"": ""Missing aggregations\n\nIf you have documents in your index that don\u2019t contain the aggregating field at all or the aggregating field has a value of NULL, use the `missing` parameter to specify the name of the bucket such documents should be placed in.\n\nThe following example adds any missing values to a bucket named \""N/A\"":\n\n```json\nGET opensearch_dashboards_sample_data_logs/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""response_codes\"": {\n      \""terms\"": {\n        \""field\"": \""response.keyword\"",\n        \""size\"": 10,\n        \""missing\"": \""N/A\""\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nBecause the default value for the `min_doc_count` parameter is 1, the `missing` parameter doesn't return any buckets in its response. Set `min_doc_count` parameter to 0 to see the \""N/A\"" bucket in the response:\n\n```json\nGET opensearch_dashboards_sample_data_logs/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""response_codes\"": {\n      \""terms\"": {\n        \""field\"": \""response.keyword\"",\n        \""size\"": 10,\n        \""missing\"": \""N/A\"",\n        \""min_doc_count\"": 0\n      }\n    }\n  }\n}\n```""}, {""relative_path"": ""_aggregations/metric/weighted-avg.md"", ""chunk"": ""Handling missing values\n\nThe `missing` parameter allows you to specify default values for documents missing the `value` field or the `weight` field instead of excluding them from the calculation.\n\nThe following is an example of this behavior. First, create an index and add sample documents. This example includes five documents with different combinations of missing values for the `rating` and `num_reviews` fields: \n\n```json\nPUT /products\n{\n  \""mappings\"": {\n    \""properties\"": {\n      \""name\"": {\n        \""type\"": \""text\""\n      },\n      \""rating\"": {\n        \""type\"": \""double\""\n      },\n      \""num_reviews\"": {\n        \""type\"": \""integer\""\n      }\n    }\n  }\n}\n\nPOST /_bulk\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product A\"", \""rating\"": 4.5, \""num_reviews\"": 100 }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product B\"", \""rating\"": 3.8, \""num_reviews\"": 50 }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product C\"", \""rating\"": null, \""num_reviews\"": 20 }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product D\"", \""rating\"": 4.2, \""num_reviews\"": null }\n{ \""index\"": { \""_index\"": \""products\"" } }\n{ \""name\"": \""Product E\"", \""rating\"": null, \""num_reviews\"": null }\n```\n{% include copy-curl.html %}\n\nNext, run the following `weighted_avg` aggregation:\n\n```json\nGET /products/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""weighted_rating\"": {\n      \""weighted_avg\"": {\n        \""value\"": {\n          \""field\"": \""rating\""\n        },\n        \""weight\"": {\n          \""field\"": \""num_reviews\""\n        }\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nIn the response, you can see that the missing values for `Product E` were completely ignored in the calculation.""}, {""relative_path"": ""_aggregations/metric/median-absolute-deviation.md"", ""chunk"": ""Missing\n\nBy default, if a field is missing or has a null value in a document, it is ignored during computation. However, you can specify a value to be used for those missing or null fields by using the `missing` parameter, as shown in the following request:\n\n```json\nGET opensearch_dashboards_sample_data_flights/_search\n{\n  \""size\"": 0,\n  \""aggs\"": {\n    \""median_absolute_deviation_distanceMiles\"": {\n      \""median_absolute_deviation\"": {\n        \""field\"": \""DistanceMiles\"",\n        \""missing\"": 1000\n      }\n    }\n  }\n}\n```\n{% include copy-curl.html %}""}, {""relative_path"": ""_search-plugins/searching-data/sort.md"", ""chunk"": ""Handling missing values\n\nThe `missing` parameter specifies the handling of missing values. The built-in valid values are `_last` (list the documents with the missing value last) and `_first` (list the documents with the missing value first). The default value is `_last`. You can also specify a custom value to be used for missing documents as the sort value. \n\nFor example, you can index a document with an `average` field and another document without an `average` field:\n\n```json\nPUT students/_doc/1\n{\n   \""name\"": \""John Doe\"",\n   \""average\"": 80\n}\n\nPUT students/_doc/2\n{\n   \""name\"": \""Mary Major\""\n}\n```\n\nSort the documents, ordering the document with a missing field first:\n\n```json\nGET students/_search\n{\n  \""query\"": {\n    \""match_all\"": {}\n  },\n  \""sort\"": [\n    {\n      \""average\"": {\n        \""order\"": \""desc\"",\n        \""missing\"": \""_first\""\n      }\n    }\n  ]\n}\n```\n\nThe response lists document 2 first:\n\n```json\n{\n  \""took\"" : 1,\n  \""timed_out\"" : false,\n  \""_shards\"" : {\n    \""total\"" : 1,\n    \""successful\"" : 1,\n    \""skipped\"" : 0,\n    \""failed\"" : 0\n  },\n  \""hits\"" : {\n    \""total\"" : {\n      \""value\"" : 2,\n      \""relation\"" : \""eq\""\n    },\n    \""max_score\"" : null,\n    \""hits\"" : [\n      {\n        \""_index\"" : \""students\"",\n        \""_id\"" : \""2\"",\n        \""_score\"" : null,\n        \""_source\"" : {\n          \""name\"" : \""Mary Major\""\n        },\n        \""sort\"" : [\n          9223372036854775807\n        ]\n      },\n      {\n        \""_index\"" : \""students\"",\n        \""_id\"" : \""1\"",\n        \""_score\"" : null,\n        \""_source\"" : {\n          \""name\"" : \""John Doe\"",\n          \""average\"" : 80\n        },\n        \""sort\"" : [\n          80\n        ]\n      }\n    ]\n  }\n}\n```""}, {""relative_path"": ""_aggregations/bucket/missing.md"", ""chunk"": ""---\nlayout: default\ntitle: Missing\nparent: Bucket aggregations\nnav_order: 120\nredirect_from:\n  - /query-dsl/aggregations/bucket/missing/\n---""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
Can you explain compound queries to me?,"[""_query-dsl/compound/index.md""]","[""_query-dsl/compound/index.md"", ""_getting-started/search-data.md"", ""_search-plugins/search-pipelines/neural-sparse-query-two-phase-processor.md"", ""_search-plugins/sql/sql/complex.md""]","[{""relative_path"": ""_query-dsl/compound/index.md"", ""chunk"": ""Compound queries\n\nCompound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. \n\nThe following table lists all compound query types.\n\nQuery type | Description\n:--- | :---\n`bool` (Boolean)| Combines multiple query clauses with Boolean logic. \n`boosting` | Changes the relevance score of documents without removing them from the search results. Returns documents that match a `positive` query, but downgrades the relevance of documents in the results that match a `negative` query.\n`constant_score` | Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the `boost` value.\n`dis_max` (disjunction max) | Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value.\n`function_score` | Recalculates the relevance score of documents that are returned by a query using a function that you define.\n`hybrid` | Combines relevance scores from multiple queries into one score for a given document.""}, {""relative_path"": ""_getting-started/search-data.md"", ""chunk"": ""Compound queries\n\nA compound query lets you combine multiple query or filter clauses. A Boolean query is an example of a compound query.\n\nFor example, to search for students whose name matches `doe` and filter by graduation year and GPA, use the following request:\n\n```json\nGET students/_search\n{\n  \""query\"": {\n    \""bool\"": {\n      \""must\"": [\n        {\n          \""match\"": {\n            \""name\"": \""doe\""\n          }\n        },\n        { \""range\"": { \""gpa\"": { \""gte\"": 3.6, \""lte\"": 3.9 } } },\n        { \""term\"":  { \""grad_year\"": 2022 }}\n      ]\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nFor more information about Boolean and other compound queries, see Compound queries.""}, {""relative_path"": ""_search-plugins/search-pipelines/neural-sparse-query-two-phase-processor.md"", ""chunk"": ""Compound query support\n\nAs of OpenSearch 2.15, only the Boolean compound query is supported.\n\nNeural sparse queries and Boolean queries with a boost parameter (not boosting queries) are also supported.""}, {""relative_path"": ""_search-plugins/sql/sql/complex.md"", ""chunk"": ""Complex queries\n\nBesides simple SFW (`SELECT-FROM-WHERE`) queries, the SQL plugin supports complex queries such as subquery, join, union, and minus. These queries operate on more than one OpenSearch index. To examine how these queries execute behind the scenes, use the `explain` operation.""}, {""relative_path"": ""_query-dsl/compound/index.md"", ""chunk"": ""---\nlayout: default\ntitle: Compound queries\nhas_children: true\nhas_toc: false\nnav_order: 40\nredirect_from: \n  - /opensearch/query-dsl/compound/index/\n  - /query-dsl/compound/index/\n  - /query-dsl/query-dsl/compound/\n  - /query-dsl/compound/\n---""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
What is the difference between vector search and full text search?,"[""_query-dsl/term-vs-full-text.md"", ""search-plugins/vector-search.md""]","[""_query-dsl/term-vs-full-text.md"", ""_search-plugins/vector-search.md"", ""_getting-started/search-data.md""]","[{""relative_path"": ""_query-dsl/term-vs-full-text.md"", ""chunk"": ""Term-level and full-text queries compared\n\nYou can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries analyze the query string. The following table summarizes the differences between term-level and full-text queries.\n\n| | Term-level queries | Full-text queries\n:--- | :--- | :---\n*Description* | Term-level queries answer which documents match a query. | Full-text queries answer how well the documents match a query.\n*Analyzer* | The search term isn't analyzed. This means that the term query searches for your search term as it is.  | The search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document's field.\n*Relevance* | Term-level queries simply return documents that match without sorting them based on the relevance score. They still calculate the relevance score, but this score is the same for all the documents that are returned. | Full-text queries calculate a relevance score for each match and sort the results by decreasing order of relevance.\n*Use Case* | Use term-level queries when you want to match exact values such as numbers, dates, or tags and don't need the matches to be sorted by relevance. | Use full-text queries to match text fields and sort by relevance after taking into account factors like casing and stemming variants.\n\nOpenSearch uses the BM25 ranking algorithm to calculate relevance scores. To learn more, see Okapi BM25.\n{: .note }""}, {""relative_path"": ""_query-dsl/term-vs-full-text.md"", ""chunk"": ""Should I use a full-text or a term-level query?\n\nTo clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster.""}, {""relative_path"": ""_search-plugins/vector-search.md"", ""chunk"": ""Vector search with filtering\n\nFor information about vector search with filtering, see k-NN search with filters.""}, {""relative_path"": ""_search-plugins/vector-search.md"", ""chunk"": ""Vector search\n\nOpenSearch is a comprehensive search platform that supports a variety of data types, including vectors. OpenSearch vector database functionality is seamlessly integrated with its generic database function.\n\nIn OpenSearch, you can generate vector embeddings, store those embeddings in an index, and use them for vector search. Choose one of the following options:\n\n- Generate embeddings using a library of your choice before ingesting them into OpenSearch. Once you ingest vectors into an index, you can perform a vector similarity search on the vector space. For more information, see Working with embeddings generated outside of OpenSearch. \n- Automatically generate embeddings within OpenSearch. To use embeddings for semantic search, the ingested text (the corpus) and the query need to be embedded using the same model. Neural search packages this functionality, eliminating the need to manage the internal details. For more information, see Generating vector embeddings within OpenSearch.""}, {""relative_path"": ""_getting-started/search-data.md"", ""chunk"": ""Full-text search\n\nYou can run a full-text search on fields mapped as `text`. By default, text fields are analyzed by the `default` analyzer. The analyzer splits text into terms and changes it to lowercase. For more information about OpenSearch analyzers, see Analyzers.\n\nFor example, the following query searches for students with the name `john`:\n\n```json\nGET /students/_search\n{\n  \""query\"": {\n    \""match\"": {\n      \""name\"": \""john\""\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nThe response contains the matching document:\n\n```json\n{\n  \""took\"": 13,\n  \""timed_out\"": false,\n  \""_shards\"": {\n    \""total\"": 1,\n    \""successful\"": 1,\n    \""skipped\"": 0,\n    \""failed\"": 0\n  },\n  \""hits\"": {\n    \""total\"": {\n      \""value\"": 1,\n      \""relation\"": \""eq\""\n    },\n    \""max_score\"": 0.9808291,\n    \""hits\"": [\n      {\n        \""_index\"": \""students\"",\n        \""_id\"": \""1\"",\n        \""_score\"": 0.9808291,\n        \""_source\"": {\n          \""name\"": \""John Doe\"",\n          \""gpa\"": 3.89,\n          \""grad_year\"": 2022\n        }\n      }\n    ]\n  }\n}\n```\n\nNotice that the query text is lowercase while the text in the field is not, but the query still returns the matching document. \n\nYou can reorder the terms in the search string. For example, the following query searches for `doe john`:\n\n```json\nGET /students/_search\n{\n  \""query\"": {\n    \""match\"": {\n      \""name\"": \""doe john\""\n    }\n  }\n}\n```\n{% include copy-curl.html %}\n\nThe response contains two matching documents:\n\n```json\n{\n  \""took\"": 14,\n  \""timed_out\"": false,\n  \""_shards\"": {\n    \""total\"": 1,\n    \""successful\"": 1,\n    \""skipped\"": 0,\n    \""failed\"": 0\n  },\n  \""hits\"": {\n    \""total\"": {\n      \""value\"": 2,\n      \""relation\"": \""eq\""\n    },\n    \""max_score\"": 1.4508327,\n    \""hits\"": [\n      {\n        \""_index\"": \""students\"",\n        \""_id\"": \""1\"",\n        \""_score\"": 1.4508327,\n        \""_source\"": {\n          \""name\"": \""John Doe\"",\n          \""gpa\"": 3.89,\n          \""grad_year\"": 2022\n        }\n      },\n      {\n        \""_index\"": \""students\"",\n        \""_id\"": \""3\"",\n        \""_score\"": 0.4700036,\n        \""_source\"": {\n          \""name\"": \""Jane Doe\"",\n          \""gpa\"": 3.52,\n          \""grad_year\"": 2024\n        }\n      }\n    ]\n  }\n}\n```\n\nThe match query type uses `OR` as an operator by default, so the query is functionally `doe OR john`. Both `John Doe` and `Jane Doe` matched the word `doe`, but `John Doe` is scored higher because it also matched `john`.""}]",1.0,0.5,1.0,0.3333333333333333,0.5,0.6131471927654584,0.2,0.5,0.6131471927654584,0.1,0.5,0.6131471927654584
"Why can't my coworker see my async searches, but I can see and delete theirs? This role thing is messing everything up!","[""_search-plugins/async/security.md""]","[""_search-plugins/async/security.md"", ""_security/access-control/permissions.md""]","[{""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""Basic permissions\n\nAs an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see Asynchronous search.\n\nThe Security plugin has two built-in roles that cover most asynchronous search use cases: `asynchronous_search_full_access` and `asynchronous_search_read_access`. For descriptions of each, see Predefined roles.\n\nIf these roles don\u2019t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the `cluster:admin/opensearch/asynchronous_search/delete` permission lets you delete a previously submitted asynchronous search.""}, {""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""Asynchronous search security\n\nYou can use the Security plugin with asynchronous searches to limit non-admin users to specific actions. For example, you might want some users to only be able to submit or delete asynchronous searches, while you might want others to only view the results.\n\nAll asynchronous search indexes are protected as system indexes. Only a super admin user or an admin user with a Transport Layer Security (TLS) certificate can access system indexes. For more information, see System indexes.""}, {""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""(Advanced) Limit access by backend role\n\nUse backend roles to configure fine-grained access to asynchronous searches based on roles. For example, users of different departments in an organization can view asynchronous searches owned by their own department.\n\nFirst, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually.\n\nNow when users view asynchronous search resources in OpenSearch Dashboards (or make REST API calls), they only see asynchronous searches submitted by users who have a subset of the backend role.\nFor example, consider two users: `judy` and `elon`.\n\n`judy` has an IT backend role:\n\n```json\nPUT _plugins/_security/api/internalusers/judy\n{\n  \""password\"": \""judy\"",\n  \""backend_roles\"": [\n    \""IT\""\n  ],\n  \""attributes\"": {}\n}\n```\n\n`elon` has an admin backend role:\n\n```json\nPUT _plugins/_security/api/internalusers/elon\n{\n  \""password\"": \""elon\"",\n  \""backend_roles\"": [\n    \""admin\""\n  ],\n  \""attributes\"": {}\n}\n```\n\nBoth `judy` and `elon` have full access to asynchronous search:\n\n```json\nPUT _plugins/_security/api/rolesmapping/async_full_access\n{\n  \""backend_roles\"": [],\n  \""hosts\"": [],\n  \""users\"": [\n    \""judy\"",\n    \""elon\""\n  ]\n}\n```\n\nBecause they have different backend roles, an asynchronous search submitted by `judy` will not be visible to `elon` and vice versa.\n\n`judy` needs to have at least the superset of all roles that `elon` has to see `elon`'s asynchronous searches.\n\nFor example, if `judy` has five backend roles and `elon` has one of these roles, then `judy` can see asynchronous searches submitted by `elon`, but `elon` can\u2019t see the asynchronous searches submitted by `judy`. This means that `judy` can perform GET and DELETE operations on asynchronous searches submitted by `elon`, but not the reverse.\n\nIf none of the users have any backend roles, all three will be able to see the others' searches.\n\nFor example, consider three users: `judy`, `elon`, and `jack`.\n\n`judy`, `elon`, and `jack` have no backend roles set up:\n\n```json\nPUT _plugins/_security/api/internalusers/judy\n{\n  \""password\"": \""judy\"",\n  \""backend_roles\"": [],\n  \""attributes\"": {}\n}\n```\n\n```json\nPUT _plugins/_security/api/internalusers/elon\n{\n  \""password\"": \""elon\"",\n  \""backend_roles\"": [],\n  \""attributes\"": {}\n}\n```\n\n```json\nPUT _plugins/_security/api/internalusers/jack\n{\n  \""password\"": \""jack\"",\n  \""backend_roles\"": [],\n  \""attributes\"": {}\n}\n```\n\nBoth `judy` and `elon` have full access to asynchronous search:\n\n```json\nPUT _plugins/_security/api/rolesmapping/async_full_access\n{\n  \""backend_roles\"": [],\n  \""hosts\"": [],\n  \""users\"": [\""judy\"",\""elon\""]\n}\n```\n\n`jack` has read access to asynchronous search results:\n\n```json\nPUT _plugins/_security/api/rolesmapping/async_read_access\n{\n  \""backend_roles\"": [],\n  \""hosts\"": [],\n  \""users\"": [\""jack\""]\n}\n```\n\nBecause none of the users have backend roles, they will be able to see each other's asynchronous searches. So, if `judy` submits an asynchronous search, `elon`, who has full access, will be able to see that search. `jack`, who has read access, will also be able to see `judy`'s asynchronous search.""}, {""relative_path"": ""_security/access-control/permissions.md"", ""chunk"": ""Asynchronous Search permissions\n\nSee Asynchronous search.\n\n- cluster:admin/opendistro/asynchronous_search/stats\n- cluster:admin/opendistro/asynchronous_search/delete\n- cluster:admin/opendistro/asynchronous_search/get\n- cluster:admin/opendistro/asynchronous_search/submit""}, {""relative_path"": ""_search-plugins/async/security.md"", ""chunk"": ""A note on Asynchronous Search and fine-grained access control\n\nBy design, the Asynchronous Search plugin extracts data from a target index and stores the data in a separate index to make search results available to users with the proper permissions. Although a user with either the `asynchronous_search_read_access` or `cluster:admin/opensearch/asynchronous_search/get` permission cannot submit the asynchronous search request itself, that user can get and view the search results using the associated search ID. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the target index. But once the data is stored outside this index, users with these access permissions are able to use search IDs to get and view asynchronous search results, which may include data that is otherwise concealed by DLS and FLS access control in the target index.\n\nTo reduce the chances of unintended users viewing search results that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for details.""}]",1.0,1.0,1.0,0.3333333333333333,1.0,1.0,0.2,1.0,1.0,0.1,1.0,1.0
