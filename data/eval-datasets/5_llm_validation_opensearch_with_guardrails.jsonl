{"query_text":"I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working? ","groundtruth_answer":"<question_answer>\nThere is not enough information available to answer this question. The context does not mention why zstd compression is not working for OpenSearch version 2.1. The context provides information about compression codecs available in OpenSearch 2.9 and later versions, but does not cover version 2.1.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Index codecs\\nnav_order: 3\\nparent: Index settings\\n---\\n\\n# Index codecs\\n\\nIndex codecs determine how the index\\u2019s stored fields are compressed and stored on disk. The index codec is controlled by the static `index.codec` setting that specifies the compression algorithm. The setting impacts the index shard size and index operation performance.  \\n\\n## Supported codecs\\n\\nOpenSearch provides support for four codecs that can be used for compressing the stored fields. Each codec offers different tradeoffs between compression ratio (storage size) and indexing performance (speed): \\n\\n* `default` -- This codec employs the [LZ4 algorithm](https:\/\/en.wikipedia.org\/wiki\/LZ4_(compression_algorithm)) with a preset dictionary, which prioritizes performance over compression ratio. It offers faster indexing and search operations when compared with `best_compression` but may result in larger index\/shard sizes. If no codec is provided in the index settings, then LZ4 is used as the default algorithm for compression.\\n* `best_compression` -- This codec uses [zlib](https:\/\/en.wikipedia.org\/wiki\/Zlib) as an underlying algorithm for compression. It achieves high compression ratios that result in smaller index sizes. However, this may incur additional CPU usage during index operations and may subsequently result in high indexing and search latencies. \\n\\nAs of OpenSearch 2.9, two new codecs based on the [Zstandard compression algorithm](https:\/\/github.com\/facebook\/zstd) are available. This algorithm provides a good balance between compression ratio and speed.\\n\\nIt may be challenging to change the codec setting of an existing index (see [Changing an index codec](#changing-an-index-codec)), so it is important to test a representative workload in a non-production environment before using a new codec setting.\\n{: .important}\\n\\n* `zstd` (OpenSearch 2.9 and later) -- This codec provides significant compression comparable to the `best_compression` codec with reasonable CPU usage and improved indexing and search performance compared to the `default` codec.\\n* `zstd_no_dict` (OpenSearch 2.9 and later) -- This codec is similar to `zstd` but excludes the dictionary compression feature. It provides faster indexing and search operations compared to `zstd` at the expense of a slightly larger index size.\\n\\nAs of OpenSearch 2.10, the `zstd` and `zstd_no_dict` compression codecs cannot be used for [k-NN]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/index\/) or [Security Analytics]({{site.url}}{{site.baseurl}}\/security-analytics\/index\/) indexes.\\n{: .warning}\\n\\nFor the `zstd` and `zstd_no_dict` codecs, you can optionally specify a compression level in the `index.codec.compression_level` setting. This setting takes integers in the [1, 6] range. A higher compression level results in a higher compression ratio (smaller storage size) with a tradeoff in speed (slower compression and decompression speeds lead to greater indexing and search latencies). \\n\\nWhen an index segment is created, it uses the current index codec for compression. If you update the index codec, any segment created after the update will use the new compression algorithm. For specific operation considerations, see [Index codec considerations for index operations](#index-codec-considerations-for-index-operations).\\n{: .note}\\n\\nAs of OpenSearch 2.15, hardware-accelerated compression codecs for the `DEFLATE` and `LZ4` compression algorithms are available. These hardware-accelerated codecs are available on the latest 4th and 5th Gen Intel\\u00ae\\ufe0f Xeon\\u00ae\\ufe0f processors running Linux kernel 3.10 and later. For all other systems and platforms, the codecs use that platform's corresponding software implementations. \\n\\nThe new hardware-accelerated codecs can be used by setting one of the following `index.codec` values:\\n* `qat_lz4` (OpenSearch 2.15 and later): Hardware-accelerated `LZ4`\\n* `qat_deflate` (OpenSearch 2.15 and later): Hardware-accelerated `DEFLATE`\\n\\n`qat_deflate` offers a much better compression ratio than `qat_lz4`, with a modest drop in compression and decompression speed.\\n{: .note}\\n\\nThe `index.codec.compression_level` setting can be used to specify the compression level for both `qat_lz4` and `qat_deflate`. \\n\\nThe `index.codec.qatmode` setting controls the behavior of the hardware accelerator and uses one of the following values:\\n\\n* `auto`: If hardware acceleration fails, then the algorithm switches to software acceleration.\\n* `hardware`: Guarantees hardware-only compression. If hardware is not available, then an exception occurs until hardware exists.\\n\\nFor information about the `index.codec.qatmode` setting's effects on snapshots, see the [Snapshots](#snapshots) section.\\n\\nFor more information about hardware acceleration on Intel, see the [Intel (R) QAT accelerator overview](https:\/\/www.intel.com\/content\/www\/us\/en\/developer\/topic-technology\/open\/quick-assist-technology\/overview.html).\\n\\n## Choosing a codec \\n\\nThe choice of index codec impacts the amount of disk space required to store the index data. Codecs like `best_compression`, `zstd`, and `zstd_no_dict` can achieve higher compression ratios, resulting in smaller index sizes. Conversely, the `default` codec doesn\\u2019t prioritize compression ratio, resulting in larger index sizes but faster search operations than `best_compression`.\\n\\n## Index codec considerations for index operations\\n\\nThe following index codec considerations apply to various index operations.\\n\\n### Writes\\n\\nEvery index consists of shards, each of which is further divided into Lucene segments. During index writes, the new segments are created based on the codec specified in the index settings. If you update the codec for an index, the new segments will use the new codec algorithm. \\n\\n### Merges\\n\\nDuring segment merges, OpenSearch combines smaller index segments into larger segments in order to provide optimal resource utilization and improve performance. The index codec setting influences the speed and efficiency of the merge operations. The number of merges that happen on an index is a factor of the segment size, and a smaller segment size directly translates into smaller merge sizes. If you update the `index.codec` setting, the new merge operations will use the new codec when creating merged segments. The merged segments will have the compression characteristics of the new codec.\\n\\n### Splits and shrinks\\n\\nThe [Split API]({{site.url}}{{site.baseurl}}\/api-reference\/index-apis\/split\/) splits an original index into a new index where each original primary shard is divided into two or more primary shards. The [Shrink API]({{site.url}}{{site.baseurl}}\/api-reference\/index-apis\/shrink-index\/) shrinks an existing index to a new index with a smaller number of primary shards. As part of split or shrink operations, any newly created segments will use the latest codec settings.\\n\\n### Snapshots\\n\\nWhen creating a [snapshot]({{site.url}}{{site.baseurl}}\/tuning-your-cluster\/availability-and-recovery\/snapshots\/index\/), the index codec setting influences the size of the snapshot and the time required for its creation. If the codec of an index is updated, newly created snapshots will use the latest codec setting. The resulting snapshot size will reflect the compression characteristics of the latest codec setting. Existing segments included in the snapshot will retain their original compression characteristics. \\n\\nWhen you restore the indexes from a snapshot of a cluster to another cluster, it is important to verify that the target cluster supports the codecs of the segments in the source snapshot. For example, if the source snapshot contains segments of the `zstd` or `zstd_no_dict` codecs (introduced in OpenSearch 2.9), you won't be able to restore the snapshot to a cluster that runs on an older OpenSearch version because it doesn't support these codecs. \\n\\nFor hardware-accelerated compression codecs, available in OpenSearch 2.15 and later, the value of `index.codec.qatmode` affects how snapshots and restores are performed. If the value is `auto` (the default), then snapshots and restores work without issue. However, if the value is `hardware`, then it must be reset to `auto` in order for the restore process to succeed on systems lacking the hardware accelerator.\\n\\nYou can modify the value of `index.codec.qatmode` during the restore process by setting its value as follows: `\\\"index_settings\\\": {\\\"index.codec.qatmode\\\": \\\"auto\\\"}`.\\n{: .note}\\n\\n### Reindexing\\n\\nWhen you are performing a [reindex]({{site.url}}{{site.baseurl}}\/im-plugin\/reindex-data\/) operation from a source index, the new segments created in the target index will have the properties of the codec settings of the target index. \\n\\n### Index rollups and transforms\\n\\nWhen an index [rollup]({{site.url}}{{site.baseurl}}\/im-plugin\/index-rollups\/) or [transform]({{site.url}}{{site.baseurl}}\/im-plugin\/index-transforms\/) job is completed, the segments created in the target index will have the properties of the index codec specified during target index creation, irrespective of the source index codec. If the target index is created dynamically through a rollup job, the default codec is used for segments of the target index.\\n\\n## Changing an index codec\\n\\nIt is not possible to change the codec setting of an open index. You can close the index, apply the new index codec setting, and reopen the index, at which point only new segments will be written with the new codec. This requires stopping all reads and writes to the index for a brief period to make the codec change and may result in inconsistent segment sizes and compression ratios. Alternatively, you can reindex all data from a source index into a new index with a different codec setting, though this is a very resource-intensive operation.\\n\\n## Performance tuning and benchmarking\\n\\nDepending on your specific use case, you might need to experiment with different index codec settings to fine-tune the performance of your OpenSearch cluster. Conducting benchmark tests with different codecs and measuring the impact on indexing speed, search performance, and resource utilization can help you identify the optimal index codec setting for your workload. With the `zstd` and `zstd_no_dict` codecs, you can also fine-tune the compression level in order to identify the optimal configuration for your cluster.\\n\\n### Benchmarking\\n\\nThe following table provides a performance comparison of the `best_compression`, `zstd`, and `zstd_no_dict` codecs against the `default` codec. The tests were performed with the [`nyc_taxi`](https:\/\/github.com\/topics\/nyc-taxi-dataset) dataset. The results are listed in terms of percent change, and bold results indicate performance improvement.\\n\\n| | `best_compression` | `zstd` | `zstd_no_dict` |\\n|:---\\t|:---\\t|:---\\t|:--- |\\n|**Write** | | | \\n|Median Latency\\t|0%\\t|0%\\t|&minus;1%\\t|\\n|p90 Latency\\t|3%\\t|2%\\t|**&minus;5%**\\t|\\n|Throughput\\t|&minus;2%\\t|**7%**\\t|**14%**\\t|\\n|**Read**\\t| | | \\n|Median Latency\\t|0%\\t|1%\\t|0%\\t|\\n|p90 Latency\\t|1%\\t|1%\\t|**&minus;2%**\\t|\\n|**Disk**\\t| | | \\n| Compression ratio\\t|**&minus;34%**\\t|**&minus;35%**\\t|**&minus;30%**\\t|\\n\\n\\n\\n---\\nlayout: default\\ntitle: Tuning for indexing speed\\nnav_order: 41\\nhas_children: false\\n---\\n\\n# Tuning your cluster for indexing speed\\n\\nThe following configurations demonstrated an improvement in throughput of around 60% when\\nrunning an indexing-only workload as compared to the out-of-the-box experience. The workload did not\\nincorporate search or other scenarios. Only the OpenSearch server process was run on the machines,\\nwith the benchmark clients hosted on a different node.\\n\\nThe execution environment was comprised of Intel EC2 instances (r7iz.2xlarge) in the AWS Cloud, and the\\nworkload used was the StackOverflow dataset available as part of OpenSearch Benchmark.\\n\\n## Java heap size\\n\\nA larger Java heap size is useful for indexing. Setting the Java min and max heap sizes to 50% of the RAM\\nsize shows better indexing performance on EC2 instances.\\n\\n## Flush translog threshold\\n\\nThe default value for `flush_threshold_size` is 512 MB. This means that the translog is flushed when it reaches 512 MB. The weight of the indexing load determines the frequency of the translog. When you increase `index.translog.flush_threshold_size`, the node performs the translog operation less frequently. Because flushes are resource-intensive operations, reducing the frequency of translogs improves indexing performance. By increasing the flush threshold size, the OpenSearch cluster also creates fewer large segments instead of multiple small segments. Large segments merge less often, and more threads are used for indexing instead of merging.\\n\\nFor pure indexing workloads, consider increasing the `flush_threshold_size` to 25% of the Java heap size, for example, to improve indexing performance.\\n\\nAn increased `index.translog.flush_threshold_size` can also increase the time that it takes for a translog to complete. If a shard fails, then recovery takes more time because the translog is larger.\\n{: .note}\\n\\nBefore increasing `index.translog.flush_threshold_size`, call the following API operation to get current flush operation statistics:\\n\\n```json\\ncurl -XPOST \\\"os-endpoint\/index-name\/_stats\/flush?pretty\\\"\\n```\\n{% include copy.html %}\\n\\n\\nReplace the `os-endpoint` and `index-name` with your endpoint and index name.\\n\\nIn the output, note the number of flushes and the total time. The following example output shows that there are 124 flushes, which took 17,690 milliseconds:\\n\\n```json\\n{\\n     \\\"flush\\\": {\\n          \\\"total\\\": 124,\\n          \\\"total_time_in_millis\\\": 17690\\n     }\\n}\\n```\\n\\nTo increase the flush threshold size, call the following API operation:\\n\\n```json\\ncurl -XPUT \\\"os-endpoint\/index-name\/_settings?pretty\\\" -d \\\"{\\\"index\\\":{\\\"translog.flush_threshold_size\\\" : \\\"1024MB\\\"}}\\\"\\n```\\n{% include copy.html %}\\n\\nIn this example, the flush threshold size is set to 1024 MB, which is ideal for instances that have more than 32 GB of memory.\\n\\nChoose the appropriate threshold size for your cluster.\\n{: .note}\\n\\nRun the stats API operation again to see whether the flush activity changed:\\n\\n```json\\ncurl -XGET \\\"os-endpoint\/index-name\/_stats\/flush?pretty\\\"\\n```\\n{% include copy.html %}\\n\\nIt's a best practice to increase the `index.translog.flush_threshold_size` only for the current index. After you confirm the outcome, apply the changes to the index template.\\n{: .note}\\n\\n## Index refresh interval\\n\\nBy default, OpenSearch refreshes indexes every second. OpenSearch only refreshes indexes that have\\nreceived at least one search request in the last 30 seconds.\\n\\nWhen you increase the refresh interval, the data node makes fewer API calls. To prevent [429 errors](https:\/\/repost.aws\/knowledge-center\/opensearch-resolve-429-error), it's a best practice to increase the refresh interval.\\n\\nIf your application can tolerate increasing the amount of time between when a document is indexed and when it\\nbecomes visible, you can increase the `index.refresh_interval` to a larger value, for example, `30s`, or even disable it in a\\npure indexing scenario in order to improve indexing speed.\\n\\n## Index buffer size\\n\\nIf the node is performing heavy indexing, ensure that the index buffer size is large enough. You can set the index buffer size to be either a percentage of the\\nJava heap size or the number of bytes. In most cases, the default value of 10% of JVM memory is sufficient. You can try\\nincreasing it to up to 25% for further improvement.\\n\\n## Concurrent merges\\n\\nThe maximum number of concurrent merges is specified as `max_merge_count`. The concurrentMergeScheduler controls the execution of\\nmerge operations when they are needed. Merges run in separate threads, and when the maximum number of\\nthreads is reached, further merges will wait until a merge thread becomes available.\\nIn cases where index throttling is an issue, consider increasing the number of merge threads beyond the\\ndefault value.\\n\\n## Shard distribution\\n\\nTo ensure that the shards are distributed evenly across the data nodes of the index into which you're ingesting, use the following formula to confirm that the shards are evenly distributed:\\n\\nNumber of shards for index = k * (Number of data nodes), where k is the number of shards per node\\n\\nFor example, if there are 24 shards in the index, and there are 8 data nodes, then OpenSearch assigns 3 shards to each node. \\n\\n## Setting replica count to zero\\n\\nIf you anticipate heavy indexing, consider setting the `index.number_of_replicas` value to `0`. Each replica duplicates the indexing process. As a result, disabling the replicas improves your cluster performance. After the heavy indexing is complete, reactivate the replicated indexes.\\n\\nIf a node fails while replicas are disabled, you might lose data. Disable the replicas only if you can tolerate data loss for a short duration.\\n{: .important }\\n\\n## Experiment to find the optimal bulk request size\\n\\nStart with a bulk request size of 5 MiB to 15 MiB. Then slowly increase the request size until the indexing performance stops improving. \\n\\n## Use an instance type that has SSD instance store volumes (such as I3)\\n\\nI3 instances provide fast and local memory express (NVMe) storage. I3 instances deliver better ingestion performance than instances that use General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volumes. For more information, see [Petabyte scale for Amazon OpenSearch Service](https:\/\/docs.aws.amazon.com\/opensearch-service\/latest\/developerguide\/petabyte-scale.html).\\n\\n## Reduce response size\\n\\nTo reduce the size of the OpenSearch response, use the `filter_path` parameter to exclude unnecessary fields. Be sure that you don't filter out any fields that are required for identifying or retrying failed requests. These fields can vary by client.\\n\\nIn the following example, the `index-name`, `type-name`, and `took` fields are excluded from the response:\\n\\n```json\\ncurl -XPOST \\\"es-endpoint\/index-name\/type-name\/_bulk?pretty&filter_path=-took,-items.index._index,-items.index._type\\\" -H 'Content-Type: application\/json' -d'\\n{ \\\"index\\\" : { \\\"_index\\\" : \\\"test2\\\", \\\"_id\\\" : \\\"1\\\" } }\\n{ \\\"user\\\" : \\\"testuser\\\" }\\n{ \\\"update\\\" : {\\\"_id\\\" : \\\"1\\\", \\\"_index\\\" : \\\"test2\\\"} }\\n{ \\\"doc\\\" : {\\\"user\\\" : \\\"example\\\"} }\\n```\\n{% include copy.html %}\\n\\n## Compression codecs\\n\\nIn OpenSearch 2.9 and later, there are two new codecs for compression: `zstd` and `zstd_no_dict`. You can optionally specify a compression level for these in the `index.codec.compression_level` setting with values in the [1, 6] range. [Benchmark]({{site.url}}{{site.baseurl}}\/im-plugin\/index-codecs\/#benchmarking) data shows that `zstd` provides a 7% better write throughput and `zstd_no_dict` provides a 14% better throughput, along with a 30% improvement in storage compared with the `default` codec. For more information about compression, see [Index codecs]({{site.url}}{{site.baseurl}}\/im-plugin\/index-codecs\/).\\n\\n\"","generated_answer":"There is not enough information available to answer this question. The report extract does not mention anything about using version 2.1 of OpenSearch or trying to use zstd compression. The information provided is about index codecs in general, but does not specifically address the issue you are facing with version 2.1 and zstd compression.","usage":"{\"inputTokens\": 4945, \"outputTokens\": 87, \"totalTokens\": 5032}","latency":1491,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.083,"cost":0.001345}
{"query_text":"I'm trying to set up this new aggregate view thing for saved objects in OpenSearch Dashboards, but I'm worried about messing up our existing multi-tenancy setup. The docs mention something about tenant indexes and a kibana_server role. How do I make sure I don't break anything when I turn this feature on? And what's the deal with not being able to turn it off once it's enabled","groundtruth_answer":"<question_answer>\nThe docs mention that the aggregate view feature for saved objects in OpenSearch Dashboards is an experimental feature that must be enabled using a feature flag in the opensearch_dashboards.yml file. Once enabled, it cannot be disabled without potentially causing issues like data loss or breaking tenant-to-tenant functionality. The docs also state that this feature should only be used in a new test cluster, not in an existing production cluster. There is no information provided about how to ensure the existing multi-tenancy setup is not broken when enabling this feature.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Dynamic configuration in OpenSearch Dashboards\\nparent: OpenSearch Dashboards multi-tenancy\\nnav_order: 147\\n---\\n\\n\\n# Dynamic configuration in OpenSearch Dashboards\\n\\nMulti-tenancy includes dynamic configuration options in OpenSearch Dashboards so you can manage common settings for tenancy without having to make changes to the configuration YAML files on each node and then restart the cluster. You can take advantage of this functionality by using the Dashboards interface or the REST API. The following list includes descriptions of the options currently covered by dynamic configuration:\\n\\n- **Disable or enable multi-tenancy**: Administrators can disable and enable multi-tenancy dynamically. Disabling multi-tenancy does not pose a risk of data loss. If and when an administrator chooses to reenable tenancy, all previously saved objects are preserved and made available. The default is `multitenancy_enabled: true`.\\n  \\n  This setting does not have an impact on the global tenant, which always remains enabled.\\n  {: .note }\\n\\n- **Disable or enable private tenant**: This option allows administrators to enable and disable private tenants. As with the enable multi-tenancy setting, when private tenants are reenabled all previously saved objects are preserved and made available.\\n- **Default tenant**: This option allows an administrator to choose either a global, private, or custom tenant as the default when users log in. In cases where a user doesn't have access to the default tenant (for example, if a custom tenant unavailable to the user was specified as the default), the default transitions to the preferred tenant, which is specified by the `opensearch_security.multitenancy.tenants.preferred` setting in the `opensearch-dashboards.yml` file. See [Multi-tenancy configuration]({{site.url}}{{site.baseurl}}\/security\/multi-tenancy\/multi-tenancy-config\/) for more information about this setting.\\n\\nDepending on the specific changes made to multi-tenancy using dynamic configuration, some users may be logged out of their Dashboards session once the changes are saved. For example, if an admin user disables multi-tenancy, users with either a private or custom tenant as their selected tenant will be logged out and will need to log back in. Similarly, if an admin user disables private tenants, users with the private tenant selected will be logged out and will need to log back in. \\n\\nThe global tenant, however, is a special case. Because this tenant is never disabled, users with the global tenant selected as their active tenant will experience no interruption to their session. Furthermore, changing the default tenant has no impact on a user's session.\\n\\n\\n## Configuring multi-tenancy in OpenSearch Dashboards\\n\\nTo configure multi-tenancy in Dashboards, follow these steps:\\n\\n1. Begin by selecting **Security** in the Dashboards home page menu. Then select **Tenancy** from the Security menu on the left side of the screen. The **Multi-tenancy** page is displayed. \\n1. By default, the **Manage** tab is displayed. Select the **Configure** tab to display the dynamic settings for multi-tenancy.\\n   * In the **Multi-tenancy** field, select the **Enable tenancy** check box to enable multi-tenancy. Clear the check box to disable the feature. The default is `true`.\\n   * In the **Tenants** field, you can enable or disable private tenants for users. By default the check box is selected and the feature is enabled.\\n   * In the **Default tenant** field, use the dropdown menu to select a default tenant. The menu includes Global, Private, and any other custom tenants that are available to users.\\n1. After making your preferred changes, select **Save changes** in the lower right corner of the window. A pop-up window appears listing the configuration items you've changed and asks you to review your changes.\\n1. Select the check boxes beside the items you want to confirm and then select **Apply changes**. The changes are implemented dynamically.\\n\\n\\n## Configuring multi-tenancy with the REST API\\n\\nIn addition to using the Dashboards interface, you can manage dynamic configurations using the REST API. \\n\\n### Get tenancy configuration\\n\\nThe GET call retrieves settings for the dynamic configuration:\\n\\n```json\\nGET \/_plugins\/_security\/api\/tenancy\/config\\n```\\n{% include copy-curl.html %}\\n\\n#### Example response\\n\\n```json\\n{\\n    \\\"mulitenancy_enabled\\\": true,\\n    \\\"private_tenant_enabled\\\": true,\\n    \\\"default_tenant\\\": \\\"global tenant\\\"\\n}\\n```\\n\\n### Update tenant configuration\\n\\nThe PUT call updates settings for dynamic configuration:\\n\\n```json\\nPUT \/_plugins\/_security\/api\/tenancy\/config\\n{\\n    \\\"default_tenant\\\": \\\"custom tenant 1\\\",\\n    \\\"private_tenant_enabled\\\": false,\\n    \\\"mulitenancy_enabled\\\": true\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Example response\\n\\n```json\\n{\\n    \\\"mulitenancy_enabled\\\": true,\\n    \\\"private_tenant_enabled\\\": false,\\n    \\\"default_tenant\\\": \\\"custom tenant 1\\\"\\n}\\n```\\n\\n### Dashboardsinfo API\\n\\nYou can also use the Dashboardsinfo API to retrieve the status of multi-tenancy settings for the user logged in to Dashboards:\\n\\n```json\\nGET \/_plugins\/_security\/dashboardsinfo\\n```\\n{% include copy-curl.html %}\\n\\n### Example response\\n\\n```json\\n{\\n  \\\"user_name\\\" : \\\"admin\\\",\\n  \\\"not_fail_on_forbidden_enabled\\\" : false,\\n  \\\"opensearch_dashboards_mt_enabled\\\" : true,\\n  \\\"opensearch_dashboards_index\\\" : \\\".kibana\\\",\\n  \\\"opensearch_dashboards_server_user\\\" : \\\"kibanaserver\\\",\\n  \\\"multitenancy_enabled\\\" : true,\\n  \\\"private_tenant_enabled\\\" : true,\\n  \\\"default_tenant\\\" : \\\"Private\\\"\\n}\\n```\\n\\n\\n\\n---\\nlayout: default\\ntitle: Multi-tenancy aggregate view for saved objects\\nparent: OpenSearch Dashboards multi-tenancy\\nnav_order: 150\\n---\\n\\n# OpenSearch Dashboards multi-tenancy aggregate view for saved objects\\n\\nThis is an experimental feature released in OpenSearch 2.4 and is not recommended for use in a production environment. For updates on the progress of the feature or if you want to leave feedback, see the [Dashboards object sharing](https:\/\/github.com\/opensearch-project\/OpenSearch-Dashboards\/issues\/2249) GitHub issue. For a more comprehensive view of the proposed future development of multi-tenancy, see the [Dashboards object sharing](https:\/\/github.com\/opensearch-project\/security\/issues\/1869) issue.\\n{: .warning}\\n\\nAggregate view for saved objects allows a user who has access to multiple tenants to see all saved objects associated with those tenants in a single view without having to switch between tenants to do so. This includes both tenants created by the user and tenants shared with the user. Aggregate view introduces a Tenant dropdown menu and column in the Saved Objects table that gives the user the option to filter by tenants and make visible their associated saved objects.\\n\\nOnce you identify a saved object of interest, you can then switch to that tenant to work with the object.\\n\\nTo access saved objects, expand the top menu and select **Management > Dashboards Management > Saved Objects**. The Saved Objects window opens. By default, all tenants the user has permissions for are displayed along with all saved objects associated with the tenants.\\n\\nAs an experimental feature, aggregate view for saved objects is kept behind a feature flag and must be enabled in the `opensearch_dashboards.yml` file before the feature is made available. See [Enabling aggregate view](#enabling-aggregate-view-for-saved-objects) for more information.\\n{: .note }\\n\\n### Feature benefits\\n\\n- Implementing an aggregate view for all saved objects on one screen allows you to quickly locate an object of interest and determine which tenant is associated with it. Once you locate an object, you can select the appropriate tenant and work with the object.\\n- This feature also adds a Tenant dropdown menu to the Saved Objects table, which allows you to filter the view by tenants and their associated saved objects.\\n\\n### Plans for future development\\n\\nIn subsequent releases, we plan to expand the functionality of this feature to include the ability to perform actions directly from aggregate view and share items without having to first select a specific tenant. In the longer term, OpenSearch plans to evolve multi-tenancy so that it becomes a much more flexible tool for sharing objects among users and employs a more sophisticated way of assigning the roles and permissions that facilitate sharing. To learn more about the features being proposed for future releases, see the GitHub issue [Dashboards object sharing](https:\/\/github.com\/opensearch-project\/security\/issues\/1869).\\n\\n### Known limitations\\n\\nIn this first experimental phase of development, there are some limitations that should be observed before enabling the feature and using it in a test environment:\\n\\n* The feature can only be used in a new cluster. At this time, the feature is not supported by clusters already in use.\\n* Also, the feature should be used only in a test environment, not in production. \\n* Finally, once the feature has been enabled and used in a test cluster, the feature cannot be disabled for the cluster. Disabling the feature once it has been used to work with tenants and saved objects can result in the loss of saved objects and can have an impact on tenant-to-tenant functionality. This can occur when disabling the feature in any one of three ways: disabling the aggregate view feature with the [feature flag](#enabling-aggregate-view-for-saved-objects); disabling multi-tenancy with the traditional [multi-tenancy configuration]({{site.url}}{{site.baseurl}}\/security\/multi-tenancy\/multi-tenancy-config\/) setting; or disabling multi-tenancy with [dynamic configuration]({{site.url}}{{site.baseurl}}\/security\/multi-tenancy\/dynamic-config\/) settings.\\n\\nThese limitations will be addressed in upcoming releases.\\n\\n## Enabling aggregate view for saved objects\\n\\nBy default, the aggregate view in the Saved Objects table is disabled. To enable the feature, add the `opensearch_security.multitenancy.enable_aggregation_view` flag to the `opensearch_dashboards.yml` file and set it to `true`:\\n\\n`opensearch_security.multitenancy.enable_aggregation_view: true`\\n\\nAfter enabling the feature you can start the new cluster and then launch Dashboards. \\n\\n## Working in aggregate view\\n\\nSelect the **Tenant** dropdown arrow to display the list of tenants available to the user. You can select multiple tenants while the menu is open. Each time you select a tenant in the menu, the list of saved objects is filtered by that tenant and any others with a check mark beside their name.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/Security\/Tenant_column.png\\\" alt=\\\"Dashboards Saved Objects view with emphasis on Tenants column\\\" width=\\\"500\\\">\\n   \\nAfter you finish specifying tenants, select anywhere outside the menu to collapse it. \\n* The Title column displays the names of the available saved objects. \\n* The Tenant column displays the tenants associated with the saved objects. \\n* Also, the number of tenants selected for filtering is shown in a red box beside the Tenant dropdown menu label.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/Security\/ten-filter-results.png\\\" alt=\\\"Dashboards Saved Objects tenant filtering\\\" width=\\\"700\\\">\\n\\nUse the **Type** dropdown menu to filter saved objects by type. The behavior of the **Type** dropdown menu is the same as the behavior of the **Tenant** dropdown menu.\\n\\n### Selecting and working with a saved object\\n\\nAfter identifying a saved object that you would like to work with, follow these steps to access the object:\\n\\n1. Note the tenant associated with the object in the Tenant column.\\n1. In the upper-right corner of the window, open the user menu and select **Switch tenants**.\\n<br><img src=\\\"{{site.url}}{{site.baseurl}}\/images\/Security\/switch_tenant.png\\\" alt=\\\"Switching tenants in the user menu\\\" width=\\\"425\\\">\\n1. In the **Select your tenant** window, choose either the Global or Private option, or one of the custom tenant options, to specify the correct tenant. Select the **Confirm** button. The tenant becomes active and is displayed in the user menu.\\n1. After the tenant is active, you can use the controls in the Actions column to work with saved objects associated with the tenant.\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/Security\/actions.png\\\" alt=\\\"Actions column controls\\\" width=\\\"700\\\">\\n\\nWhen a tenant is not active, you cannot use the Actions column controls to work with its associated objects. To work with those objects, follow the preceding steps to make the tenant active.\\n{: .note }\\n\\n\\n\\n---\\nlayout: default\\ntitle: Multi-tenancy configuration\\nparent: OpenSearch Dashboards multi-tenancy\\nnav_order: 145\\n---\\n\\n\\n# Multi-tenancy configuration\\n\\nMulti-tenancy is enabled in OpenSearch Dashboards by default. If you need to disable or change settings related to multi-tenancy, see the `kibana` settings in `config\/opensearch-security\/config.yml`, as shown in the following example:\\n\\n```yml\\nconfig:\\n  dynamic:\\n    kibana:\\n      multitenancy_enabled: true\\n      private_tenant_enabled: true\\n      default_tenant: global tenant\\n      server_username: kibanaserver\\n      index: '.kibana'\\n    do_not_fail_on_forbidden: false\\n```\\n\\n| Setting | Description |\\n| :--- | :--- |\\n| `multitenancy_enabled` | Enable or disable multi-tenancy. Default is `true`. |\\n| `private_tenant_enabled` | Enable or disable the private tenant. Default is `true`. |\\n| `default_tenant` | Use to set the tenant that is available when users log in. |\\n| `server_username` | Must match the name of the OpenSearch Dashboards server user in `opensearch_dashboards.yml`. Default is `kibanaserver`. If a different user is configured, then make sure that user is mapped to the `kibana_server` role through the `role_mappings.yml` file in order to give them the appropriate permissions listed in [kibana_server role details]({{site.url}}{{site.baseurl}}\/security\/multi-tenancy\/multi-tenancy-config\/#kibana_server-role-details). |\\n| `index` | Must match the name of the OpenSearch Dashboards index from `opensearch_dashboards.yml`. Default is `.kibana`. |\\n| `do_not_fail_on_forbidden` | When `true`, the Security plugin removes any content that a user is not allowed to see from the search results. When `false`, the plugin returns a security exception. Default is `false`. |\\n\\nThe `opensearch_dashboards.yml` file includes additional settings:\\n\\n```yml\\nopensearch.username: kibanaserver\\nopensearch.password: kibanaserver\\nopensearch.requestHeadersAllowlist: [\\\"securitytenant\\\",\\\"Authorization\\\"]\\nopensearch_security.multitenancy.enabled: true\\nopensearch_security.multitenancy.tenants.enable_global: true\\nopensearch_security.multitenancy.tenants.enable_private: true\\nopensearch_security.multitenancy.tenants.preferred: [\\\"Private\\\", \\\"Global\\\"]\\nopensearch_security.multitenancy.enable_filter: false\\n```\\n\\n| Setting | Description |\\n| :--- | :--- |\\n| `opensearch.requestHeadersAllowlist` | OpenSearch Dashboards requires that you add all HTTP headers to the allow list so that the headers pass to OpenSearch. Multi-tenancy uses a specific header, `securitytenant`, that must be present with the standard `Authorization` header. If the `securitytenant` header is not on the allow list, OpenSearch Dashboards starts with a red status.\\n| `opensearch_security.multitenancy.enabled` | Enables or disables multi-tenancy in OpenSearch Dashboards. Default is `true`. |\\n| `opensearch_security.multitenancy.tenants.enable_global` | Enables or disables the global tenant. Default is `true`. |\\n| `opensearch_security.multitenancy.tenants.enable_private` | Enables or disables private tenants. Default is `true`. |\\n| `opensearch_security.multitenancy.tenants.preferred` | Lets you change ordering in the **Tenants** tab of OpenSearch Dashboards. By default, the list starts with Global and Private (if enabled) and then proceeds alphabetically. You can add tenants here to move them to the top of the list. |\\n| `opensearch_security.multitenancy.enable_filter` | If you have many tenants, you can add a search bar to the top of the list. Default is `false`. |\\n\\n\\n## Add tenants\\n\\nTo create tenants, use OpenSearch Dashboards, the REST API, or `tenants.yml`.\\n\\n\\n#### OpenSearch Dashboards\\n\\n1. Open OpenSearch Dashboards.\\n1. Choose **Security**, **Tenants**, and **Create tenant**.\\n1. Give the tenant a name and description.\\n1. Choose **Create**.\\n\\n\\n#### REST API\\n\\nSee [Create tenant]({{site.url}}{{site.baseurl}}\/security\/access-control\/api\/#create-tenant).\\n\\n\\n#### tenants.yml\\n\\n```yml\\n---\\n_meta:\\n  type: \\\"tenants\\\"\\n  config_version: 2\\n\\n## Demo tenants\\nadmin_tenant:\\n  reserved: false\\n  description: \\\"Demo tenant for admin user\\\"\\n```\\n\\n## Give roles access to tenants\\n\\nAfter creating a tenant, give a role access to it using OpenSearch Dashboards, the REST API, or `roles.yml`.\\n\\n- Read-write (`kibana_all_write`) permissions let the role view and modify objects in the tenant.\\n- Read-only (`kibana_all_read`) permissions let the role view objects, but not modify them.\\n\\n\\n#### OpenSearch Dashboards\\n\\n1. Open OpenSearch Dashboards.\\n1. Choose **Security**, **Roles**, and a role.\\n1. For **Tenant permissions**, add tenants, press Enter, and give the role read and\/or write permissions to it.\\n\\n\\n#### REST API\\n\\nSee [Create role]({{site.url}}{{site.baseurl}}\/security\/access-control\/api\/#create-role).\\n\\n\\n#### roles.yml\\n\\n```yml\\n---\\ntest-role:\\n  reserved: false\\n  hidden: false\\n  cluster_permissions:\\n  - \\\"cluster_composite_ops\\\"\\n  - \\\"indices_monitor\\\"\\n  index_permissions:\\n  - index_patterns:\\n    - \\\"movies*\\\"\\n    dls: \\\"\\\"\\n    fls: []\\n    masked_fields: []\\n    allowed_actions:\\n    - \\\"read\\\"\\n  tenant_permissions:\\n  - tenant_patterns:\\n    - \\\"human_resources\\\"\\n    allowed_actions:\\n    - \\\"kibana_all_read\\\"\\n  static: false\\n_meta:\\n  type: \\\"roles\\\"\\n  config_version: 2\\n```\\n\\n\\n## Manage OpenSearch Dashboards indexes\\n\\nThe open source version of OpenSearch Dashboards saves all objects to a single index: `.kibana`. The Security plugin uses this index for the global tenant, but separate indexes for every other tenant. Each user also has a private tenant, so you might see a large number of indexes that follow two patterns:\\n\\n```\\n.kibana_<hash>_<tenant_name>\\n.kibana_<hash>_<username>\\n```\\n\\nThe Security plugin scrubs these index names of special characters, so they might not be a perfect match of tenant names and usernames.\\n{: .tip }\\n\\nTo back up your OpenSearch Dashboards data, [take a snapshot]({{site.url}}{{site.baseurl}}\/opensearch\/snapshots\/snapshot-restore\/) of all tenant indexes using an index pattern such as `.kibana*`.\\n\\n## `kibana_server` role details\\n\\nOpenSearch Dashboards uses the`kibana_server` role to perform necessary OpenSearch operations. By default, `kibanauser` is mapped to this role through the `role_mappings.yml` file. You can view the full list of permissions assigned to this role by sending a GET request to the `_plugins\/_security\/api\/roles\/kibana_server` API (include the admin certificate, key, and certificate authority file in the GET request).\\nThe following list includes the permissions assigned to this role:\\n\\n```\\n{\\n  \\\"kibana_server\\\" : {\\n    \\\"reserved\\\" : true,\\n    \\\"hidden\\\" : false,\\n    \\\"description\\\" : \\\"Provide the minimum permissions for the Kibana server\\\",\\n    \\\"cluster_permissions\\\" : [\\n      \\\"cluster_monitor\\\",\\n      \\\"cluster_composite_ops\\\",\\n      \\\"manage_point_in_time\\\",\\n      \\\"indices:admin\/template*\\\",\\n      \\\"indices:admin\/index_template*\\\",\\n      \\\"indices:data\/read\/scroll*\\\"\\n    ],\\n    \\\"index_permissions\\\" : [\\n      {\\n        \\\"index_patterns\\\" : [\\n          \\\".kibana\\\",\\n          \\\".opensearch_dashboards\\\"\\n        ],\\n        \\\"fls\\\" : [ ],\\n        \\\"masked_fields\\\" : [ ],\\n        \\\"allowed_actions\\\" : [\\n          \\\"indices_all\\\"\\n        ]\\n      },\\n      {\\n        \\\"index_patterns\\\" : [\\n          \\\".kibana-6\\\",\\n          \\\".opensearch_dashboards-6\\\"\\n        ],\\n        \\\"fls\\\" : [ ],\\n        \\\"masked_fields\\\" : [ ],\\n        \\\"allowed_actions\\\" : [\\n          \\\"indices_all\\\"\\n        ]\\n      },\\n      {\\n        \\\"index_patterns\\\" : [\\n          \\\".kibana_*\\\",\\n          \\\".opensearch_dashboards_*\\\"\\n        ],\\n        \\\"fls\\\" : [ ],\\n        \\\"masked_fields\\\" : [ ],\\n        \\\"allowed_actions\\\" : [\\n          \\\"indices_all\\\"\\n        ]\\n      },\\n      {\\n        \\\"index_patterns\\\" : [\\n          \\\".tasks\\\"\\n        ],\\n        \\\"fls\\\" : [ ],\\n        \\\"masked_fields\\\" : [ ],\\n        \\\"allowed_actions\\\" : [\\n          \\\"indices_all\\\"\\n        ]\\n      },\\n      {\\n        \\\"index_patterns\\\" : [\\n          \\\".management-beats*\\\"\\n        ],\\n        \\\"fls\\\" : [ ],\\n        \\\"masked_fields\\\" : [ ],\\n        \\\"allowed_actions\\\" : [\\n          \\\"indices_all\\\"\\n        ]\\n      },\\n      {\\n        \\\"index_patterns\\\" : [\\n          \\\"*\\\"\\n        ],\\n        \\\"fls\\\" : [ ],\\n        \\\"masked_fields\\\" : [ ],\\n        \\\"allowed_actions\\\" : [\\n          \\\"indices:admin\/aliases*\\\"\\n        ]\\n      }\\n    ],\\n    \\\"tenant_permissions\\\" : [ ],\\n    \\\"static\\\" : true\\n  }\\n}\\n```\\n\\n\\n\"","generated_answer":"There is not enough information available to answer this question. The report extract does not contain any information about setting up an aggregate view for saved objects in OpenSearch Dashboards or how to avoid breaking an existing multi-tenancy setup. The information provided is focused on configuring multi-tenancy in general, but does not cover the specific details about the aggregate view feature.","usage":"{\"inputTokens\": 5545, \"outputTokens\": 90, \"totalTokens\": 5635}","latency":1909,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.125,"cost":0.00149875}
{"query_text":"What software license does Opensearch use? ","groundtruth_answer":"<question_answer>There is not enough information available to answer this question<\/question_answer>","retrieved_chunks":"\"<img src=\\\"https:\/\/opensearch.org\/assets\/img\/opensearch-logo-themed.svg\\\" height=\\\"64px\\\">\\n\\n# About the OpenSearch documentation repo\\n\\nThe `documentation-website` repository contains the user documentation for OpenSearch. You can find the rendered documentation at [opensearch.org\/docs](https:\/\/opensearch.org\/docs).\\n\\n\\n## Contributing\\n\\nCommunity contributions remain essential to keeping the documentation comprehensive, useful, well organized, and up to date. If you are interested in submitting an issue or contributing content, see [CONTRIBUTING](CONTRIBUTING.md). \\n\\nThe following resources provide important guidance regarding contributions to the documentation:  \\n\\n- [OpenSearch Project Style Guidelines](STYLE_GUIDE.md) -- The style guide covers the style standards to be observed when creating OpenSearch Project content.\\n- [OpenSearch terms](TERMS.md) -- The terms list contains key OpenSearch terms and tips on how and when to use them.  \\n- [API Style Guide](API_STYLE_GUIDE.md) -- The API Style Guide provides the basic structure for creating OpenSearch API documentation.\\n- [Formatting Guide](FORMATTING_GUIDE.md) -- The OpenSearch documentation uses a modified version of the [just-the-docs](https:\/\/github.com\/pmarsceill\/just-the-docs) Jekyll theme. The Formatting Guide provides an overview of the commonly used formatting elements and how to add a page to the website.\\n\\n\\n## Points of contact\\n\\nIf you encounter problems or have questions when contributing to the documentation, these people can help:\\n\\n- [kolchfa-aws](https:\/\/github.com\/kolchfa-aws)\\n- [Naarcha-AWS](https:\/\/github.com\/Naarcha-AWS)\\n- [vagimeli](https:\/\/github.com\/vagimeli)\\n\\n\\n## Code of conduct\\n\\nThis project has adopted an [Open Source Code of Conduct](https:\/\/opensearch.org\/codeofconduct.html).\\n\\n\\n## Security\\n\\nIf you discover a potential security issue in this project, notify OpenSearch Security directly by emailing security@opensearch.org. To prevent any additional risk caused by the potential issue, do **not** create a public GitHub issue.\\n\\n## License\\n\\nThis project is licensed under the [Apache 2.0 License](LICENSE).\\n\\n\\n## Copyright\\n\\nCopyright OpenSearch contributors.\\n\\n\\n---\\nlayout: default\\ntitle: Getting started\\nnav_order: 1\\nhas_children: false\\nhas_toc: false\\nnav_exclude: true\\npermalink: \/about\/\\nredirect_from:\\n  - \/docs\/opensearch\/\\n  - \/opensearch\/\\n  - \/opensearch\/index\/\\n---\\n\\n{%- comment -%}The `\/docs\/opensearch\/` redirect is specifically to support the UI links in OpenSearch Dashboards 1.0.0.{%- endcomment -%}\\n\\n# OpenSearch and OpenSearch Dashboards\\n**Version {{site.opensearch_major_minor_version}}**\\n{: .label .label-blue }\\n\\nThis section contains documentation for OpenSearch and OpenSearch Dashboards.\\n\\n## Getting started\\n\\n- [Intro to OpenSearch]({{site.url}}{{site.baseurl}}\/intro\/)\\n- [Quickstart]({{site.url}}{{site.baseurl}}\/quickstart\/)\\n- [Install OpenSearch]({{site.url}}{{site.baseurl}}\/install-and-configure\/install-opensearch\/index\/)\\n- [Install OpenSearch Dashboards]({{site.url}}{{site.baseurl}}\/install-and-configure\/install-dashboards\/index\/)\\n- [See the FAQ](https:\/\/opensearch.org\/faq)\\n\\n## Why use OpenSearch?\\n\\nWith OpenSearch, you can perform the following use cases:\\n\\n<table style=\\\"table-layout: auto ; width: 100%;\\\">\\n<tbody>\\n<tr style=\\\"text-align: center; vertical-align:center;\\\">\\n<td><img src=\\\"{{site.url}}{{site.baseurl}}\/images\/1_search.png\\\" class=\\\"no-border\\\" alt=\\\"Fast, scalable full-text search\\\" height=\\\"100\\\"\/><\/td>\\n<td><img src=\\\"{{site.url}}{{site.baseurl}}\/images\/2_monitoring.png\\\" class=\\\"no-border\\\" alt=\\\"Application and infrastructure monitoring\\\" height=\\\"100\\\"\/><\/td>\\n<td><img src=\\\"{{site.url}}{{site.baseurl}}\/images\/3_security.png\\\" class=\\\"no-border\\\" alt=\\\"Security and event information management\\\" height=\\\"100\\\"\/><\/td>\\n<td><img src=\\\"{{site.url}}{{site.baseurl}}\/images\/4_tracking.png\\\" class=\\\"no-border\\\" alt=\\\"Operational health tracking\\\" height=\\\"100\\\"\/><\/td>\\n<\/tr>\\n<tr style=\\\"text-align: left; vertical-align:top; font-weight: bold; color: rgb(0,59,92)\\\">\\n<td>Fast, Scalable Full-text Search<\/td>\\n<td>Application and Infrastructure Monitoring<\/td>\\n<td>Security and Event Information Management<\/td>\\n<td>Operational Health Tracking<\/td>\\n<\/tr>\\n<tr style=\\\"text-align: left; vertical-align:top;\\\">\\n<td>Help users find the right information within your application, website, or data lake catalog. <\/td>\\n<td>Easily store and analyze log data, and set automated alerts for underperformance.<\/td>\\n<td>Centralize logs to enable real-time security monitoring and forensic analysis.<\/td>\\n<td>Use observability logs, metrics, and traces to monitor your applications and business in real time.<\/td>\\n<\/tr>\\n<\/tbody>\\n<\/table>\\n\\n**Additional features and plugins:**\\n\\nOpenSearch has several features and plugins to help index, secure, monitor, and analyze your data. Most OpenSearch plugins have corresponding OpenSearch Dashboards plugins that provide a convenient, unified user interface.\\n- [Anomaly detection]({{site.url}}{{site.baseurl}}\/monitoring-plugins\/ad\/) - Identify atypical data and receive automatic notifications\\n- [KNN]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/) - Find \\u201cnearest neighbors\\u201d in your vector data\\n- [Performance Analyzer]({{site.url}}{{site.baseurl}}\/monitoring-plugins\/pa\/) - Monitor and optimize your cluster\\n- [SQL]({{site.url}}{{site.baseurl}}\/search-plugins\/sql\/index\/) - Use SQL or a piped processing language to query your data\\n- [Index State Management]({{site.url}}{{site.baseurl}}\/im-plugin\/) - Automate index operations\\n- [ML Commons plugin]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/index\/) - Train and execute machine-learning models\\n- [Asynchronous search]({{site.url}}{{site.baseurl}}\/search-plugins\/async\/) - Run search requests in the background\\n- [Cross-cluster replication]({{site.url}}{{site.baseurl}}\/replication-plugin\/index\/) - Replicate your data across multiple OpenSearch clusters\\n\\n\\n## The secure path forward\\nOpenSearch includes a demo configuration so that you can get up and running quickly, but before using OpenSearch in a production environment, you must [configure the Security plugin manually]({{site.url}}{{site.baseurl}}\/security\/configuration\/index\/) with your own certificates, authentication method, users, and passwords.\\n\\n## Looking for the Javadoc?\\n\\nSee [opensearch.org\/javadocs\/](https:\/\/opensearch.org\/javadocs\/).\\n\\n## Get involved\\n\\n[OpenSearch](https:\/\/opensearch.org) is supported by Amazon Web Services. All components are available under the [Apache License, Version 2.0](https:\/\/www.apache.org\/licenses\/LICENSE-2.0.html) on [GitHub](https:\/\/github.com\/opensearch-project\/).\\nThe project welcomes GitHub issues, bug fixes, features, plugins, documentation---anything at all. To get involved, see [Contributing](https:\/\/opensearch.org\/source.html) on the OpenSearch website.\\n\\n---\\n\\n<small>OpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V.<\/small>\\n\\n\"","generated_answer":"The OpenSearch project is licensed under the Apache 2.0 License.","usage":"{\"inputTokens\": 2098, \"outputTokens\": 31, \"totalTokens\": 2129}","latency":637,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.118,"cost":0.00056325}
{"query_text":"Does GPU accelerated nodes support Pytorch?","groundtruth_answer":"<question_answer>Yes, GPU accelerated nodes support PyTorch 1.12.1 according to the report.<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: GPU acceleration\\nparent: Using ML models within OpenSearch\\ngrand_parent: Integrating ML models\\nnav_order: 150\\n---\\n\\n\\n# GPU acceleration \\n\\nWhen running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training. \\n\\n## Supported GPUs\\n\\nCurrently, ML nodes support the following GPU instances:\\n\\n- [NVIDIA instances with CUDA 11.6](https:\/\/aws.amazon.com\/nvidia\/)\\n- [AWS Inferentia](https:\/\/aws.amazon.com\/machine-learning\/inferentia\/)\\n\\nIf you need GPU power, you can provision GPU instances through [Amazon Elastic Compute Cloud (Amazon EC2)](https:\/\/aws.amazon.com\/ec2\/). For more information on how to provision a GPU instance, see [Recommended GPU Instances](https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/gpu.html).\\n\\n## Supported images\\n\\nYou can use GPU acceleration with both [Docker images](https:\/\/gitlab.com\/nvidia\/container-images\/cuda\/blob\/master\/doc\/supported-tags.md) with CUDA 11.6 and [Amazon Machine Images (AMIs)](https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/AMIs.html).\\n\\n## PyTorch\\n\\nGPU-accelerated ML nodes require [PyTorch](https:\/\/pytorch.org\/docs\/stable\/index.html) 1.12.1 work with ML models.\\n\\n## Setting up a GPU-accelerated ML node\\n\\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts. \\n\\n### Preparing an NVIDIA ML node\\n\\nNVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the `nvidia-uvm` kernel inside the `\/dev` directory. To check for the kernel, enter `ls -al \/dev | grep nvidia-uvm`.\\n\\nIf the `nvidia-uvm` kernel does not exist, run `nvidia-uvm-init.sh`:\\n\\n```\\n#!\/bin\/bash\\n## Script to initialize nvidia device nodes.\\n## https:\/\/docs.nvidia.com\/cuda\/cuda-installation-guide-linux\/index.html#runfile-verifications\\n\/sbin\/modprobe nvidia\\nif [ \\\"$?\\\" -eq 0 ]; then\\n  # Count the number of NVIDIA controllers found.\\n  NVDEVS=`lspci | grep -i NVIDIA`\\n  N3D=`echo \\\"$NVDEVS\\\" | grep \\\"3D controller\\\" | wc -l`\\n  NVGA=`echo \\\"$NVDEVS\\\" | grep \\\"VGA compatible controller\\\" | wc -l`\\n  N=`expr $N3D + $NVGA - 1`\\n  for i in `seq 0 $N`; do\\n    mknod -m 666 \/dev\/nvidia$i c 195 $i\\n  done\\n  mknod -m 666 \/dev\/nvidiactl c 195 255\\nelse\\n  exit 1\\nfi\\n\/sbin\/modprobe nvidia-uvm\\nif [ \\\"$?\\\" -eq 0 ]; then\\n  # Find out the major device number used by the nvidia-uvm driver\\n  D=`grep nvidia-uvm \/proc\/devices | awk '{print $1}'`\\n  mknod -m 666 \/dev\/nvidia-uvm c $D 0\\n  mknod -m 666 \/dev\/nvidia-uvm-tools c $D 0\\nelse\\n  exit 1\\nfi\\n```\\n\\nAfter verifying that `nvidia-uvm` exists under `\/dev`, you can start OpenSearch inside your cluster. \\n\\n### Preparing AWS Inferentia ML node\\n\\nDepending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster. \\n\\nTo start, [download and install OpenSearch]({{site.url}}{{site.baseurl}}\/install-and-configure\/index\/) on your cluster.\\n\\nThen export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory `opensearch-2.5.0`, so `OPENSEARCH_HOME` = `opensearch-2.5.0`:\\n\\n```\\necho \\\"export OPENSEARCH_HOME=~\/opensearch-2.5.0\\\" | tee -a ~\/.bash_profile\\necho \\\"export PYTORCH_VERSION=1.12.1\\\" | tee -a ~\/.bash_profile\\nsource ~\/.bash_profile\\n```\\n\\nNext, create a shell script file called `prepare_torch_neuron.sh`. You can copy and customize one of the following examples based on your Linux operating system:\\n\\n- [Ubuntu 20.04](#ubuntu-2004)\\n- [Amazon Linux 2](#amazon-linux-2)\\n\\nAfter you've run the scripts, exit your current terminal and open a new terminal to start OpenSearch.\\n\\nGPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems.\\n{: .note}\\n\\n#### Ubuntu 20.04\\n\\n```\\n. \/etc\/os-release\\nsudo tee \/etc\/apt\/sources.list.d\/neuron.list > \/dev\/null <<EOF\\ndeb https:\/\/apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main\\nEOF\\nwget -qO - https:\/\/apt.repos.neuron.amazonaws.com\/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -\\n\\n# Update OS packages\\nsudo apt-get update -y\\n\\n################################################################################################################\\n# To install or update to Neuron versions 1.19.1 and newer from previous releases:\\n# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver\\n################################################################################################################\\n\\n# Install OS headers\\nsudo apt-get install linux-headers-$(uname -r) -y\\n\\n# Install Neuron Driver\\nsudo apt-get install aws-neuronx-dkms -y\\n\\n####################################################################################\\n# Warning: If Linux kernel is updated as a result of OS package update\\n#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot\\n####################################################################################\\n\\n# Install Neuron Tools\\nsudo apt-get install aws-neuronx-tools -y\\n\\n######################################################\\n#   Only for Ubuntu 20 - Install Python3.7\\nsudo add-apt-repository ppa:deadsnakes\/ppa\\nsudo apt-get install python3.7\\n######################################################\\n# Install Python venv and activate Python virtual environment to install    \\n# Neuron pip packages.\\ncd ~\\nsudo apt-get install -y python3.7-venv g++\\npython3.7 -m venv pytorch_venv\\nsource pytorch_venv\/bin\/activate\\npip install -U pip\\n\\n# Set pip repository to point to the Neuron repository\\npip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\\n\\n#Install Neuron PyTorch\\npip install torch-neuron torchvision\\n# If you need to trace the neuron model, install torch neuron with this command\\n# pip install torch-neuron neuron-cc[tensorflow] \\\"protobuf==3.20.1\\\" torchvision\\n\\n# If you need to trace neuron model, install the transformers for tracing the Huggingface model.\\n# pip install transformers\\n\\n# Copy torch neuron lib to OpenSearch\\nPYTORCH_NEURON_LIB_PATH=~\/pytorch_venv\/lib\/python3.7\/site-packages\/torch_neuron\/lib\/\\nmkdir -p $OPENSEARCH_HOME\/lib\/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH\/ $OPENSEARCH_HOME\/lib\/torch_neuron\\nexport PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\necho \\\"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\\" | tee -a ~\/.bash_profile\\n\\n# Increase JVm stack size to >=2MB\\necho \\\"-Xss2m\\\" | tee -a $OPENSEARCH_HOME\/config\/jvm.options\\n# Increase max file descriptors to 65535\\necho \\\"$(whoami) - nofile 65535\\\" | sudo tee -a \/etc\/security\/limits.conf\\n# max virtual memory areas vm.max_map_count to 262144\\nsudo sysctl -w vm.max_map_count=262144\\n```\\n\\n#### Amazon Linux 2\\n\\n```\\n# Configure Linux for Neuron repository updates\\nsudo tee \/etc\/yum.repos.d\/neuron.repo > \/dev\/null <<EOF\\n[neuron]\\nname=Neuron YUM Repository\\nbaseurl=https:\/\/yum.repos.neuron.amazonaws.com\\nenabled=1\\nmetadata_expire=0\\nEOF\\nsudo rpm --import https:\/\/yum.repos.neuron.amazonaws.com\/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\\n# Update OS packages\\nsudo yum update -y\\n################################################################################################################\\n# To install or update to Neuron versions 1.19.1 and newer from previous releases:\\n# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver\\n################################################################################################################\\n# Install OS headers\\nsudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y\\n# Install Neuron Driver\\n####################################################################################\\n# Warning: If Linux kernel is updated as a result of OS package update\\n#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot\\n####################################################################################\\nsudo yum install aws-neuronx-dkms -y\\n# Install Neuron Tools\\nsudo yum install aws-neuronx-tools -y\\n\\n# Install Python venv and activate Python virtual environment to install    \\n# Neuron pip packages.\\ncd ~\\nsudo yum install -y python3.7-venv gcc-c++\\npython3.7 -m venv pytorch_venv\\nsource pytorch_venv\/bin\/activate\\npip install -U pip\\n\\n# Set Pip repository  to point to the Neuron repository\\npip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\\n\\n# Install Neuron PyTorch\\npip install torch-neuron torchvision\\n# If you need to trace the neuron model, install torch neuron with this command\\n# pip install torch-neuron neuron-cc[tensorflow] \\\"protobuf<4\\\" torchvision\\n\\n# If you need to run the trace neuron model, install transformers for tracing Huggingface model.\\n# pip install transformers\\n\\n# Copy torch neuron lib to OpenSearch\\nPYTORCH_NEURON_LIB_PATH=~\/pytorch_venv\/lib\/python3.7\/site-packages\/torch_neuron\/lib\/\\nmkdir -p $OPENSEARCH_HOME\/lib\/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH\/ $OPENSEARCH_HOME\/lib\/torch_neuron\\nexport PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\necho \\\"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\\" | tee -a ~\/.bash_profile\\n# Increase JVm stack size to >=2MB\\necho \\\"-Xss2m\\\" | tee -a $OPENSEARCH_HOME\/config\/jvm.options\\n# Increase max file descriptors to 65535\\necho \\\"$(whoami) - nofile 65535\\\" | sudo tee -a \/etc\/security\/limits.conf\\n# max virtual memory areas vm.max_map_count to 262144\\nsudo sysctl -w vm.max_map_count=262144\\n```\\n\\nWhen the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch.\\n\\nOpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually.\\n\\n#### Prepare ML node manually\\n\\nIf the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually:\\n\\n1. Deploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see [Deploy on AWS accelerator instance](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/frameworks\/torch\/torch-neuron\/setup\/pytorch-install.html#deploy-on-aws-ml-accelerator-instance).\\n\\n2. Copy the Neuron library into OpenSearch. The following command uses a directory named `opensearch-2.5.0`:\\n\\n   ```\\n   OPENSEARCH_HOME=~\/opensearch-2.5.0\\n   ```\\n\\n3. Set the `PYTORCH_EXTRA_LIBRARY_PATH` path. In this example, we create a `pytorch` virtual environment in the OPENSEARCH_HOME folder:\\n\\n   ```\\n   PYTORCH_NEURON_LIB_PATH=~\/pytorch_venv\/lib\/python3.7\/site-packages\/torch_neuron\/lib\/\\n\\n\\n   mkdir -p $OPENSEARCH_HOME\/lib\/torch_neuron; cp -r  $PYTORCH_NEURON_LIB_PATH\/ $OPENSEARCH_HOME\/lib\/torch_neuron\\n   export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\n  ```\\n\\n4. (Optional) To monitor the GPU usage of your accelerator instance, install [Neuron tools](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/tools\/index.html), which allows models to be used inside your instance:\\n\\n   ```\\n   # Install Neuron Tools\\n   sudo apt-get install aws-neuronx-tools -y\\n   ```\\n\\n   ```\\n   # Add Neuron tools your PATH\\n   export PATH=\/opt\/aws\/neuron\/bin:$PATH\\n   ```\\n  \\n   ```\\n   # Test Neuron tools\\n   neuron-top\\n   ```\\n\\n\\n5. To make sure you have enough memory to upload a model, increase the JVM stack size to `>+2MB`:\\n\\n   ```\\n   echo \\\"-Xss2m\\\" | sudo tee -a $OPENSEARCH_HOME\/config\/jvm.options\\n   ```\\n\\n6. Start OpenSearch. \\n\\n## Troubleshooting\\n\\nDue to the amount of data required to work with ML models, you might encounter the following `max file descriptors` or `vm.max_map_count` errors when trying to run OpenSearch in a your cluster: \\n\\n```\\n[1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535]\\n[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\\n```\\n\\nTo troubleshoot the max file descriptors error, run the following command:\\n\\n```\\necho \\\"$(whoami) - nofile 65535\\\" | sudo tee -a \/etc\/security\/limits.conf\\n```\\n\\nTo fix the `vm.max_map_count` error, run this command to increase the count to `262114`:\\n\\n```\\nsudo sysctl -w vm.max_map_count=262144\\n```\\n\\n## Next steps\\n\\nIf you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see [Compiling and Deploying HuggingFace Pretrained BERT](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/src\/examples\/pytorch\/bert_tutorial\/tutorial_pretrained_bert.html).\\n\\n\\n\\n\"","generated_answer":"Yes, GPU-accelerated nodes support PyTorch. The report states that \"GPU-accelerated ML nodes require PyTorch 1.12.1 to work with ML models.\"","usage":"{\"inputTokens\": 4048, \"outputTokens\": 55, \"totalTokens\": 4103}","latency":859,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.139,"cost":0.00108075}
{"query_text":"Does opensearch support hugging face models? If so which ones? ","groundtruth_answer":"<question_answer>\nYes, OpenSearch supports Hugging Face models for text embedding and cross-encoding tasks. The supported Hugging Face models are listed under the \"Sentence transformers\" and \"Cross-encoder models\" sections in the provided context.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Pretrained models\\nparent: Using ML models within OpenSearch\\ngrand_parent: Integrating ML models\\nnav_order: 120\\n---\\n\\n# OpenSearch-provided pretrained models\\n**Introduced 2.9**\\n{: .label .label-purple }\\n\\nOpenSearch provides a variety of open-source pretrained models that can assist with a range of machine learning (ML) search and analytics use cases. You can upload any supported model to the OpenSearch cluster and use it locally.\\n\\n## Supported pretrained models\\n\\nOpenSearch supports the following models, categorized by type. Text embedding models are sourced from [Hugging Face](https:\/\/huggingface.co\/). Sparse encoding models are trained by OpenSearch. Although models with the same type will have similar use cases, each model has a different model size and will perform differently depending on your cluster setup. For a performance comparison of some pretrained models, see the [SBERT documentation](https:\/\/www.sbert.net\/docs\/pretrained_models.html#model-overview).\\n\\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\\n{: .important}\\n\\n### Sentence transformers\\n\\nSentence transformer models map sentences and paragraphs across a dimensional dense vector space. The number of vectors depends on the type of model. You can use these models for use cases such as clustering or semantic search.\\n\\nThe following table provides a list of sentence transformer models and artifact links you can use to download them. Note that you must prefix the model name with `huggingface\/`, as shown in the **Model name** column. \\n\\n| Model name | Version | Vector dimensions | Auto-truncation | TorchScript artifact | ONNX artifact |\\n|:---|:---|:---|:---|:---|:---|\\n| `huggingface\/sentence-transformers\/all-distilroberta-v1` | 1.0.1 | 768-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-distilroberta-v1\/1.0.1\/torch_script\/sentence-transformers_all-distilroberta-v1-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-distilroberta-v1\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-distilroberta-v1\/1.0.1\/onnx\/sentence-transformers_all-distilroberta-v1-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-distilroberta-v1\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/all-MiniLM-L6-v2` | 1.0.1 | 384-dimensional dense vector space.  | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L6-v2\/1.0.1\/torch_script\/sentence-transformers_all-MiniLM-L6-v2-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L6-v2\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L6-v2\/1.0.1\/onnx\/sentence-transformers_all-MiniLM-L6-v2-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L6-v2\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/all-MiniLM-L12-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L12-v2\/1.0.1\/torch_script\/sentence-transformers_all-MiniLM-L12-v2-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L12-v2\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L12-v2\/1.0.1\/onnx\/sentence-transformers_all-MiniLM-L12-v2-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L12-v2\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/all-mpnet-base-v2` | 1.0.1 | 768-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-mpnet-base-v2\/1.0.1\/torch_script\/sentence-transformers_all-mpnet-base-v2-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-mpnet-base-v2\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-mpnet-base-v2\/1.0.1\/onnx\/sentence-transformers_all-mpnet-base-v2-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-mpnet-base-v2\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b` | 1.0.2 | 768-dimensional dense vector space. Optimized for semantic search. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.2\/torch_script\/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.2-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.2\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.2\/onnx\/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.2-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.2\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/multi-qa-MiniLM-L6-cos-v1` | 1.0.1 | 384-dimensional dense vector space. Designed for semantic search and trained on 215 million question\/answer pairs. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-MiniLM-L6-cos-v1\/1.0.1\/torch_script\/sentence-transformers_multi-qa-MiniLM-L6-cos-v1-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-MiniLM-L6-cos-v1\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-MiniLM-L6-cos-v1\/1.0.1\/onnx\/sentence-transformers_multi-qa-MiniLM-L6-cos-v1-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-MiniLM-L6-cos-v1\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/multi-qa-mpnet-base-dot-v1` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-mpnet-base-dot-v1\/1.0.1\/torch_script\/sentence-transformers_multi-qa-mpnet-base-dot-v1-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-mpnet-base-dot-v1\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-mpnet-base-dot-v1\/1.0.1\/onnx\/sentence-transformers_multi-qa-mpnet-base-dot-v1-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/multi-qa-mpnet-base-dot-v1\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/paraphrase-MiniLM-L3-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-MiniLM-L3-v2\/1.0.1\/torch_script\/sentence-transformers_paraphrase-MiniLM-L3-v2-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-MiniLM-L3-v2\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-MiniLM-L3-v2\/1.0.1\/onnx\/sentence-transformers_paraphrase-MiniLM-L3-v2-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-MiniLM-L3-v2\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/paraphrase-multilingual-MiniLM-L12-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-multilingual-MiniLM-L12-v2\/1.0.1\/torch_script\/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-multilingual-MiniLM-L12-v2\/1.0.1\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-multilingual-MiniLM-L12-v2\/1.0.1\/onnx\/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2-1.0.1-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-multilingual-MiniLM-L12-v2\/1.0.1\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/paraphrase-mpnet-base-v2` | 1.0.0 | 768-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-mpnet-base-v2\/1.0.0\/torch_script\/sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-mpnet-base-v2\/1.0.0\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-mpnet-base-v2\/1.0.0\/onnx\/sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/paraphrase-mpnet-base-v2\/1.0.0\/onnx\/config.json) |\\n| `huggingface\/sentence-transformers\/distiluse-base-multilingual-cased-v1` | 1.0.1 | 512-dimensional dense vector space. | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/distiluse-base-multilingual-cased-v1\/1.0.1\/torch_script\/sentence-transformers_distiluse-base-multilingual-cased-v1-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/distiluse-base-multilingual-cased-v1\/1.0.1\/torch_script\/config.json) | Not available |\\n\\n\\n### Sparse encoding models\\n**Introduced 2.11**\\n{: .label .label-purple }\\n\\nSparse encoding models transfer text into a sparse vector and convert the vector to a list of `<token: weight>` pairs representing the text entry and its corresponding weight in the sparse vector. You can use these models for use cases such as clustering or sparse neural search.\\n\\nWe recommend the following combinations for optimal performance:\\n\\n- Use the `amazon\/neural-sparse\/opensearch-neural-sparse-encoding-v2-distill` model during both ingestion and search.\\n- Use the `amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-distill` model during ingestion and the\\n`amazon\/neural-sparse\/opensearch-neural-sparse-tokenizer-v1` tokenizer during search.\\n\\nFor more information about the preceding options for running neural sparse search, see [Generating sparse vector embeddings within OpenSearch]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-sparse-with-pipelines\/).\\n\\nThe following table provides a list of sparse encoding models and artifact links you can use to download them.\\n\\n| Model name | Version | Auto-truncation | TorchScript artifact | Description |\\n|:---|:---|:---|:---|:---|\\n| `amazon\/neural-sparse\/opensearch-neural-sparse-encoding-v1` | 1.0.1 | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-v1\/1.0.1\/torch_script\/neural-sparse_opensearch-neural-sparse-encoding-v1-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-v1\/1.0.1\/torch_script\/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https:\/\/huggingface.co\/opensearch-project\/opensearch-neural-sparse-encoding-v1). |\\n| `amazon\/neural-sparse\/opensearch-neural-sparse-encoding-v2-distill` | 1.0.0 | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-v2-distill\/1.0.0\/torch_script\/neural-sparse_opensearch-neural-sparse-encoding-v2-distill-1.0.0-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-v2-distill\/1.0.0\/torch_script\/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https:\/\/huggingface.co\/opensearch-project\/opensearch-neural-sparse-encoding-v2-distill). |\\n| `amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v1` | 1.0.1 | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v1\/1.0.1\/torch_script\/neural-sparse_opensearch-neural-sparse-encoding-doc-v1-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v1\/1.0.1\/torch_script\/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https:\/\/huggingface.co\/opensearch-project\/opensearch-neural-sparse-encoding-doc-v1). |\\n| `amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-distill` | 1.0.0 | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-distill\/1.0.0\/torch_script\/neural-sparse_opensearch-neural-sparse-encoding-doc-v2-distill-1.0.0-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-distill\/1.0.0\/torch_script\/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https:\/\/huggingface.co\/opensearch-project\/opensearch-neural-sparse-encoding-doc-v2-distill). |\\n| `amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-mini` | 1.0.0 | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-mini\/1.0.0\/torch_script\/neural-sparse_opensearch-neural-sparse-encoding-doc-v2-mini-1.0.0-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-mini\/1.0.0\/torch_script\/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https:\/\/huggingface.co\/opensearch-project\/opensearch-neural-sparse-encoding-doc-v2-mini). |\\n| `amazon\/neural-sparse\/opensearch-neural-sparse-tokenizer-v1` | 1.0.1 | Yes | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-tokenizer-v1\/1.0.1\/torch_script\/neural-sparse_opensearch-neural-sparse-tokenizer-v1-1.0.1-torch_script.zip)<br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-tokenizer-v1\/1.0.1\/torch_script\/config.json) | A neural sparse tokenizer. The tokenizer splits text into tokens and assigns each token a predefined weight, which is the token's inverse document frequency (IDF). If the IDF file is not provided, the weight defaults to 1. For more information, see [Preparing a model]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/custom-local-models\/#preparing-a-model). |\\n\\n### Cross-encoder models\\n**Introduced 2.12**\\n{: .label .label-purple }\\n\\nCross-encoder models support query reranking. \\n\\nThe following table provides a list of cross-encoder models and artifact links you can use to download them. Note that you must prefix the model name with `huggingface\/cross-encoders`, as shown in the **Model name** column. \\n\\n| Model name | Version | TorchScript artifact | ONNX artifact |\\n|:---|:---|:---|:---|\\n| `huggingface\/cross-encoders\/ms-marco-MiniLM-L-6-v2` | 1.0.2 | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-6-v2\/1.0.2\/torch_script\/cross-encoders_ms-marco-MiniLM-L-6-v2-1.0.2-torch_script.zip) <br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-6-v2\/1.0.2\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-6-v2\/1.0.2\/onnx\/cross-encoders_ms-marco-MiniLM-L-6-v2-1.0.2-onnx.zip) <br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-6-v2\/1.0.2\/onnx\/config.json) |\\n| `huggingface\/cross-encoders\/ms-marco-MiniLM-L-12-v2` | 1.0.2 | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-12-v2\/1.0.2\/torch_script\/cross-encoders_ms-marco-MiniLM-L-12-v2-1.0.2-torch_script.zip) <br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-12-v2\/1.0.2\/torch_script\/config.json) | - [model_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-12-v2\/1.0.2\/onnx\/cross-encoders_ms-marco-MiniLM-L-12-v2-1.0.2-onnx.zip) <br>- [config_url](https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/cross-encoders\/ms-marco-MiniLM-L-12-v2\/1.0.2\/onnx\/config.json) \\n\\n## Prerequisites\\n\\nOn clusters with dedicated ML nodes, specify `\\\"only_run_on_ml_node\\\": \\\"true\\\"` for improved performance. For more information, see [ML Commons cluster settings]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/cluster-settings\/). \\n\\nThis example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. To ensure that this basic local setup works, specify the following cluster settings:\\n\\n```json\\nPUT _cluster\/settings\\n{\\n  \\\"persistent\\\": {\\n    \\\"plugins\\\": {\\n      \\\"ml_commons\\\": {\\n        \\\"only_run_on_ml_node\\\": \\\"false\\\",\\n        \\\"model_access_control_enabled\\\": \\\"true\\\",\\n        \\\"native_memory_threshold\\\": \\\"99\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Step 1: Register a model group\\n\\nTo register a model, you have the following options:\\n\\n- You can use `model_group_id` to register a model version to an existing model group.\\n- If you do not use `model_group_id`, ML Commons creates a model with a new model group.\\n\\nTo register a model group, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/model_groups\/_register\\n{\\n  \\\"name\\\": \\\"local_model_group\\\",\\n  \\\"description\\\": \\\"A model group for local models\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the model group ID that you'll use to register a model to this model group:\\n\\n```json\\n{\\n \\\"model_group_id\\\": \\\"wlcnb4kBJ1eYAeTMHlV6\\\",\\n \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nTo learn more about model groups, see [Model access control]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/model-access-control\/).\\n\\n## Step 2: Register a local OpenSearch-provided model\\n\\nTo register an OpenSearch-provided model to the model group created in step 1, provide the model group ID from step 1 in the following request.\\n\\nBecause pretrained models originate from the ML Commons model repository, you only need to provide the `name`, `version`, `model_group_id`, and `model_format` in the register API request:  \\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n  \\\"name\\\": \\\"huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\\\",\\n  \\\"version\\\": \\\"1.0.2\\\",\\n  \\\"model_group_id\\\": \\\"Z1eQf4oB5Vm0Tdw8EIP2\\\",\\n  \\\"model_format\\\": \\\"TORCH_SCRIPT\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nOpenSearch returns the task ID of the register operation:\\n\\n```json\\n{\\n  \\\"task_id\\\": \\\"cVeMb4kBJ1eYAeTMFFgj\\\",\\n  \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nTo check the status of the operation, provide the task ID to the [Tasks API]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/tasks-apis\/get-task\/):\\n\\n```bash\\nGET \/_plugins\/_ml\/tasks\/cVeMb4kBJ1eYAeTMFFgj\\n```\\n{% include copy-curl.html %}\\n\\nWhen the operation is complete, the state changes to `COMPLETED`:\\n\\n```json\\n{\\n  \\\"model_id\\\": \\\"cleMb4kBJ1eYAeTMFFg4\\\",\\n  \\\"task_type\\\": \\\"REGISTER_MODEL\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"state\\\": \\\"COMPLETED\\\",\\n  \\\"worker_node\\\": [\\n    \\\"XPcXLV7RQoi5m8NI_jEOVQ\\\"\\n  ],\\n  \\\"create_time\\\": 1689793598499,\\n  \\\"last_update_time\\\": 1689793598530,\\n  \\\"is_async\\\": false\\n}\\n```\\n\\nTake note of the returned `model_id` because you\\u2019ll need it to deploy the model.\\n\\n## Step 3: Deploy the model\\n\\nThe deploy operation reads the model's chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into and longer it takes for the model to load into memory.\\n\\nTo deploy the registered model, provide its model ID from step 3 in the following request:\\n\\n```bash\\nPOST \/_plugins\/_ml\/models\/cleMb4kBJ1eYAeTMFFg4\/_deploy\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the task ID that you can use to check the status of the deploy operation:\\n\\n```json\\n{\\n  \\\"task_id\\\": \\\"vVePb4kBJ1eYAeTM7ljG\\\",\\n  \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nAs in the previous step, check the status of the operation by calling the Tasks API:\\n\\n```bash\\nGET \/_plugins\/_ml\/tasks\/vVePb4kBJ1eYAeTM7ljG\\n```\\n{% include copy-curl.html %}\\n\\nWhen the operation is complete, the state changes to `COMPLETED`:\\n\\n```json\\n{\\n  \\\"model_id\\\": \\\"cleMb4kBJ1eYAeTMFFg4\\\",\\n  \\\"task_type\\\": \\\"DEPLOY_MODEL\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"state\\\": \\\"COMPLETED\\\",\\n  \\\"worker_node\\\": [\\n    \\\"n-72khvBTBi3bnIIR8FTTw\\\"\\n  ],\\n  \\\"create_time\\\": 1689793851077,\\n  \\\"last_update_time\\\": 1689793851101,\\n  \\\"is_async\\\": true\\n}\\n```\\n\\nIf a cluster or node is restarted, then you need to redeploy the model. To learn how to set up automatic redeployment, see [Enable auto redeploy]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/cluster-settings\/#enable-auto-redeploy).\\n{: .tip} \\n\\n## Step 4 (Optional): Test the model\\n\\nUse the [Predict API]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/train-predict\/predict\/) to test the model.\\n\\n### Text embedding model\\n\\nFor a text embedding model, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/text_embedding\/cleMb4kBJ1eYAeTMFFg4\\n{\\n  \\\"text_docs\\\":[ \\\"today is sunny\\\"],\\n  \\\"return_number\\\": true,\\n  \\\"target_response\\\": [\\\"sentence_embedding\\\"]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains text embeddings for the provided sentence:\\n\\n```json\\n{\\n  \\\"inference_results\\\" : [\\n    {\\n      \\\"output\\\" : [\\n        {\\n          \\\"name\\\" : \\\"sentence_embedding\\\",\\n          \\\"data_type\\\" : \\\"FLOAT32\\\",\\n          \\\"shape\\\" : [\\n            768\\n          ],\\n          \\\"data\\\" : [\\n            0.25517133,\\n            -0.28009856,\\n            0.48519906,\\n            ...\\n          ]\\n        }\\n      ]\\n    }\\n  ]\\n}\\n```\\n\\n### Sparse encoding model\\n\\nFor a sparse encoding model, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/sparse_encoding\/cleMb4kBJ1eYAeTMFFg4\\n{\\n  \\\"text_docs\\\":[ \\\"today is sunny\\\"]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the tokens and weights:\\n\\n```json\\n{\\n  \\\"inference_results\\\": [\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"name\\\": \\\"output\\\",\\n          \\\"dataAsMap\\\": {\\n            \\\"response\\\": [\\n              {\\n                \\\"saturday\\\": 0.48336542,\\n                \\\"week\\\": 0.1034762,\\n                \\\"mood\\\": 0.09698499,\\n                \\\"sunshine\\\": 0.5738209,\\n                \\\"bright\\\": 0.1756877,\\n                ...\\n              }\\n          }\\n        }\\n    }\\n}\\n```\\n\\n### Cross-encoder model\\n\\nFor a cross-encoder model, send the following request:\\n\\n```json\\nPOST _plugins\/_ml\/models\/<model_id>\/_predict\\n{\\n    \\\"query_text\\\": \\\"today is sunny\\\",\\n    \\\"text_docs\\\": [\\n        \\\"how are you\\\",\\n        \\\"today is sunny\\\",\\n        \\\"today is july fifth\\\",\\n        \\\"it is winter\\\"\\n    ]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe model calculates the similarity score of `query_text` and each document in `text_docs` and returns a list of scores for each document in the order they were provided in `text_docs`:\\n\\n```json\\n{\\n  \\\"inference_results\\\": [\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"name\\\": \\\"similarity\\\",\\n          \\\"data_type\\\": \\\"FLOAT32\\\",\\n          \\\"shape\\\": [\\n            1\\n          ],\\n          \\\"data\\\": [\\n            -6.077798\\n          ],\\n          \\\"byte_buffer\\\": {\\n            \\\"array\\\": \\\"Un3CwA==\\\",\\n            \\\"order\\\": \\\"LITTLE_ENDIAN\\\"\\n          }\\n        }\\n      ]\\n    },\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"name\\\": \\\"similarity\\\",\\n          \\\"data_type\\\": \\\"FLOAT32\\\",\\n          \\\"shape\\\": [\\n            1\\n          ],\\n          \\\"data\\\": [\\n            10.223609\\n          ],\\n          \\\"byte_buffer\\\": {\\n            \\\"array\\\": \\\"55MjQQ==\\\",\\n            \\\"order\\\": \\\"LITTLE_ENDIAN\\\"\\n          }\\n        }\\n      ]\\n    },\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"name\\\": \\\"similarity\\\",\\n          \\\"data_type\\\": \\\"FLOAT32\\\",\\n          \\\"shape\\\": [\\n            1\\n          ],\\n          \\\"data\\\": [\\n            -1.3987057\\n          ],\\n          \\\"byte_buffer\\\": {\\n            \\\"array\\\": \\\"ygizvw==\\\",\\n            \\\"order\\\": \\\"LITTLE_ENDIAN\\\"\\n          }\\n        }\\n      ]\\n    },\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"name\\\": \\\"similarity\\\",\\n          \\\"data_type\\\": \\\"FLOAT32\\\",\\n          \\\"shape\\\": [\\n            1\\n          ],\\n          \\\"data\\\": [\\n            -4.5923924\\n          ],\\n          \\\"byte_buffer\\\": {\\n            \\\"array\\\": \\\"4fSSwA==\\\",\\n            \\\"order\\\": \\\"LITTLE_ENDIAN\\\"\\n          }\\n        }\\n      ]\\n    }\\n  ]\\n}\\n```\\n\\nA higher document score means higher similarity. In the preceding response, documents are scored as follows against the query text `today is sunny`:\\n\\nDocument text | Score\\n:--- | :---\\n`how are you` | -6.077798\\n`today is sunny` | 10.223609\\n`today is july fifth` | -1.3987057\\n`it is winter` | -4.5923924\\n\\nThe document that contains the same text as the query is scored the highest, and the remaining documents are scored based on the text similarity.\\n\\n## Step 5: Use the model for search\\n\\nTo learn how to set up a vector index and use text embedding models for search, see [Semantic search]({{site.url}}{{site.baseurl}}\/search-plugins\/semantic-search\/).\\n\\nTo learn how to set up a vector index and use sparse encoding models for search, see [Neural sparse search]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-sparse-search\/).\\n\\nTo learn how to use cross-encoder models for reranking, see [Reranking search results]({{site.url}}{{site.baseurl}}\/search-plugins\/search-relevance\/reranking-search-results\/).\\n\\n\\n\\n\"","generated_answer":"Yes, OpenSearch supports Hugging Face models. The following Hugging Face sentence transformer models are supported:\n\n- `huggingface\/sentence-transformers\/all-distilroberta-v1`\n- `huggingface\/sentence-transformers\/all-MiniLM-L6-v2`\n- `huggingface\/sentence-transformers\/all-MiniLM-L12-v2`\n- `huggingface\/sentence-transformers\/all-mpnet-base-v2`\n- `huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b`\n- `huggingface\/sentence-transformers\/multi-qa-MiniLM-L6-cos-v1`\n- `huggingface\/sentence-transformers\/multi-qa-mpnet-base-dot-v1`\n- `huggingface\/sentence-transformers\/paraphrase-MiniLM-L3-v2`\n- `huggingface\/sentence-transformers\/paraphrase-multilingual-MiniLM-L12-v2`\n- `huggingface\/sentence-transformers\/paraphrase-mpnet-base-v2`\n- `huggingface\/sentence-transformers\/distiluse-base-multilingual-cased-v1`","usage":"{\"inputTokens\": 9854, \"outputTokens\": 332, \"totalTokens\": 10186}","latency":3399,"reasoning":"None","score":null,"faithfulness":0.5,"answer_relevancy":0.14,"cost":0.0028785}
{"query_text":"I have a custom model, can I run it in Opensearch?","groundtruth_answer":"<question_answer>\nYes, you can run your custom model in OpenSearch. The documentation provides instructions on how to register and deploy a custom model in OpenSearch. You need to provide the model file URL, model configuration details like model type and embedding dimension, and other metadata to register the model. Once registered, you can deploy the model and use it for inference tasks like text embedding or neural search.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Custom models\\nparent: Using ML models within OpenSearch\\ngrand_parent: Integrating ML models\\nnav_order: 120\\n---\\n\\n# Custom local models\\n**Introduced 2.9**\\n{: .label .label-purple }\\n\\nTo use a custom model locally, you can upload it to the OpenSearch cluster.\\n\\n## Model support\\n\\nAs of OpenSearch 2.6, OpenSearch supports local text embedding models.\\n\\nAs of OpenSearch 2.11, OpenSearch supports local sparse encoding models.\\n\\nAs of OpenSearch 2.12, OpenSearch supports local cross-encoder models.\\n\\nAs of OpenSearch 2.13, OpenSearch supports local question answering models.\\n\\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\\n{: .important}\\n\\n## Preparing a model\\n\\nFor all the models, you must provide a tokenizer JSON file within the model zip file.\\n\\nFor sparse encoding models, make sure your output format is `{\\\"output\\\":<sparse_vector>}` so that ML Commons can post-process the sparse vector.\\n\\nIf you fine-tune a sparse model on your own dataset, you may also want to use your own sparse tokenizer model. It is preferable to provide your own [IDF](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf) JSON file in the tokenizer model zip file because this increases query performance when you use the tokenizer model in the query. Alternatively, you can use an OpenSearch-provided generic [IDF from MSMARCO](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-tokenizer-v1\/1.0.0\/torch_script\/opensearch-neural-sparse-tokenizer-v1-1.0.0.zip). If the IDF file is not provided, the default weight of each token is set to 1, which may influence sparse neural search performance.  \\n\\n### Model format\\n\\nTo use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the [TorchScript](https:\/\/pytorch.org\/docs\/stable\/jit.html) and [ONNX](https:\/\/onnx.ai\/) formats.\\n\\nYou must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript [model file](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/ml-algorithms\/src\/test\/resources\/org\/opensearch\/ml\/engine\/algorithms\/text_embedding\/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip).\\n\\nAdditionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:\\n\\n```bash\\nshasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip\\n```\\n\\n### Model size\\n\\nMost deep learning models are more than 100 MB, making it difficult to fit them into a single document. OpenSearch splits the model file into smaller chunks to be stored in a model index. When allocating ML or data nodes for your OpenSearch cluster, make sure you correctly size your ML nodes so that you have enough memory when making ML inferences.\\n\\n## Prerequisites \\n\\nTo upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from [Hugging Face](https:\/\/huggingface.co\/), or train a new model in accordance with your needs.\\n\\n### Cluster settings\\n\\nThis example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. \\n\\nOn clusters with dedicated ML nodes, specify `\\\"only_run_on_ml_node\\\": \\\"true\\\"` for improved performance. For more information, see [ML Commons cluster settings]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/cluster-settings\/).\\n\\nTo ensure that this basic local setup works, specify the following cluster settings:\\n\\n```json\\nPUT _cluster\/settings\\n{\\n  \\\"persistent\\\": {\\n    \\\"plugins\\\": {\\n      \\\"ml_commons\\\": {\\n        \\\"allow_registering_model_via_url\\\": \\\"true\\\",\\n        \\\"only_run_on_ml_node\\\": \\\"false\\\",\\n        \\\"model_access_control_enabled\\\": \\\"true\\\",\\n        \\\"native_memory_threshold\\\": \\\"99\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Step 1: Register a model group\\n\\nTo register a model, you have the following options:\\n\\n- You can use `model_group_id` to register a model version to an existing model group.\\n- If you do not use `model_group_id`, ML Commons creates a model with a new model group.\\n\\nTo register a model group, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/model_groups\/_register\\n{\\n  \\\"name\\\": \\\"local_model_group\\\",\\n  \\\"description\\\": \\\"A model group for local models\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the model group ID that you'll use to register a model to this model group:\\n\\n```json\\n{\\n \\\"model_group_id\\\": \\\"wlcnb4kBJ1eYAeTMHlV6\\\",\\n \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nTo learn more about model groups, see [Model access control]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/model-access-control\/).\\n\\n## Step 2: Register a local model\\n\\nTo register a local model to the model group created in step 1, send a Register Model API request. For descriptions of Register Model API parameters, see [Register a model]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/model-apis\/register-model\/). \\n\\nThe `function_name` corresponds to the model type. For text embedding models, set this parameter to `TEXT_EMBEDDING`. For sparse encoding models, set this parameter to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`. For cross-encoder models, set this parameter to `TEXT_SIMILARITY`. For question answering models, set this parameter to `QUESTION_ANSWERING`. In this example, set `function_name` to `TEXT_EMBEDDING` because you're registering a text embedding model. \\n\\nProvide the model group ID from step 1 and send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n  \\\"name\\\": \\\"huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\\\",\\n  \\\"version\\\": \\\"1.0.1\\\",\\n  \\\"model_group_id\\\": \\\"wlcnb4kBJ1eYAeTMHlV6\\\",\\n  \\\"description\\\": \\\"This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"model_format\\\": \\\"TORCH_SCRIPT\\\",\\n  \\\"model_content_size_in_bytes\\\": 266352827,\\n  \\\"model_content_hash_value\\\": \\\"acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413\\\",\\n  \\\"model_config\\\": {\\n    \\\"model_type\\\": \\\"distilbert\\\",\\n    \\\"embedding_dimension\\\": 768,\\n    \\\"framework_type\\\": \\\"sentence_transformers\\\",\\n    \\\"all_config\\\": \\\"{\\\\\\\"_name_or_path\\\\\\\":\\\\\\\"old_models\/msmarco-distilbert-base-tas-b\/0_Transformer\\\\\\\",\\\\\\\"activation\\\\\\\":\\\\\\\"gelu\\\\\\\",\\\\\\\"architectures\\\\\\\":[\\\\\\\"DistilBertModel\\\\\\\"],\\\\\\\"attention_dropout\\\\\\\":0.1,\\\\\\\"dim\\\\\\\":768,\\\\\\\"dropout\\\\\\\":0.1,\\\\\\\"hidden_dim\\\\\\\":3072,\\\\\\\"initializer_range\\\\\\\":0.02,\\\\\\\"max_position_embeddings\\\\\\\":512,\\\\\\\"model_type\\\\\\\":\\\\\\\"distilbert\\\\\\\",\\\\\\\"n_heads\\\\\\\":12,\\\\\\\"n_layers\\\\\\\":6,\\\\\\\"pad_token_id\\\\\\\":0,\\\\\\\"qa_dropout\\\\\\\":0.1,\\\\\\\"seq_classif_dropout\\\\\\\":0.2,\\\\\\\"sinusoidal_pos_embds\\\\\\\":false,\\\\\\\"tie_weights_\\\\\\\":true,\\\\\\\"transformers_version\\\\\\\":\\\\\\\"4.7.0\\\\\\\",\\\\\\\"vocab_size\\\\\\\":30522}\\\"\\n  },\\n  \\\"created_time\\\": 1676073973126,\\n  \\\"url\\\": \\\"https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.1\/torch_script\/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nNote that in OpenSearch Dashboards, wrapping the `all_config` field contents in triple quotes (`\\\"\\\"\\\"`) automatically escapes quotation marks within the field and provides better readability:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n  \\\"name\\\": \\\"huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\\\",\\n  \\\"version\\\": \\\"1.0.1\\\",\\n  \\\"model_group_id\\\": \\\"wlcnb4kBJ1eYAeTMHlV6\\\",\\n  \\\"description\\\": \\\"This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"model_format\\\": \\\"TORCH_SCRIPT\\\",\\n  \\\"model_content_size_in_bytes\\\": 266352827,\\n  \\\"model_content_hash_value\\\": \\\"acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413\\\",\\n  \\\"model_config\\\": {\\n    \\\"model_type\\\": \\\"distilbert\\\",\\n    \\\"embedding_dimension\\\": 768,\\n    \\\"framework_type\\\": \\\"sentence_transformers\\\",\\n    \\\"all_config\\\": \\\"\\\"\\\"{\\\"_name_or_path\\\":\\\"old_models\/msmarco-distilbert-base-tas-b\/0_Transformer\\\",\\\"activation\\\":\\\"gelu\\\",\\\"architectures\\\":[\\\"DistilBertModel\\\"],\\\"attention_dropout\\\":0.1,\\\"dim\\\":768,\\\"dropout\\\":0.1,\\\"hidden_dim\\\":3072,\\\"initializer_range\\\":0.02,\\\"max_position_embeddings\\\":512,\\\"model_type\\\":\\\"distilbert\\\",\\\"n_heads\\\":12,\\\"n_layers\\\":6,\\\"pad_token_id\\\":0,\\\"qa_dropout\\\":0.1,\\\"seq_classif_dropout\\\":0.2,\\\"sinusoidal_pos_embds\\\":false,\\\"tie_weights_\\\":true,\\\"transformers_version\\\":\\\"4.7.0\\\",\\\"vocab_size\\\":30522}\\\"\\\"\\\"\\n  },\\n  \\\"created_time\\\": 1676073973126,\\n  \\\"url\\\": \\\"https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.1\/torch_script\/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip\\\"\\n}\\n```\\n{% include copy.html %}\\n\\nOpenSearch returns the task ID of the register operation:\\n\\n```json\\n{\\n  \\\"task_id\\\": \\\"cVeMb4kBJ1eYAeTMFFgj\\\",\\n  \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nTo check the status of the operation, provide the task ID to the [Get task]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/tasks-apis\/get-task\/):\\n\\n```bash\\nGET \/_plugins\/_ml\/tasks\/cVeMb4kBJ1eYAeTMFFgj\\n```\\n{% include copy-curl.html %}\\n\\nWhen the operation is complete, the state changes to `COMPLETED`:\\n\\n```json\\n{\\n  \\\"model_id\\\": \\\"cleMb4kBJ1eYAeTMFFg4\\\",\\n  \\\"task_type\\\": \\\"REGISTER_MODEL\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"state\\\": \\\"COMPLETED\\\",\\n  \\\"worker_node\\\": [\\n    \\\"XPcXLV7RQoi5m8NI_jEOVQ\\\"\\n  ],\\n  \\\"create_time\\\": 1689793598499,\\n  \\\"last_update_time\\\": 1689793598530,\\n  \\\"is_async\\\": false\\n}\\n```\\n\\nTake note of the returned `model_id` because you\\u2019ll need it to deploy the model.\\n\\n## Step 3: Deploy the model\\n\\nThe deploy operation reads the model's chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into and longer it takes for the model to load into memory.\\n\\nTo deploy the registered model, provide its model ID from step 3 in the following request:\\n\\n```bash\\nPOST \/_plugins\/_ml\/models\/cleMb4kBJ1eYAeTMFFg4\/_deploy\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the task ID that you can use to check the status of the deploy operation:\\n\\n```json\\n{\\n  \\\"task_id\\\": \\\"vVePb4kBJ1eYAeTM7ljG\\\",\\n  \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nAs in the previous step, check the status of the operation by calling the Tasks API:\\n\\n```bash\\nGET \/_plugins\/_ml\/tasks\/vVePb4kBJ1eYAeTM7ljG\\n```\\n{% include copy-curl.html %}\\n\\nWhen the operation is complete, the state changes to `COMPLETED`:\\n\\n```json\\n{\\n  \\\"model_id\\\": \\\"cleMb4kBJ1eYAeTMFFg4\\\",\\n  \\\"task_type\\\": \\\"DEPLOY_MODEL\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"state\\\": \\\"COMPLETED\\\",\\n  \\\"worker_node\\\": [\\n    \\\"n-72khvBTBi3bnIIR8FTTw\\\"\\n  ],\\n  \\\"create_time\\\": 1689793851077,\\n  \\\"last_update_time\\\": 1689793851101,\\n  \\\"is_async\\\": true\\n}\\n```\\n\\nIf a cluster or node is restarted, then you need to redeploy the model. To learn how to set up automatic redeployment, see [Enable auto redeploy]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/cluster-settings\/#enable-auto-redeploy).\\n{: .tip} \\n\\n## Step 4 (Optional): Test the model\\n\\nUse the [Predict API]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/train-predict\/predict\/) to test the model.\\n\\nFor a text embedding model, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/text_embedding\/cleMb4kBJ1eYAeTMFFg4\\n{\\n  \\\"text_docs\\\":[ \\\"today is sunny\\\"],\\n  \\\"return_number\\\": true,\\n  \\\"target_response\\\": [\\\"sentence_embedding\\\"]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains text embeddings for the provided sentence:\\n\\n```json\\n{\\n  \\\"inference_results\\\" : [\\n    {\\n      \\\"output\\\" : [\\n        {\\n          \\\"name\\\" : \\\"sentence_embedding\\\",\\n          \\\"data_type\\\" : \\\"FLOAT32\\\",\\n          \\\"shape\\\" : [\\n            768\\n          ],\\n          \\\"data\\\" : [\\n            0.25517133,\\n            -0.28009856,\\n            0.48519906,\\n            ...\\n          ]\\n        }\\n      ]\\n    }\\n  ]\\n}\\n```\\n\\nFor a sparse encoding model, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/sparse_encoding\/cleMb4kBJ1eYAeTMFFg4\\n{\\n  \\\"text_docs\\\":[ \\\"today is sunny\\\"]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the tokens and weights:\\n\\n```json\\n{\\n  \\\"inference_results\\\": [\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"name\\\": \\\"output\\\",\\n          \\\"dataAsMap\\\": {\\n            \\\"response\\\": [\\n              {\\n                \\\"saturday\\\": 0.48336542,\\n                \\\"week\\\": 0.1034762,\\n                \\\"mood\\\": 0.09698499,\\n                \\\"sunshine\\\": 0.5738209,\\n                \\\"bright\\\": 0.1756877,\\n                ...\\n              }\\n          }\\n        }\\n    }\\n}\\n```\\n\\n## Step 5: Use the model for search\\n\\nTo learn how to use the model for vector search, see [Using an ML model for neural search]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-search\/#using-an-ml-model-for-neural-search).\\n\\n## Question answering models\\n\\nA question answering model extracts the answer to a question from a given context. ML Commons supports context in `text` format.\\n\\nTo register a question answering model, send a request in the following format. Specify the `function_name` as `QUESTION_ANSWERING`:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n    \\\"name\\\": \\\"question_answering\\\",\\n    \\\"version\\\": \\\"1.0.0\\\",\\n    \\\"function_name\\\": \\\"QUESTION_ANSWERING\\\",\\n    \\\"description\\\": \\\"test model\\\",\\n    \\\"model_format\\\": \\\"TORCH_SCRIPT\\\",\\n    \\\"model_group_id\\\": \\\"lN4AP40BKolAMNtR4KJ5\\\",\\n    \\\"model_content_hash_value\\\": \\\"e837c8fc05fd58a6e2e8383b319257f9c3859dfb3edc89b26badfaf8a4405ff6\\\",\\n    \\\"model_config\\\": { \\n        \\\"model_type\\\": \\\"bert\\\",\\n        \\\"framework_type\\\": \\\"huggingface_transformers\\\"\\n    },\\n    \\\"url\\\": \\\"https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/main\/ml-algorithms\/src\/test\/resources\/org\/opensearch\/ml\/engine\/algorithms\/question_answering\/question_answering_pt.zip?raw=true\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThen send a request to deploy the model:\\n\\n```json\\nPOST _plugins\/_ml\/models\/<model_id>\/_deploy\\n```\\n{% include copy-curl.html %}\\n\\nTo test a question answering model, send the following request. It requires a `question` and the relevant `context` from which the answer will be generated:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/question_answering\/<model_id>\\n{\\n  \\\"question\\\": \\\"Where do I live?\\\"\\n  \\\"context\\\": \\\"My name is John. I live in New York\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response provides the answer based on the context:\\n\\n```json\\n{\\n  \\\"inference_results\\\": [\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"result\\\": \\\"New York\\\"\\n        }\\n    }\\n}\\n```\\n\\n\\n---\\nlayout: default\\ntitle: Register model\\nparent: Model APIs\\ngrand_parent: ML Commons APIs\\nnav_order: 10\\n---\\n\\n# Register a model\\n\\nAll versions of a particular model are held in a model group. You can either [register a model group]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/model-group-apis\/register-model-group\/) before registering a model to the group or register a first version of a model, thereby creating the group. Each model group name in the cluster must be globally unique. \\n\\nIf you are registering the first version of a model without first registering the model group, a new model group is created automatically with the following name and access level:\\n\\n- Name: The new model group will have the same name as the model. Because the model group name must be unique, ensure that your model name does not have the same name as any model groups in the cluster. \\n- Access level: The access level for the new model group is determined using the `access_mode`, `backend_roles`, and `add_all_backend_roles` parameters that you pass in the request. If you provide none of the three parameters, the new model group will be `private` if model access control is enabled on your cluster and `public` if model access control is disabled. The newly registered model is the first model version assigned to that model group. \\n\\nOnce a model group is created, provide its `model_group_id` to register a new model version to the model group. In this case, the model name does not need to be unique.\\n\\nIf you're using [pretrained models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/pretrained-models#supported-pretrained-models) provided by OpenSearch, we recommend that you first register a model group with a unique name for these models. Then register the pretrained models as versions to that model group. This ensures that every model group has a globally unique model group name.\\n{: .tip}\\n\\nFor information about user access for this API, see [Model access control considerations]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/model-apis\/index\/#model-access-control-considerations).\\n\\nIf the model is more than 10 MB in size, ML Commons splits it into smaller chunks and saves those chunks in the model's index.\\n\\n## Path and HTTP methods\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n```\\n\\n## Query parameters\\n\\nThe following table lists the available query parameters. All query parameters are optional.\\n\\n| Parameter | Data type | Description |\\n| :--- | :--- | :--- |\\n| `deploy` | Boolean | Whether to deploy the model after registering it. The deploy operation is performed by calling the [Deploy Model API]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/model-apis\/deploy-model\/). Default is `false`. |\\n\\n## Register an OpenSearch-provided pretrained model\\n\\nOpenSearch provides several pretrained models. For more information, see [OpenSearch-provided pretrained models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/pretrained-models\/).\\n\\n### Register a pretrained text embedding model\\n\\nTo register a pretrained text embedding model, the only required parameters are `name`, `version`, and `model_format`.\\n\\n#### Request fields\\n\\nThe following table lists the available request fields.\\n\\nField | Data type | Required\/Optional | Description\\n:---  | :--- | :--- \\n`name`| String | Required | The model name. |\\n`version` | String | Required | The model version. |\\n`model_format` | String | Required | The portable format of the model file. Valid values are `TORCH_SCRIPT` and `ONNX`. |\\n`description` | String | Optional| The model description. |\\n`model_group_id` | String | Optional | The ID of the model group to which to register the model.\\n\\n#### Example request: OpenSearch-provided text embedding model\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n  \\\"name\\\": \\\"huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\\\",\\n  \\\"version\\\": \\\"1.0.1\\\",\\n  \\\"model_group_id\\\": \\\"Z1eQf4oB5Vm0Tdw8EIP2\\\",\\n  \\\"model_format\\\": \\\"TORCH_SCRIPT\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Register a pretrained sparse encoding model\\n\\nTo register a pretrained sparse encoding model, you must set the function name to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`.\\n\\n#### Request fields\\n\\nThe following table lists the available request fields.\\n\\nField | Data type | Required\/Optional | Description\\n:---  | :--- | :--- \\n`name`| String | Required | The model name. |\\n`version` | String | Required | The model version. |\\n`model_format` | String | Required | The portable format of the model file. Valid values are `TORCH_SCRIPT` and `ONNX`. |\\n`function_name` | String | Required | For text embedding models, set this parameter to `TEXT_EMBEDDING`. For sparse encoding models, set this parameter to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`. For cross-encoder models, set this parameter to `TEXT_SIMILARITY`. For question answering models, set this parameter to `QUESTION_ANSWERING`.\\n`model_content_hash_value` | String | Required | The model content hash generated using the SHA-256 hashing algorithm.\\n`url` | String | Required | The URL that contains the model. |\\n`description` | String | Optional| The model description. |\\n`model_group_id` | String | Optional | The ID of the model group to which to register this model.\\n\\n#### Example request: OpenSearch-provided sparse encoding model\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n    \\\"name\\\": \\\"amazon\/neural-sparse\/opensearch-neural-sparse-encoding-doc-v2-distill\\\",\\n    \\\"version\\\": \\\"1.0.0\\\",\\n    \\\"model_group_id\\\": \\\"Z1eQf4oB5Vm0Tdw8EIP2\\\",\\n    \\\"model_format\\\": \\\"TORCH_SCRIPT\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Register a custom model \\n\\nTo use a custom model locally within the OpenSearch cluster, you need to provide a URL and a config object for that model. For more information, see [Custom local models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/custom-local-models\/).\\n\\n### Request fields\\n\\nThe following table lists the available request fields.\\n\\nField | Data type | Required\/Optional | Description\\n:---  | :--- | :--- \\n`name`| String | Required | The model name. |\\n`version` | String | Required | The model version. |\\n`model_format` | String | Required | The portable format of the model file. Valid values are `TORCH_SCRIPT` and `ONNX`. |\\n`function_name` | String | Required | Set this parameter to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`.\\n`model_content_hash_value` | String | Required | The model content hash generated using the SHA-256 hashing algorithm.\\n[`model_config`](#the-model_config-object)  | Object | Required | The model's configuration, including the `model_type`, `embedding_dimension`, and `framework_type`. `all_config` is an optional JSON string that contains all model configurations. |\\n`url` | String | Required | The URL that contains the model. |\\n`description` | String | Optional| The model description. |\\n`model_group_id` | String | Optional | The model group ID of the model group to register this model to. \\n`is_enabled`| Boolean | Optional | Specifies whether the model is enabled. Disabling the model makes it unavailable for Predict API requests, regardless of the model's deployment status. Default is `true`.\\n`rate_limiter` | Object | Optional | Limits the number of times that any user can call the Predict API on the model. For more information, see [Rate limiting inference calls]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/integrating-ml-models\/#rate-limiting-inference-calls).\\n`interface`| Object | Optional | The interface for the model. For more information, see [Interface](#the-interface-parameter).|\\n\\n#### The `model_config` object\\n\\n| Field | Data type | Description |\\n| :--- | :--- | :--- \\n| `model_type` | String | The model type, such as `bert`. For a Hugging Face model, the model type is specified in `config.json`. For an example, see the [`all-MiniLM-L6-v2` Hugging Face model `config.json`](https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\/blob\/main\/config.json#L15). Required. |\\n| `embedding_dimension` | Integer | The dimension of the model-generated dense vector. For a Hugging Face model, the dimension is specified in the model card. For example, in the [`all-MiniLM-L6-v2` Hugging Face model card](https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2), the statement `384 dimensional dense vector space` specifies 384 as the embedding dimension. Required. |\\n| `framework_type` | String  | The framework the model is using. Currently, OpenSearch supports `sentence_transformers` and `huggingface_transformers` frameworks. The `sentence_transformers` model outputs text embeddings directly, so ML Commons does not perform any post processing. For `huggingface_transformers`, ML Commons performs post processing by applying mean pooling to get text embeddings. See the example [`all-MiniLM-L6-v2` Hugging Face model](https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2) for more details. Required. |\\n| `all_config` | String | This field is used for reference purposes. You can specify all model configurations in this field. For example, if you are using a Hugging Face model, you can minify the `config.json` file to one line and save its contents in the `all_config` field. Once the model is uploaded, you can use the get model API operation to get all model configurations stored in this field. Optional. |\\n\\nYou can further customize a pretrained sentence transformer model's post-processing logic with the following optional fields in the `model_config` object.\\n\\n| Field | Data type | Description |\\n| :--- | :--- | :--- |\\n| `pooling_mode` | String | The post-process model output, either `mean`, `mean_sqrt_len`, `max`, `weightedmean`, or `cls`.|\\n| `normalize_result` | Boolean | When set to `true`, normalizes the model output in order to scale to a standard range for the model. |\\n\\n#### Example request: Custom model\\n\\nThe following example request registers a version `1.0.0` of an NLP sentence transformation model named `all-MiniLM-L6-v2`.\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n    \\\"name\\\": \\\"all-MiniLM-L6-v2\\\",\\n    \\\"version\\\": \\\"1.0.0\\\",\\n    \\\"description\\\": \\\"test model\\\",\\n    \\\"model_format\\\": \\\"TORCH_SCRIPT\\\",\\n    \\\"model_group_id\\\": \\\"FTNlQ4gBYW0Qyy5ZoxfR\\\",\\n    \\\"model_content_hash_value\\\": \\\"c15f0d2e62d872be5b5bc6c84d2e0f4921541e29fefbef51d59cc10a8ae30e0f\\\",\\n    \\\"model_config\\\": {\\n        \\\"model_type\\\": \\\"bert\\\",\\n        \\\"embedding_dimension\\\": 384,\\n        \\\"framework_type\\\": \\\"sentence_transformers\\\",\\n       \\\"all_config\\\": \\\"{\\\\\\\"_name_or_path\\\\\\\":\\\\\\\"nreimers\/MiniLM-L6-H384-uncased\\\\\\\",\\\\\\\"architectures\\\\\\\":[\\\\\\\"BertModel\\\\\\\"],\\\\\\\"attention_probs_dropout_prob\\\\\\\":0.1,\\\\\\\"gradient_checkpointing\\\\\\\":false,\\\\\\\"hidden_act\\\\\\\":\\\\\\\"gelu\\\\\\\",\\\\\\\"hidden_dropout_prob\\\\\\\":0.1,\\\\\\\"hidden_size\\\\\\\":384,\\\\\\\"initializer_range\\\\\\\":0.02,\\\\\\\"intermediate_size\\\\\\\":1536,\\\\\\\"layer_norm_eps\\\\\\\":1e-12,\\\\\\\"max_position_embeddings\\\\\\\":512,\\\\\\\"model_type\\\\\\\":\\\\\\\"bert\\\\\\\",\\\\\\\"num_attention_heads\\\\\\\":12,\\\\\\\"num_hidden_layers\\\\\\\":6,\\\\\\\"pad_token_id\\\\\\\":0,\\\\\\\"position_embedding_type\\\\\\\":\\\\\\\"absolute\\\\\\\",\\\\\\\"transformers_version\\\\\\\":\\\\\\\"4.8.2\\\\\\\",\\\\\\\"type_vocab_size\\\\\\\":2,\\\\\\\"use_cache\\\\\\\":true,\\\\\\\"vocab_size\\\\\\\":30522}\\\"\\n    },\\n    \\\"url\\\": \\\"https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/all-MiniLM-L6-v2\/1.0.1\/torch_script\/sentence-transformers_all-MiniLM-L6-v2-1.0.1-torch_script.zip\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Register a model hosted on a third-party platform\\n\\nTo register a model hosted on a third-party platform, you can either first create a standalone connector and provide the ID of that connector or specify an internal connector for the model. For more information, see [Creating connectors for third-party ML platforms]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/connectors\/).\\n\\n### Request fields\\n\\nThe following table lists the available request fields.\\n\\nField | Data type | Required\/Optional | Description\\n:---  | :--- | :--- \\n`name`| String | Required | The model name. |\\n`function_name` | String | Required | Set this parameter to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`.\\n`connector_id` | Optional | Required | The connector ID of a standalone connector for a model hosted on a third-party platform. For more information, see [Standalone connector]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/connectors\/#creating-a-standalone-connector). You must provide either `connector_id` or `connector`.\\n`connector` | Object | Required | Contains specifications for a connector for a model hosted on a third-party platform. For more information, see [Creating a connector for a specific model]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/connectors\/#creating-a-connector-for-a-specific-model). You must provide either `connector_id` or `connector`.\\n`description` | String | Optional| The model description. |\\n`model_group_id` | String | Optional | The model group ID of the model group to register this model to. \\n`is_enabled`| Boolean | Optional | Specifies whether the model is enabled. Disabling the model makes it unavailable for Predict API requests, regardless of the model's deployment status. Default is `true`.\\n`rate_limiter` | Object | Optional | Limits the number of times that any user can call the Predict API on the model. For more information, see [Rate limiting inference calls]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/integrating-ml-models\/#rate-limiting-inference-calls).\\n`guardrails`| Object | Optional | The guardrails for the model input. For more information, see [Guardrails](#the-guardrails-parameter).|\\n`interface`| Object | Optional | The interface for the model. For more information, see [Interface](#the-interface-parameter).|\\n\\n#### Example request: Externally hosted with a standalone connector\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n    \\\"name\\\": \\\"openAI-gpt-3.5-turbo\\\",\\n    \\\"function_name\\\": \\\"remote\\\",\\n    \\\"model_group_id\\\": \\\"1jriBYsBq7EKuKzZX131\\\",\\n    \\\"description\\\": \\\"test model\\\",\\n    \\\"connector_id\\\": \\\"a1eMb4kBJ1eYAeTMAljY\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\n#### Example request: Externally hosted with a connector specified as part of the model\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n    \\\"name\\\": \\\"openAI-GPT-3.5: internal connector\\\",\\n    \\\"function_name\\\": \\\"remote\\\",\\n    \\\"model_group_id\\\": \\\"lEFGL4kB4ubqQRzegPo2\\\",\\n    \\\"description\\\": \\\"test model\\\",\\n    \\\"connector\\\": {\\n        \\\"name\\\": \\\"OpenAI Connector\\\",\\n        \\\"description\\\": \\\"The connector to public OpenAI model service for GPT 3.5\\\",\\n        \\\"version\\\": 1,\\n        \\\"protocol\\\": \\\"http\\\",\\n        \\\"parameters\\\": {\\n            \\\"endpoint\\\": \\\"api.openai.com\\\",\\n            \\\"max_tokens\\\": 7,\\n            \\\"temperature\\\": 0,\\n            \\\"model\\\": \\\"text-davinci-003\\\"\\n        },\\n        \\\"credential\\\": {\\n            \\\"openAI_key\\\": \\\"...\\\"\\n        },\\n        \\\"actions\\\": [\\n            {\\n                \\\"action_type\\\": \\\"predict\\\",\\n                \\\"method\\\": \\\"POST\\\",\\n                \\\"url\\\": \\\"https:\/\/${parameters.endpoint}\/v1\/completions\\\",\\n                \\\"headers\\\": {\\n                    \\\"Authorization\\\": \\\"Bearer ${credential.openAI_key}\\\"\\n                },\\n                \\\"request_body\\\": \\\"{ \\\\\\\"model\\\\\\\": \\\\\\\"${parameters.model}\\\\\\\", \\\\\\\"prompt\\\\\\\": \\\\\\\"${parameters.prompt}\\\\\\\", \\\\\\\"max_tokens\\\\\\\": ${parameters.max_tokens}, \\\\\\\"temperature\\\\\\\": ${parameters.temperature} }\\\"\\n            }\\n        ]\\n    }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n#### Example response\\n\\nOpenSearch responds with the `task_id`, task `status`, and `model_id`:\\n\\n```json\\n{\\n  \\\"task_id\\\" : \\\"ew8I44MBhyWuIwnfvDIH\\\", \\n  \\\"status\\\" : \\\"CREATED\\\",\\n  \\\"model_id\\\": \\\"t8qvDY4BChVAiNVEuo8q\\\"\\n}\\n```\\n\\n### The `guardrails` parameter\\n\\nGuardrails are safety measures for large language models (LLMs). They provide a set of rules and boundaries that control how an LLM behaves and what kind of output it generates. \\n\\nTo register an externally hosted model with guardrails, provide the `guardrails` parameter, which supports the following fields. All fields are optional.\\n\\nField | Data type | Description\\n:---  | :--- | :---\\n`type` | String | The guardrail type. Valid values are [`local_regex`](#example-request-regex-and-stopword-validation) and [`model`](#example-request-guardrail-model-validation). Using `local_regex`, you can specify a regular expression or stop words. Using `model`, you can specify a guardrail model. For more information, see [Guardrails]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/guardrails\/). \\n`input_guardrail`| Object |  The guardrail for the model input. \\n`output_guardrail`| Object |  The guardrail for the model output. \\n`stop_words`| Object | The list of indexes containing stopwords used for model input\/output validation. If the model prompt\/response contains a stopword contained in any of the indexes, then the predict request on the model is rejected. \\n`index_name`| Object | The name of the index storing the stopwords. \\n`source_fields`| Object | The name of the field storing the stopwords. \\n`regex`| Object |  A regular expression used for input\/output validation. If the model prompt\/response matches the regular expression, then the predict request on the model is rejected. \\n`model_id`| String  | The guardrail model used to validate user input and LLM output. \\n`response_filter`| String | The dot path of the field containing the guardrail model response. \\n`response_validation_regex`| String | The regular expression used to validate the guardrail model response.     \\n\\n## Examples\\n\\nThe following examples configure an externally hosted model with guardrails.\\n\\n#### Example request: Regex and stopword validation\\n\\nThe following example uses a regular expression and a set of stopwords to validate the LLM response:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n  \\\"name\\\": \\\"openAI-gpt-3.5-turbo\\\",\\n  \\\"function_name\\\": \\\"remote\\\",\\n  \\\"model_group_id\\\": \\\"1jriBYsBq7EKuKzZX131\\\",\\n  \\\"description\\\": \\\"test model\\\",\\n  \\\"connector_id\\\": \\\"a1eMb4kBJ1eYAeTMAljY\\\",\\n  \\\"guardrails\\\": {\\n    \\\"type\\\": \\\"local_regex\\\",\\n    \\\"input_guardrail\\\": {\\n      \\\"stop_words\\\": [\\n        {\\n          \\\"index_name\\\": \\\"stop_words_input\\\",\\n          \\\"source_fields\\\": [\\\"title\\\"]\\n        }\\n      ],\\n      \\\"regex\\\": [\\\"regex1\\\", \\\"regex2\\\"]\\n    },\\n    \\\"output_guardrail\\\": {\\n      \\\"stop_words\\\": [\\n        {\\n          \\\"index_name\\\": \\\"stop_words_output\\\",\\n          \\\"source_fields\\\": [\\\"title\\\"]\\n        }\\n      ],\\n      \\\"regex\\\": [\\\"regex1\\\", \\\"regex2\\\"]\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nFor a complete example, see [Validating input\/output using stopwords and regex]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/guardrails\/#validating-inputoutput-using-stopwords-and-regex).\\n\\n#### Example request: Guardrail model validation\\n\\nThe following example uses a guardrail model to validate the LLM response:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register?deploy=true\\n{\\n    \\\"name\\\": \\\"Bedrock Claude V2 model with guardrails model\\\",\\n    \\\"function_name\\\": \\\"remote\\\",\\n    \\\"model_group_id\\\": \\\"ppSmpo8Bi-GZ0tf1i7cD\\\",\\n    \\\"description\\\": \\\"Bedrock Claude V2 model with guardrails model\\\",\\n    \\\"connector_id\\\": \\\"xnJjDZABNFJeYR3IPvTO\\\",\\n    \\\"guardrails\\\": {\\n        \\\"input_guardrail\\\": {\\n            \\\"model_id\\\": \\\"o3JaDZABNFJeYR3I2fRV\\\",\\n            \\\"response_validation_regex\\\": \\\"^\\\\\\\\s*\\\\\\\"[Aa]ccept\\\\\\\"\\\\\\\\s*$\\\"\\n        },\\n        \\\"output_guardrail\\\": {\\n            \\\"model_id\\\": \\\"o3JaDZABNFJeYR3I2fRV\\\",\\n            \\\"response_validation_regex\\\": \\\"^\\\\\\\\s*\\\\\\\"[Aa]ccept\\\\\\\"\\\\\\\\s*$\\\"\\n        },\\n        \\\"type\\\": \\\"model\\\"\\n    }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nFor a complete example, see [Validating input\/output using a guardrail model]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/guardrails\/#validating-inputoutput-using-a-guardrail-model).\\n\\n#### Example response\\n\\nOpenSearch responds with the `task_id`, task `status`, and `model_id`:\\n\\n```json\\n{\\n    \\\"task_id\\\": \\\"tsqvDY4BChVAiNVEuo8F\\\",\\n    \\\"status\\\": \\\"CREATED\\\",\\n    \\\"model_id\\\": \\\"t8qvDY4BChVAiNVEuo8q\\\"\\n}\\n```\\n\\n### The `interface` parameter\\n\\nThe model interface provides a highly flexible way to add arbitrary metadata annotations to all local deep learning models and externally hosted models in a JSON schema syntax. This annotation initiates a validation check on the input and output fields of the model during the model's invocation. The validation check ensures that the input and output fields are in the correct format both before and after the model performs inference.\\n\\nTo register a model with a model interface, provide the `interface` parameter, which supports the following fields.\\n\\nField | Data type | Description                         \\n:---  | :--- |:------------------------------------\\n`input`| Object | The JSON schema for the model input. |\\n`output`| Object | The JSON schema for the model output. |\\n\\nThe input and output fields are evaluated against the provided JSON schema. You do not need to provide both fields simultaneously.\\n\\n#### Connector model interfaces\\n\\nTo simplify your workflow, you can register an externally hosted model using a connector in one of the [connector blueprint]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/blueprints\/) formats. If you do so, a predefined model interface for this connector is generated automatically during model registration. The predefined model interface is generated based on the connector blueprint and the model's metadata, so you must strictly follow the blueprint when creating the connector in order to avoid errors.\\n\\nThe following connector blueprints currently support creating predefined model interfaces:\\n\\n- [Amazon Comprehend](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/amazon_comprehend_connector_blueprint.md)\\n- [Amazon Textract](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/amazon_textract_connector_blueprint.md) (Note that a predefined model interface is only available for the `DetectDocumentText` API; the `DetectEnities` API is not currently supported).\\n- [Amazon Bedrock AI21 Labs Jurassic](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/bedrock_connector_ai21labs_jurassic_blueprint.md)\\n- [Amazon Bedrock Anthropic Claude 3](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/bedrock_connector_anthropic_claude3_blueprint.md)\\n- [Amazon Bedrock Anthropic Claude](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/bedrock_connector_anthropic_claude_blueprint.md)\\n- [Amazon Bedrock Cohere Embed English v3](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/bedrock_connector_cohere_cohere.embed-english-v3_blueprint.md)\\n- [Amazon Bedrock Cohere Embed Multilingual v3](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/bedrock_connector_cohere_cohere.embed-multilingual-v3_blueprint.md)\\n- [Amazon Bedrock Titan Text Embeddings](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/bedrock_connector_titan_embedding_blueprint.md)\\n- [Amazon Bedrock Titan Multimodal Embeddings](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/docs\/remote_inference_blueprints\/bedrock_connector_titan_multimodal_embedding_blueprint.md)\\n\\nTo learn more about connector blueprints, see [Connector blueprints]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/remote-models\/blueprints\/).\\n\\n#### Example request: Externally hosted model with an interface\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n    \\\"name\\\": \\\"openAI-gpt-3.5-turbo\\\",\\n    \\\"function_name\\\": \\\"remote\\\",\\n    \\\"description\\\": \\\"test model\\\",\\n    \\\"connector_id\\\": \\\"A-j7K48BZzNMh1sWVdJu\\\",\\n    \\\"interface\\\": {\\n        \\\"input\\\": {\\n            \\\"properties\\\": {\\n                \\\"parameters\\\": {\\n                    \\\"properties\\\": {\\n                        \\\"messages\\\": {\\n                            \\\"type\\\": \\\"string\\\",\\n                            \\\"description\\\": \\\"This is a test description field\\\"\\n                        }\\n                    }\\n                }\\n            }\\n        },\\n        \\\"output\\\": {\\n            \\\"properties\\\": {\\n                \\\"inference_results\\\": {\\n                    \\\"type\\\": \\\"array\\\",\\n                    \\\"items\\\": {\\n                        \\\"type\\\": \\\"object\\\",\\n                        \\\"properties\\\": {\\n                            \\\"output\\\": {\\n                                \\\"type\\\": \\\"array\\\",\\n                                \\\"items\\\": {\\n                                    \\\"properties\\\": {\\n                                        \\\"name\\\": {\\n                                            \\\"type\\\": \\\"string\\\",\\n                                            \\\"description\\\": \\\"This is a test description field\\\"\\n                                        },\\n                                        \\\"dataAsMap\\\": {\\n                                            \\\"type\\\": \\\"object\\\",\\n                                            \\\"description\\\": \\\"This is a test description field\\\"\\n                                        }\\n                                    }\\n                                },\\n                                \\\"description\\\": \\\"This is a test description field\\\"\\n                            },\\n                            \\\"status_code\\\": {\\n                                \\\"type\\\": \\\"integer\\\",\\n                                \\\"description\\\": \\\"This is a test description field\\\"\\n                            }\\n                        }\\n                    },\\n                    \\\"description\\\": \\\"This is a test description field\\\"\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n#### Example response\\n\\nOpenSearch responds with the `task_id`, task `status`, and `model_id`:\\n\\n```json\\n{\\n    \\\"task_id\\\": \\\"tsqvDY4BChVAiNVEuo8F\\\",\\n    \\\"status\\\": \\\"CREATED\\\",\\n    \\\"model_id\\\": \\\"t8qvDY4BChVAiNVEuo8q\\\"\\n}\\n```\\n\\n## Check the status of model registration\\n\\nTo see the status of your model registration and retrieve the model ID created for the new model version, pass the `task_id` as a path parameter to the Tasks API:\\n\\n```json\\nGET \/_plugins\/_ml\/tasks\/<task_id>\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the model ID of the model version:\\n\\n```json\\n{\\n  \\\"model_id\\\": \\\"Qr1YbogBYOqeeqR7sI9L\\\",\\n  \\\"task_type\\\": \\\"DEPLOY_MODEL\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"state\\\": \\\"COMPLETED\\\",\\n  \\\"worker_node\\\": [\\n    \\\"N77RInqjTSq_UaLh1k0BUg\\\"\\n  ],\\n  \\\"create_time\\\": 1685478486057,\\n  \\\"last_update_time\\\": 1685478491090,\\n  \\\"is_async\\\": true\\n}\\n```\\n\\n\\n\"","generated_answer":"Yes, you can run a custom model in Opensearch. The report indicates that Opensearch supports running local text embedding, sparse encoding, cross-encoder, and question answering models. To use a custom model, you need to register it with Opensearch by providing details such as the model name, version, format, and configuration. Once registered, you can deploy the model and use it for search and other applications within Opensearch.","usage":"{\"inputTokens\": 12282, \"outputTokens\": 106, \"totalTokens\": 12388}","latency":2122,"reasoning":"None","score":null,"faithfulness":0.25,"answer_relevancy":0.101,"cost":0.003203}
{"query_text":"I have a model and some ML nodes, how do I boost it's performance?","groundtruth_answer":"<question_answer>\nTo boost the performance of your ML model, you can take advantage of GPU acceleration on your ML node. The report mentions that OpenSearch supports GPU instances from NVIDIA with CUDA 11.6 and AWS Inferentia. It provides instructions for preparing NVIDIA and AWS Inferentia ML nodes to enable GPU acceleration.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: GPU acceleration\\nparent: Using ML models within OpenSearch\\ngrand_parent: Integrating ML models\\nnav_order: 150\\n---\\n\\n\\n# GPU acceleration \\n\\nWhen running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training. \\n\\n## Supported GPUs\\n\\nCurrently, ML nodes support the following GPU instances:\\n\\n- [NVIDIA instances with CUDA 11.6](https:\/\/aws.amazon.com\/nvidia\/)\\n- [AWS Inferentia](https:\/\/aws.amazon.com\/machine-learning\/inferentia\/)\\n\\nIf you need GPU power, you can provision GPU instances through [Amazon Elastic Compute Cloud (Amazon EC2)](https:\/\/aws.amazon.com\/ec2\/). For more information on how to provision a GPU instance, see [Recommended GPU Instances](https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/gpu.html).\\n\\n## Supported images\\n\\nYou can use GPU acceleration with both [Docker images](https:\/\/gitlab.com\/nvidia\/container-images\/cuda\/blob\/master\/doc\/supported-tags.md) with CUDA 11.6 and [Amazon Machine Images (AMIs)](https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/AMIs.html).\\n\\n## PyTorch\\n\\nGPU-accelerated ML nodes require [PyTorch](https:\/\/pytorch.org\/docs\/stable\/index.html) 1.12.1 work with ML models.\\n\\n## Setting up a GPU-accelerated ML node\\n\\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts. \\n\\n### Preparing an NVIDIA ML node\\n\\nNVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the `nvidia-uvm` kernel inside the `\/dev` directory. To check for the kernel, enter `ls -al \/dev | grep nvidia-uvm`.\\n\\nIf the `nvidia-uvm` kernel does not exist, run `nvidia-uvm-init.sh`:\\n\\n```\\n#!\/bin\/bash\\n## Script to initialize nvidia device nodes.\\n## https:\/\/docs.nvidia.com\/cuda\/cuda-installation-guide-linux\/index.html#runfile-verifications\\n\/sbin\/modprobe nvidia\\nif [ \\\"$?\\\" -eq 0 ]; then\\n  # Count the number of NVIDIA controllers found.\\n  NVDEVS=`lspci | grep -i NVIDIA`\\n  N3D=`echo \\\"$NVDEVS\\\" | grep \\\"3D controller\\\" | wc -l`\\n  NVGA=`echo \\\"$NVDEVS\\\" | grep \\\"VGA compatible controller\\\" | wc -l`\\n  N=`expr $N3D + $NVGA - 1`\\n  for i in `seq 0 $N`; do\\n    mknod -m 666 \/dev\/nvidia$i c 195 $i\\n  done\\n  mknod -m 666 \/dev\/nvidiactl c 195 255\\nelse\\n  exit 1\\nfi\\n\/sbin\/modprobe nvidia-uvm\\nif [ \\\"$?\\\" -eq 0 ]; then\\n  # Find out the major device number used by the nvidia-uvm driver\\n  D=`grep nvidia-uvm \/proc\/devices | awk '{print $1}'`\\n  mknod -m 666 \/dev\/nvidia-uvm c $D 0\\n  mknod -m 666 \/dev\/nvidia-uvm-tools c $D 0\\nelse\\n  exit 1\\nfi\\n```\\n\\nAfter verifying that `nvidia-uvm` exists under `\/dev`, you can start OpenSearch inside your cluster. \\n\\n### Preparing AWS Inferentia ML node\\n\\nDepending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster. \\n\\nTo start, [download and install OpenSearch]({{site.url}}{{site.baseurl}}\/install-and-configure\/index\/) on your cluster.\\n\\nThen export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory `opensearch-2.5.0`, so `OPENSEARCH_HOME` = `opensearch-2.5.0`:\\n\\n```\\necho \\\"export OPENSEARCH_HOME=~\/opensearch-2.5.0\\\" | tee -a ~\/.bash_profile\\necho \\\"export PYTORCH_VERSION=1.12.1\\\" | tee -a ~\/.bash_profile\\nsource ~\/.bash_profile\\n```\\n\\nNext, create a shell script file called `prepare_torch_neuron.sh`. You can copy and customize one of the following examples based on your Linux operating system:\\n\\n- [Ubuntu 20.04](#ubuntu-2004)\\n- [Amazon Linux 2](#amazon-linux-2)\\n\\nAfter you've run the scripts, exit your current terminal and open a new terminal to start OpenSearch.\\n\\nGPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems.\\n{: .note}\\n\\n#### Ubuntu 20.04\\n\\n```\\n. \/etc\/os-release\\nsudo tee \/etc\/apt\/sources.list.d\/neuron.list > \/dev\/null <<EOF\\ndeb https:\/\/apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main\\nEOF\\nwget -qO - https:\/\/apt.repos.neuron.amazonaws.com\/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -\\n\\n# Update OS packages\\nsudo apt-get update -y\\n\\n################################################################################################################\\n# To install or update to Neuron versions 1.19.1 and newer from previous releases:\\n# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver\\n################################################################################################################\\n\\n# Install OS headers\\nsudo apt-get install linux-headers-$(uname -r) -y\\n\\n# Install Neuron Driver\\nsudo apt-get install aws-neuronx-dkms -y\\n\\n####################################################################################\\n# Warning: If Linux kernel is updated as a result of OS package update\\n#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot\\n####################################################################################\\n\\n# Install Neuron Tools\\nsudo apt-get install aws-neuronx-tools -y\\n\\n######################################################\\n#   Only for Ubuntu 20 - Install Python3.7\\nsudo add-apt-repository ppa:deadsnakes\/ppa\\nsudo apt-get install python3.7\\n######################################################\\n# Install Python venv and activate Python virtual environment to install    \\n# Neuron pip packages.\\ncd ~\\nsudo apt-get install -y python3.7-venv g++\\npython3.7 -m venv pytorch_venv\\nsource pytorch_venv\/bin\/activate\\npip install -U pip\\n\\n# Set pip repository to point to the Neuron repository\\npip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\\n\\n#Install Neuron PyTorch\\npip install torch-neuron torchvision\\n# If you need to trace the neuron model, install torch neuron with this command\\n# pip install torch-neuron neuron-cc[tensorflow] \\\"protobuf==3.20.1\\\" torchvision\\n\\n# If you need to trace neuron model, install the transformers for tracing the Huggingface model.\\n# pip install transformers\\n\\n# Copy torch neuron lib to OpenSearch\\nPYTORCH_NEURON_LIB_PATH=~\/pytorch_venv\/lib\/python3.7\/site-packages\/torch_neuron\/lib\/\\nmkdir -p $OPENSEARCH_HOME\/lib\/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH\/ $OPENSEARCH_HOME\/lib\/torch_neuron\\nexport PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\necho \\\"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\\" | tee -a ~\/.bash_profile\\n\\n# Increase JVm stack size to >=2MB\\necho \\\"-Xss2m\\\" | tee -a $OPENSEARCH_HOME\/config\/jvm.options\\n# Increase max file descriptors to 65535\\necho \\\"$(whoami) - nofile 65535\\\" | sudo tee -a \/etc\/security\/limits.conf\\n# max virtual memory areas vm.max_map_count to 262144\\nsudo sysctl -w vm.max_map_count=262144\\n```\\n\\n#### Amazon Linux 2\\n\\n```\\n# Configure Linux for Neuron repository updates\\nsudo tee \/etc\/yum.repos.d\/neuron.repo > \/dev\/null <<EOF\\n[neuron]\\nname=Neuron YUM Repository\\nbaseurl=https:\/\/yum.repos.neuron.amazonaws.com\\nenabled=1\\nmetadata_expire=0\\nEOF\\nsudo rpm --import https:\/\/yum.repos.neuron.amazonaws.com\/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\\n# Update OS packages\\nsudo yum update -y\\n################################################################################################################\\n# To install or update to Neuron versions 1.19.1 and newer from previous releases:\\n# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver\\n################################################################################################################\\n# Install OS headers\\nsudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y\\n# Install Neuron Driver\\n####################################################################################\\n# Warning: If Linux kernel is updated as a result of OS package update\\n#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot\\n####################################################################################\\nsudo yum install aws-neuronx-dkms -y\\n# Install Neuron Tools\\nsudo yum install aws-neuronx-tools -y\\n\\n# Install Python venv and activate Python virtual environment to install    \\n# Neuron pip packages.\\ncd ~\\nsudo yum install -y python3.7-venv gcc-c++\\npython3.7 -m venv pytorch_venv\\nsource pytorch_venv\/bin\/activate\\npip install -U pip\\n\\n# Set Pip repository  to point to the Neuron repository\\npip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\\n\\n# Install Neuron PyTorch\\npip install torch-neuron torchvision\\n# If you need to trace the neuron model, install torch neuron with this command\\n# pip install torch-neuron neuron-cc[tensorflow] \\\"protobuf<4\\\" torchvision\\n\\n# If you need to run the trace neuron model, install transformers for tracing Huggingface model.\\n# pip install transformers\\n\\n# Copy torch neuron lib to OpenSearch\\nPYTORCH_NEURON_LIB_PATH=~\/pytorch_venv\/lib\/python3.7\/site-packages\/torch_neuron\/lib\/\\nmkdir -p $OPENSEARCH_HOME\/lib\/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH\/ $OPENSEARCH_HOME\/lib\/torch_neuron\\nexport PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\necho \\\"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\\" | tee -a ~\/.bash_profile\\n# Increase JVm stack size to >=2MB\\necho \\\"-Xss2m\\\" | tee -a $OPENSEARCH_HOME\/config\/jvm.options\\n# Increase max file descriptors to 65535\\necho \\\"$(whoami) - nofile 65535\\\" | sudo tee -a \/etc\/security\/limits.conf\\n# max virtual memory areas vm.max_map_count to 262144\\nsudo sysctl -w vm.max_map_count=262144\\n```\\n\\nWhen the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch.\\n\\nOpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually.\\n\\n#### Prepare ML node manually\\n\\nIf the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually:\\n\\n1. Deploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see [Deploy on AWS accelerator instance](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/frameworks\/torch\/torch-neuron\/setup\/pytorch-install.html#deploy-on-aws-ml-accelerator-instance).\\n\\n2. Copy the Neuron library into OpenSearch. The following command uses a directory named `opensearch-2.5.0`:\\n\\n   ```\\n   OPENSEARCH_HOME=~\/opensearch-2.5.0\\n   ```\\n\\n3. Set the `PYTORCH_EXTRA_LIBRARY_PATH` path. In this example, we create a `pytorch` virtual environment in the OPENSEARCH_HOME folder:\\n\\n   ```\\n   PYTORCH_NEURON_LIB_PATH=~\/pytorch_venv\/lib\/python3.7\/site-packages\/torch_neuron\/lib\/\\n\\n\\n   mkdir -p $OPENSEARCH_HOME\/lib\/torch_neuron; cp -r  $PYTORCH_NEURON_LIB_PATH\/ $OPENSEARCH_HOME\/lib\/torch_neuron\\n   export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME\/lib\/torch_neuron\/lib\/libtorchneuron.so\\n  ```\\n\\n4. (Optional) To monitor the GPU usage of your accelerator instance, install [Neuron tools](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/tools\/index.html), which allows models to be used inside your instance:\\n\\n   ```\\n   # Install Neuron Tools\\n   sudo apt-get install aws-neuronx-tools -y\\n   ```\\n\\n   ```\\n   # Add Neuron tools your PATH\\n   export PATH=\/opt\/aws\/neuron\/bin:$PATH\\n   ```\\n  \\n   ```\\n   # Test Neuron tools\\n   neuron-top\\n   ```\\n\\n\\n5. To make sure you have enough memory to upload a model, increase the JVM stack size to `>+2MB`:\\n\\n   ```\\n   echo \\\"-Xss2m\\\" | sudo tee -a $OPENSEARCH_HOME\/config\/jvm.options\\n   ```\\n\\n6. Start OpenSearch. \\n\\n## Troubleshooting\\n\\nDue to the amount of data required to work with ML models, you might encounter the following `max file descriptors` or `vm.max_map_count` errors when trying to run OpenSearch in a your cluster: \\n\\n```\\n[1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535]\\n[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\\n```\\n\\nTo troubleshoot the max file descriptors error, run the following command:\\n\\n```\\necho \\\"$(whoami) - nofile 65535\\\" | sudo tee -a \/etc\/security\/limits.conf\\n```\\n\\nTo fix the `vm.max_map_count` error, run this command to increase the count to `262114`:\\n\\n```\\nsudo sysctl -w vm.max_map_count=262144\\n```\\n\\n## Next steps\\n\\nIf you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see [Compiling and Deploying HuggingFace Pretrained BERT](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/src\/examples\/pytorch\/bert_tutorial\/tutorial_pretrained_bert.html).\\n\\n\\n\\n---\\nlayout: default\\ntitle: Custom models\\nparent: Using ML models within OpenSearch\\ngrand_parent: Integrating ML models\\nnav_order: 120\\n---\\n\\n# Custom local models\\n**Introduced 2.9**\\n{: .label .label-purple }\\n\\nTo use a custom model locally, you can upload it to the OpenSearch cluster.\\n\\n## Model support\\n\\nAs of OpenSearch 2.6, OpenSearch supports local text embedding models.\\n\\nAs of OpenSearch 2.11, OpenSearch supports local sparse encoding models.\\n\\nAs of OpenSearch 2.12, OpenSearch supports local cross-encoder models.\\n\\nAs of OpenSearch 2.13, OpenSearch supports local question answering models.\\n\\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\\n{: .important}\\n\\n## Preparing a model\\n\\nFor all the models, you must provide a tokenizer JSON file within the model zip file.\\n\\nFor sparse encoding models, make sure your output format is `{\\\"output\\\":<sparse_vector>}` so that ML Commons can post-process the sparse vector.\\n\\nIf you fine-tune a sparse model on your own dataset, you may also want to use your own sparse tokenizer model. It is preferable to provide your own [IDF](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf) JSON file in the tokenizer model zip file because this increases query performance when you use the tokenizer model in the query. Alternatively, you can use an OpenSearch-provided generic [IDF from MSMARCO](https:\/\/artifacts.opensearch.org\/models\/ml-models\/amazon\/neural-sparse\/opensearch-neural-sparse-tokenizer-v1\/1.0.0\/torch_script\/opensearch-neural-sparse-tokenizer-v1-1.0.0.zip). If the IDF file is not provided, the default weight of each token is set to 1, which may influence sparse neural search performance.  \\n\\n### Model format\\n\\nTo use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the [TorchScript](https:\/\/pytorch.org\/docs\/stable\/jit.html) and [ONNX](https:\/\/onnx.ai\/) formats.\\n\\nYou must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript [model file](https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/2.x\/ml-algorithms\/src\/test\/resources\/org\/opensearch\/ml\/engine\/algorithms\/text_embedding\/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip).\\n\\nAdditionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:\\n\\n```bash\\nshasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip\\n```\\n\\n### Model size\\n\\nMost deep learning models are more than 100 MB, making it difficult to fit them into a single document. OpenSearch splits the model file into smaller chunks to be stored in a model index. When allocating ML or data nodes for your OpenSearch cluster, make sure you correctly size your ML nodes so that you have enough memory when making ML inferences.\\n\\n## Prerequisites \\n\\nTo upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from [Hugging Face](https:\/\/huggingface.co\/), or train a new model in accordance with your needs.\\n\\n### Cluster settings\\n\\nThis example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. \\n\\nOn clusters with dedicated ML nodes, specify `\\\"only_run_on_ml_node\\\": \\\"true\\\"` for improved performance. For more information, see [ML Commons cluster settings]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/cluster-settings\/).\\n\\nTo ensure that this basic local setup works, specify the following cluster settings:\\n\\n```json\\nPUT _cluster\/settings\\n{\\n  \\\"persistent\\\": {\\n    \\\"plugins\\\": {\\n      \\\"ml_commons\\\": {\\n        \\\"allow_registering_model_via_url\\\": \\\"true\\\",\\n        \\\"only_run_on_ml_node\\\": \\\"false\\\",\\n        \\\"model_access_control_enabled\\\": \\\"true\\\",\\n        \\\"native_memory_threshold\\\": \\\"99\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Step 1: Register a model group\\n\\nTo register a model, you have the following options:\\n\\n- You can use `model_group_id` to register a model version to an existing model group.\\n- If you do not use `model_group_id`, ML Commons creates a model with a new model group.\\n\\nTo register a model group, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/model_groups\/_register\\n{\\n  \\\"name\\\": \\\"local_model_group\\\",\\n  \\\"description\\\": \\\"A model group for local models\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the model group ID that you'll use to register a model to this model group:\\n\\n```json\\n{\\n \\\"model_group_id\\\": \\\"wlcnb4kBJ1eYAeTMHlV6\\\",\\n \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nTo learn more about model groups, see [Model access control]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/model-access-control\/).\\n\\n## Step 2: Register a local model\\n\\nTo register a local model to the model group created in step 1, send a Register Model API request. For descriptions of Register Model API parameters, see [Register a model]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/model-apis\/register-model\/). \\n\\nThe `function_name` corresponds to the model type. For text embedding models, set this parameter to `TEXT_EMBEDDING`. For sparse encoding models, set this parameter to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`. For cross-encoder models, set this parameter to `TEXT_SIMILARITY`. For question answering models, set this parameter to `QUESTION_ANSWERING`. In this example, set `function_name` to `TEXT_EMBEDDING` because you're registering a text embedding model. \\n\\nProvide the model group ID from step 1 and send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n  \\\"name\\\": \\\"huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\\\",\\n  \\\"version\\\": \\\"1.0.1\\\",\\n  \\\"model_group_id\\\": \\\"wlcnb4kBJ1eYAeTMHlV6\\\",\\n  \\\"description\\\": \\\"This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"model_format\\\": \\\"TORCH_SCRIPT\\\",\\n  \\\"model_content_size_in_bytes\\\": 266352827,\\n  \\\"model_content_hash_value\\\": \\\"acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413\\\",\\n  \\\"model_config\\\": {\\n    \\\"model_type\\\": \\\"distilbert\\\",\\n    \\\"embedding_dimension\\\": 768,\\n    \\\"framework_type\\\": \\\"sentence_transformers\\\",\\n    \\\"all_config\\\": \\\"{\\\\\\\"_name_or_path\\\\\\\":\\\\\\\"old_models\/msmarco-distilbert-base-tas-b\/0_Transformer\\\\\\\",\\\\\\\"activation\\\\\\\":\\\\\\\"gelu\\\\\\\",\\\\\\\"architectures\\\\\\\":[\\\\\\\"DistilBertModel\\\\\\\"],\\\\\\\"attention_dropout\\\\\\\":0.1,\\\\\\\"dim\\\\\\\":768,\\\\\\\"dropout\\\\\\\":0.1,\\\\\\\"hidden_dim\\\\\\\":3072,\\\\\\\"initializer_range\\\\\\\":0.02,\\\\\\\"max_position_embeddings\\\\\\\":512,\\\\\\\"model_type\\\\\\\":\\\\\\\"distilbert\\\\\\\",\\\\\\\"n_heads\\\\\\\":12,\\\\\\\"n_layers\\\\\\\":6,\\\\\\\"pad_token_id\\\\\\\":0,\\\\\\\"qa_dropout\\\\\\\":0.1,\\\\\\\"seq_classif_dropout\\\\\\\":0.2,\\\\\\\"sinusoidal_pos_embds\\\\\\\":false,\\\\\\\"tie_weights_\\\\\\\":true,\\\\\\\"transformers_version\\\\\\\":\\\\\\\"4.7.0\\\\\\\",\\\\\\\"vocab_size\\\\\\\":30522}\\\"\\n  },\\n  \\\"created_time\\\": 1676073973126,\\n  \\\"url\\\": \\\"https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.1\/torch_script\/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nNote that in OpenSearch Dashboards, wrapping the `all_config` field contents in triple quotes (`\\\"\\\"\\\"`) automatically escapes quotation marks within the field and provides better readability:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n  \\\"name\\\": \\\"huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\\\",\\n  \\\"version\\\": \\\"1.0.1\\\",\\n  \\\"model_group_id\\\": \\\"wlcnb4kBJ1eYAeTMHlV6\\\",\\n  \\\"description\\\": \\\"This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"model_format\\\": \\\"TORCH_SCRIPT\\\",\\n  \\\"model_content_size_in_bytes\\\": 266352827,\\n  \\\"model_content_hash_value\\\": \\\"acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413\\\",\\n  \\\"model_config\\\": {\\n    \\\"model_type\\\": \\\"distilbert\\\",\\n    \\\"embedding_dimension\\\": 768,\\n    \\\"framework_type\\\": \\\"sentence_transformers\\\",\\n    \\\"all_config\\\": \\\"\\\"\\\"{\\\"_name_or_path\\\":\\\"old_models\/msmarco-distilbert-base-tas-b\/0_Transformer\\\",\\\"activation\\\":\\\"gelu\\\",\\\"architectures\\\":[\\\"DistilBertModel\\\"],\\\"attention_dropout\\\":0.1,\\\"dim\\\":768,\\\"dropout\\\":0.1,\\\"hidden_dim\\\":3072,\\\"initializer_range\\\":0.02,\\\"max_position_embeddings\\\":512,\\\"model_type\\\":\\\"distilbert\\\",\\\"n_heads\\\":12,\\\"n_layers\\\":6,\\\"pad_token_id\\\":0,\\\"qa_dropout\\\":0.1,\\\"seq_classif_dropout\\\":0.2,\\\"sinusoidal_pos_embds\\\":false,\\\"tie_weights_\\\":true,\\\"transformers_version\\\":\\\"4.7.0\\\",\\\"vocab_size\\\":30522}\\\"\\\"\\\"\\n  },\\n  \\\"created_time\\\": 1676073973126,\\n  \\\"url\\\": \\\"https:\/\/artifacts.opensearch.org\/models\/ml-models\/huggingface\/sentence-transformers\/msmarco-distilbert-base-tas-b\/1.0.1\/torch_script\/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip\\\"\\n}\\n```\\n{% include copy.html %}\\n\\nOpenSearch returns the task ID of the register operation:\\n\\n```json\\n{\\n  \\\"task_id\\\": \\\"cVeMb4kBJ1eYAeTMFFgj\\\",\\n  \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nTo check the status of the operation, provide the task ID to the [Get task]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/tasks-apis\/get-task\/):\\n\\n```bash\\nGET \/_plugins\/_ml\/tasks\/cVeMb4kBJ1eYAeTMFFgj\\n```\\n{% include copy-curl.html %}\\n\\nWhen the operation is complete, the state changes to `COMPLETED`:\\n\\n```json\\n{\\n  \\\"model_id\\\": \\\"cleMb4kBJ1eYAeTMFFg4\\\",\\n  \\\"task_type\\\": \\\"REGISTER_MODEL\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"state\\\": \\\"COMPLETED\\\",\\n  \\\"worker_node\\\": [\\n    \\\"XPcXLV7RQoi5m8NI_jEOVQ\\\"\\n  ],\\n  \\\"create_time\\\": 1689793598499,\\n  \\\"last_update_time\\\": 1689793598530,\\n  \\\"is_async\\\": false\\n}\\n```\\n\\nTake note of the returned `model_id` because you\\u2019ll need it to deploy the model.\\n\\n## Step 3: Deploy the model\\n\\nThe deploy operation reads the model's chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into and longer it takes for the model to load into memory.\\n\\nTo deploy the registered model, provide its model ID from step 3 in the following request:\\n\\n```bash\\nPOST \/_plugins\/_ml\/models\/cleMb4kBJ1eYAeTMFFg4\/_deploy\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the task ID that you can use to check the status of the deploy operation:\\n\\n```json\\n{\\n  \\\"task_id\\\": \\\"vVePb4kBJ1eYAeTM7ljG\\\",\\n  \\\"status\\\": \\\"CREATED\\\"\\n}\\n```\\n\\nAs in the previous step, check the status of the operation by calling the Tasks API:\\n\\n```bash\\nGET \/_plugins\/_ml\/tasks\/vVePb4kBJ1eYAeTM7ljG\\n```\\n{% include copy-curl.html %}\\n\\nWhen the operation is complete, the state changes to `COMPLETED`:\\n\\n```json\\n{\\n  \\\"model_id\\\": \\\"cleMb4kBJ1eYAeTMFFg4\\\",\\n  \\\"task_type\\\": \\\"DEPLOY_MODEL\\\",\\n  \\\"function_name\\\": \\\"TEXT_EMBEDDING\\\",\\n  \\\"state\\\": \\\"COMPLETED\\\",\\n  \\\"worker_node\\\": [\\n    \\\"n-72khvBTBi3bnIIR8FTTw\\\"\\n  ],\\n  \\\"create_time\\\": 1689793851077,\\n  \\\"last_update_time\\\": 1689793851101,\\n  \\\"is_async\\\": true\\n}\\n```\\n\\nIf a cluster or node is restarted, then you need to redeploy the model. To learn how to set up automatic redeployment, see [Enable auto redeploy]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/cluster-settings\/#enable-auto-redeploy).\\n{: .tip} \\n\\n## Step 4 (Optional): Test the model\\n\\nUse the [Predict API]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/api\/train-predict\/predict\/) to test the model.\\n\\nFor a text embedding model, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/text_embedding\/cleMb4kBJ1eYAeTMFFg4\\n{\\n  \\\"text_docs\\\":[ \\\"today is sunny\\\"],\\n  \\\"return_number\\\": true,\\n  \\\"target_response\\\": [\\\"sentence_embedding\\\"]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains text embeddings for the provided sentence:\\n\\n```json\\n{\\n  \\\"inference_results\\\" : [\\n    {\\n      \\\"output\\\" : [\\n        {\\n          \\\"name\\\" : \\\"sentence_embedding\\\",\\n          \\\"data_type\\\" : \\\"FLOAT32\\\",\\n          \\\"shape\\\" : [\\n            768\\n          ],\\n          \\\"data\\\" : [\\n            0.25517133,\\n            -0.28009856,\\n            0.48519906,\\n            ...\\n          ]\\n        }\\n      ]\\n    }\\n  ]\\n}\\n```\\n\\nFor a sparse encoding model, send the following request:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/sparse_encoding\/cleMb4kBJ1eYAeTMFFg4\\n{\\n  \\\"text_docs\\\":[ \\\"today is sunny\\\"]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the tokens and weights:\\n\\n```json\\n{\\n  \\\"inference_results\\\": [\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"name\\\": \\\"output\\\",\\n          \\\"dataAsMap\\\": {\\n            \\\"response\\\": [\\n              {\\n                \\\"saturday\\\": 0.48336542,\\n                \\\"week\\\": 0.1034762,\\n                \\\"mood\\\": 0.09698499,\\n                \\\"sunshine\\\": 0.5738209,\\n                \\\"bright\\\": 0.1756877,\\n                ...\\n              }\\n          }\\n        }\\n    }\\n}\\n```\\n\\n## Step 5: Use the model for search\\n\\nTo learn how to use the model for vector search, see [Using an ML model for neural search]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-search\/#using-an-ml-model-for-neural-search).\\n\\n## Question answering models\\n\\nA question answering model extracts the answer to a question from a given context. ML Commons supports context in `text` format.\\n\\nTo register a question answering model, send a request in the following format. Specify the `function_name` as `QUESTION_ANSWERING`:\\n\\n```json\\nPOST \/_plugins\/_ml\/models\/_register\\n{\\n    \\\"name\\\": \\\"question_answering\\\",\\n    \\\"version\\\": \\\"1.0.0\\\",\\n    \\\"function_name\\\": \\\"QUESTION_ANSWERING\\\",\\n    \\\"description\\\": \\\"test model\\\",\\n    \\\"model_format\\\": \\\"TORCH_SCRIPT\\\",\\n    \\\"model_group_id\\\": \\\"lN4AP40BKolAMNtR4KJ5\\\",\\n    \\\"model_content_hash_value\\\": \\\"e837c8fc05fd58a6e2e8383b319257f9c3859dfb3edc89b26badfaf8a4405ff6\\\",\\n    \\\"model_config\\\": { \\n        \\\"model_type\\\": \\\"bert\\\",\\n        \\\"framework_type\\\": \\\"huggingface_transformers\\\"\\n    },\\n    \\\"url\\\": \\\"https:\/\/github.com\/opensearch-project\/ml-commons\/blob\/main\/ml-algorithms\/src\/test\/resources\/org\/opensearch\/ml\/engine\/algorithms\/question_answering\/question_answering_pt.zip?raw=true\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThen send a request to deploy the model:\\n\\n```json\\nPOST _plugins\/_ml\/models\/<model_id>\/_deploy\\n```\\n{% include copy-curl.html %}\\n\\nTo test a question answering model, send the following request. It requires a `question` and the relevant `context` from which the answer will be generated:\\n\\n```json\\nPOST \/_plugins\/_ml\/_predict\/question_answering\/<model_id>\\n{\\n  \\\"question\\\": \\\"Where do I live?\\\"\\n  \\\"context\\\": \\\"My name is John. I live in New York\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response provides the answer based on the context:\\n\\n```json\\n{\\n  \\\"inference_results\\\": [\\n    {\\n      \\\"output\\\": [\\n        {\\n          \\\"result\\\": \\\"New York\\\"\\n        }\\n    }\\n}\\n```\\n\\n\\n---\\nlayout: default\\ntitle: Using ML models within OpenSearch\\nparent: Integrating ML models\\nhas_children: true\\nnav_order: 50\\nredirect_from:\\n   - \/ml-commons-plugin\/model-serving-framework\/\\n   - \/ml-commons-plugin\/ml-framework\/\\n---\\n\\n# Using ML models within OpenSearch\\n**Introduced 2.9**\\n{: .label .label-purple }\\n\\nTo integrate machine learning (ML) models into your OpenSearch cluster, you can upload and serve them locally. Choose one of the following options:\\n\\n- **Pretrained models provided by OpenSearch**: To learn more, see [OpenSearch-provided pretrained models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/pretrained-models\/). For a list of supported models, see [Supported pretrained models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/pretrained-models\/#supported-pretrained-models). \\n\\n- **Custom models** such as PyTorch deep learning models: To learn more, see [Custom models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/custom-local-models\/).\\n\\nRunning local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.\\n{: .important}\\n\\n## GPU acceleration\\n\\nFor better performance, you can take advantage of GPU acceleration on your ML node. For more information, see [GPU acceleration]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/gpu-acceleration\/).\\n\\n\\n\"","generated_answer":"To boost the performance of your model and ML nodes, you can try the following:\n\n1. Use GPU acceleration on your ML nodes. GPU acceleration can speed up the model upload and training process. Make sure to provision GPU instances and set up the necessary drivers and libraries.\n\n2. Increase the JVM stack size and max file descriptors on your OpenSearch nodes to handle the large amount of data required for ML models.\n\n3. Ensure that your ML nodes have enough memory to load and run the ML models. Increase the memory allocation as needed.\n\n4. Consider using pretrained models provided by OpenSearch, which are optimized for performance, instead of custom models.","usage":"{\"inputTokens\": 9100, \"outputTokens\": 155, \"totalTokens\": 9255}","latency":2581,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.059,"cost":0.00246875}
{"query_text":"Can you show me an example of how to use lat\/long coordinates?","groundtruth_answer":"<question_answer>\nThe report provides several examples of how to use latitude and longitude coordinates to index a geopoint field:\n\n- An object with \"lat\" and \"lon\" properties: { \"lat\": 40.71, \"lon\": 74.00 }\n- A string with latitude and longitude separated by a comma: \"40.71,74.00\"\n- A geohash string: \"txhxegj0uyp3\"\n- An array with longitude first, then latitude: [74.00, 40.71]\n- A Well-Known Text string: \"POINT (74.00 40.71)\"\n- A GeoJSON object with \"type\": \"Point\" and \"coordinates\" array: { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] }\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Geopoint\\nnav_order: 56\\nhas_children: false\\nparent: Geographic field types\\ngrand_parent: Supported field types\\nredirect_from:\\n  - \/opensearch\/supported-field-types\/geo-point\/\\n  - \/field-types\/geo-point\/\\n---\\n\\n# Geopoint field type\\n\\nA geopoint field type contains a geographic point specified by latitude and longitude. \\n\\n## Example\\n\\nCreate a mapping with a geopoint field type:\\n\\n```json\\nPUT testindex1\\n{\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"point\\\": {\\n        \\\"type\\\": \\\"geo_point\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Formats\\n\\nGeopoints can be indexed in the following formats:\\n\\n- An object with a latitude and longitude\\n\\n```json\\nPUT testindex1\/_doc\/1\\n{\\n  \\\"point\\\": { \\n    \\\"lat\\\": 40.71,\\n    \\\"lon\\\": 74.00\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n- A string in the \\\"`latitude`,`longitude`\\\" format\\n\\n```json\\nPUT testindex1\/_doc\/2\\n{\\n  \\\"point\\\": \\\"40.71,74.00\\\" \\n}\\n```\\n{% include copy-curl.html %}\\n\\n- A geohash\\n\\n```json\\nPUT testindex1\/_doc\/3\\n{\\n  \\\"point\\\": \\\"txhxegj0uyp3\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\n- An array in the [`longitude`, `latitude`] format\\n\\n```json\\nPUT testindex1\/_doc\/4\\n{\\n  \\\"point\\\": [74.00, 40.71] \\n}\\n```\\n{% include copy-curl.html %}\\n\\n- A [Well-Known Text](https:\/\/docs.opengeospatial.org\/is\/12-063r5\/12-063r5.html) POINT in the \\\"POINT(`longitude` `latitude`)\\\" format\\n\\n```json\\nPUT testindex1\/_doc\/5\\n{\\n  \\\"point\\\": \\\"POINT (74.00 40.71)\\\"\\n}\\n```\\n{% include copy-curl.html %}\\n\\n- GeoJSON format, where the `coordinates` are in the [`longitude`, `latitude`] format\\n\\n```json\\nPUT testindex1\/_doc\/6\\n{\\n  \\\"point\\\": {\\n    \\\"type\\\": \\\"Point\\\",\\n    \\\"coordinates\\\": [74.00, 40.71]\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Parameters\\n\\nThe following table lists the parameters accepted by geopoint field types. All parameters are optional.\\n\\nParameter | Description \\n:--- | :--- \\n`ignore_malformed` | A Boolean value that specifies to ignore malformed values and not to throw an exception. Valid values for latitude are [-90, 90]. Valid values for longitude are [-180, 180]. Default is `false`.\\n`ignore_z_value` | Specific to points with three coordinates. If `ignore_z_value` is `true`, the third coordinate is not indexed but is still stored in the _source field. If `ignore_z_value` is `false`, an exception is thrown.\\n[`null_value`]({{site.url}}{{site.baseurl}}\/opensearch\/supported-field-types\/index#null-value) | A  value to be used in place of `null`. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is `null`. Default is `null`.\\n\\n\"","generated_answer":"Here is an example of how to use latitude and longitude coordinates:\n\nYou can index a geopoint field in the following formats:\n\n- As an object with \"lat\" and \"lon\" properties:\n{\n  \"point\": {\n    \"lat\": 40.71,\n    \"lon\": 74.00\n  }\n}\n\n- As a string in the \"latitude,longitude\" format:\n{\n  \"point\": \"40.71,74.00\"\n}\n\n- As an array in the [longitude, latitude] format:\n{\n  \"point\": [74.00, 40.71]\n}\n\n- As a Well-Known Text (WKT) POINT in the \"POINT(longitude latitude)\" format:\n{\n  \"point\": \"POINT (74.00 40.71)\"\n}\n\n- As a GeoJSON object with \"type\" and \"coordinates\" properties:\n{\n  \"point\": {\n    \"type\": \"Point\",\n    \"coordinates\": [74.00, 40.71]\n  }\n}","usage":"{\"inputTokens\": 1039, \"outputTokens\": 255, \"totalTokens\": 1294}","latency":2242,"reasoning":"None","score":null,"faithfulness":1.0,"answer_relevancy":0.084,"cost":0.0005785}
{"query_text":"How do I use vector search?","groundtruth_answer":"<question_answer>\nThere is not enough information available to answer this question. The provided context describes what vector search is and how to create vector indexes, but does not provide instructions on how to use vector search.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Vector search\\nnav_order: 22\\nhas_children: false\\nhas_toc: false\\n---\\n\\n# Vector search\\n\\nOpenSearch is a comprehensive search platform that supports a variety of data types, including vectors. OpenSearch vector database functionality is seamlessly integrated with its generic database function.\\n\\nIn OpenSearch, you can generate vector embeddings, store those embeddings in an index, and use them for vector search. Choose one of the following options:\\n\\n- Generate embeddings using a library of your choice before ingesting them into OpenSearch. Once you ingest vectors into an index, you can perform a vector similarity search on the vector space. For more information, see [Working with embeddings generated outside of OpenSearch](#working-with-embeddings-generated-outside-of-opensearch). \\n- Automatically generate embeddings within OpenSearch. To use embeddings for semantic search, the ingested text (the corpus) and the query need to be embedded using the same model. [Neural search]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-search\/) packages this functionality, eliminating the need to manage the internal details. For more information, see [Generating vector embeddings within OpenSearch](#generating-vector-embeddings-in-opensearch).\\n\\n## Working with embeddings generated outside of OpenSearch\\n\\nAfter you generate vector embeddings, upload them to an OpenSearch index and search the index using vector search. For a complete example, see [Example](#example).\\n\\n### k-NN index\\n\\nTo build a vector database and use vector search, you must specify your index as a [k-NN index]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index\/) when creating it by setting `index.knn` to `true`:\\n\\n```json\\nPUT test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 1024,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"engine\\\": \\\"nmslib\\\",\\n          \\\"parameters\\\": {\\n            \\\"ef_construction\\\": 128,\\n            \\\"m\\\": 24\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### k-NN vector\\n\\nYou must designate the field that will store vectors as a [`knn_vector`]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector\/) field type. OpenSearch supports vectors of up to 16,000 dimensions, each of which is represented as a 32-bit or 16-bit float. \\n\\nTo save storage space, you can use `byte` or `binary` vectors. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#lucene-byte-vector) and [Binary k-NN vectors]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#binary-k-nn-vectors).\\n\\n### k-NN vector search\\n\\nVector search finds the vectors in your database that are most similar to the query vector. OpenSearch supports the following search methods:\\n\\n- [Approximate search](#approximate-search) (approximate k-NN, or ANN): Returns approximate nearest neighbors to the query vector. Usually, approximate search algorithms sacrifice indexing speed and search accuracy in exchange for performance benefits such as lower latency, smaller memory footprints, and more scalable search. For most use cases, approximate search is the best option.\\n\\n- Exact search (exact k-NN): A brute-force, exact k-NN search of vector fields. OpenSearch supports the following types of exact search: \\n  - [Exact k-NN with scoring script]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-score-script\/): Using the k-NN scoring script, you can apply a filter to an index before executing the nearest neighbor search. \\n  - [Painless extensions]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/painless-functions\/): Adds the distance functions as Painless extensions that you can use in more complex combinations. You can use this method to perform a brute-force, exact k-NN search of an index, which also supports pre-filtering. \\n\\n### Approximate search\\n\\nOpenSearch supports several algorithms for approximate vector search, each with its own advantages. For complete documentation, see [Approximate search]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/). For more information about the search methods and engines, see [Method definitions]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index\/#method-definitions). For method recommendations, see [Choosing the right method]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index\/#choosing-the-right-method).\\n\\nTo use approximate vector search, specify one of the following search methods (algorithms) in the `method` parameter:\\n\\n- Hierarchical Navigable Small World (HNSW)\\n- Inverted File System (IVF)\\n\\nAdditionally, specify the engine (library) that implements this method in the `engine` parameter:\\n\\n- [Non-Metric Space Library (NMSLIB)](https:\/\/github.com\/nmslib\/nmslib)\\n- [Facebook AI Similarity Search (Faiss)](https:\/\/github.com\/facebookresearch\/faiss)\\n- Lucene\\n\\nThe following table lists the combinations of search methods and libraries supported by the k-NN engine for approximate vector search.\\n\\nMethod | Engine\\n:--- | :---\\nHNSW | NMSLIB, Faiss, Lucene\\nIVF | Faiss \\n\\n### Engine recommendations\\n\\nIn general, select NMSLIB or Faiss for large-scale use cases. Lucene is a good option for smaller deployments and offers benefits like smart filtering, where the optimal filtering strategy\\u2014pre-filtering, post-filtering, or exact k-NN\\u2014is automatically applied depending on the situation. The following table summarizes the differences between each option.\\n\\n| |  NMSLIB\/HNSW |  Faiss\/HNSW |  Faiss\/IVF |  Lucene\/HNSW |\\n|:---|:---|:---|:---|:---|\\n|  Max dimensions |  16,000  |  16,000 |  16,000 |  16,000 |\\n|  Filter |  Post-filter |  Post-filter |  Post-filter |  Filter during search |\\n|  Training required |  No |  No |  Yes |  No |\\n|  Similarity metrics |  `l2`, `innerproduct`, `cosinesimil`, `l1`, `linf`  |  `l2`, `innerproduct` |  `l2`, `innerproduct` |  `l2`, `cosinesimil` |\\n|  Number of vectors   |  Tens of billions |  Tens of billions |  Tens of billions |  Less than 10 million |\\n|  Indexing latency |  Low |  Low  |  Lowest  |  Low  |\\n|  Query latency and quality  |  Low latency and high quality |  Low latency and high quality  |  Low latency and low quality  |  High latency and high quality  |\\n|  Vector compression  |  Flat |  Flat <br>Product quantization |  Flat <br>Product quantization |  Flat  |\\n|  Memory consumption |  High  |  High <br> Low with PQ |  Medium <br> Low with PQ |  High  |\\n\\n### Example\\n\\nIn this example, you'll create a k-NN index, add data to the index, and search the data.\\n\\n#### Step 1: Create a k-NN index\\n\\nFirst, create an index that will store sample hotel data. Set `index.knn` to `true` and specify the `location` field as a `knn_vector`:\\n\\n```json\\nPUT \/hotels-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100,\\n      \\\"number_of_shards\\\": 1,\\n      \\\"number_of_replicas\\\": 0\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"location\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 2,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"parameters\\\": {\\n            \\\"ef_construction\\\": 100,\\n            \\\"m\\\": 16\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n#### Step 2: Add data to your index\\n\\nNext, add data to your index. Each document represents a hotel. The `location` field in each document contains a vector specifying the hotel's location:\\n\\n```json\\nPOST \/_bulk\\n{ \\\"index\\\": { \\\"_index\\\": \\\"hotels-index\\\", \\\"_id\\\": \\\"1\\\" } }\\n{ \\\"location\\\": [5.2, 4.4] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"hotels-index\\\", \\\"_id\\\": \\\"2\\\" } }\\n{ \\\"location\\\": [5.2, 3.9] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"hotels-index\\\", \\\"_id\\\": \\\"3\\\" } }\\n{ \\\"location\\\": [4.9, 3.4] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"hotels-index\\\", \\\"_id\\\": \\\"4\\\" } }\\n{ \\\"location\\\": [4.2, 4.6] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"hotels-index\\\", \\\"_id\\\": \\\"5\\\" } }\\n{ \\\"location\\\": [3.3, 4.5] }\\n```\\n{% include copy-curl.html %}\\n\\n#### Step 3: Search your data\\n\\nNow search for hotels closest to the pin location `[5, 4]`. This location is labeled `Pin` in the following image. Each hotel is labeled with its document number.\\n\\n![Hotels on a coordinate plane]({{site.url}}{{site.baseurl}}\/images\/k-nn-search-hotels.png\/)\\n\\nTo search for the top three closest hotels, set `k` to `3`:\\n\\n```json\\nPOST \/hotels-index\/_search\\n{\\n  \\\"size\\\": 3,\\n  \\\"query\\\": {\\n    \\\"knn\\\": {\\n      \\\"location\\\": {\\n        \\\"vector\\\": [\\n          5,\\n          4\\n        ],\\n        \\\"k\\\": 3\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the hotels closest to the specified pin location:\\n\\n```json\\n{\\n  \\\"took\\\": 1093,\\n  \\\"timed_out\\\": false,\\n  \\\"_shards\\\": {\\n    \\\"total\\\": 1,\\n    \\\"successful\\\": 1,\\n    \\\"skipped\\\": 0,\\n    \\\"failed\\\": 0\\n  },\\n  \\\"hits\\\": {\\n    \\\"total\\\": {\\n      \\\"value\\\": 3,\\n      \\\"relation\\\": \\\"eq\\\"\\n    },\\n    \\\"max_score\\\": 0.952381,\\n    \\\"hits\\\": [\\n      {\\n        \\\"_index\\\": \\\"hotels-index\\\",\\n        \\\"_id\\\": \\\"2\\\",\\n        \\\"_score\\\": 0.952381,\\n        \\\"_source\\\": {\\n          \\\"location\\\": [\\n            5.2,\\n            3.9\\n          ]\\n        }\\n      },\\n      {\\n        \\\"_index\\\": \\\"hotels-index\\\",\\n        \\\"_id\\\": \\\"1\\\",\\n        \\\"_score\\\": 0.8333333,\\n        \\\"_source\\\": {\\n          \\\"location\\\": [\\n            5.2,\\n            4.4\\n          ]\\n        }\\n      },\\n      {\\n        \\\"_index\\\": \\\"hotels-index\\\",\\n        \\\"_id\\\": \\\"3\\\",\\n        \\\"_score\\\": 0.72992706,\\n        \\\"_source\\\": {\\n          \\\"location\\\": [\\n            4.9,\\n            3.4\\n          ]\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n\\n### Vector search with filtering\\n\\nFor information about vector search with filtering, see [k-NN search with filters]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/filter-search-knn\/).\\n\\n## Generating vector embeddings in OpenSearch\\n\\n[Neural search]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-search\/) encapsulates the infrastructure needed to perform semantic vector searches. After you integrate an inference (embedding) service, neural search functions like lexical search, accepting a textual query and returning relevant documents.\\n\\nWhen you index your data, neural search transforms text into vector embeddings and indexes both the text and its vector embeddings in a vector index. When you use a neural query during search, neural search converts the query text into vector embeddings and uses vector search to return the results.\\n\\n### Choosing a model\\n\\nThe first step in setting up neural search is choosing a model. You can upload a model to your OpenSearch cluster, use one of the pretrained models provided by OpenSearch, or connect to an externally hosted model. For more information, see [Integrating ML models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/integrating-ml-models\/).\\n\\n### Neural search tutorial\\n\\nFor a step-by-step tutorial, see [Neural search tutorial]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-search-tutorial\/).\\n\\n### Search methods\\n\\nChoose one of the following search methods to use your model for neural search:\\n\\n- [Semantic search]({{site.url}}{{site.baseurl}}\/search-plugins\/semantic-search\/): Uses dense retrieval based on text embedding models to search text data. \\n\\n- [Hybrid search]({{site.url}}{{site.baseurl}}\/search-plugins\/hybrid-search\/): Combines lexical and neural search to improve search relevance. \\n\\n- [Multimodal search]({{site.url}}{{site.baseurl}}\/search-plugins\/multimodal-search\/): Uses neural search with multimodal embedding models to search text and image data.\\n\\n- [Neural sparse search]({{site.url}}{{site.baseurl}}\/search-plugins\/neural-sparse-search\/): Uses neural search with sparse retrieval based on sparse embedding models to search text data.\\n\\n- [Conversational search]({{site.url}}{{site.baseurl}}\/search-plugins\/conversational-search\/): With conversational search, you can ask questions in natural language, receive a text response, and ask additional clarifying questions.\\n\\n\\n---\\nlayout: default\\ntitle: k-NN search\\nnav_order: 20\\nhas_children: true\\nhas_toc: false\\nredirect_from:\\n  - \/search-plugins\/knn\/\\n---\\n\\n# k-NN search\\n\\nShort for *k-nearest neighbors*, the k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points.\\n\\nUse cases include recommendations (for example, an \\\"other songs you might like\\\" feature in a music application), image recognition, and fraud detection. For more background information about k-NN search, see [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Nearest_neighbor_search).\\n\\nThis plugin supports three different methods for obtaining the k-nearest neighbors from an index of vectors:\\n\\n1. **Approximate k-NN**\\n\\n    The first method takes an approximate nearest neighbor approach---it uses one of several algorithms to return the approximate k-nearest neighbors to a query vector. Usually, these algorithms sacrifice indexing speed and search accuracy in return for performance benefits such as lower latency, smaller memory footprints and more scalable search. To learn more about the algorithms, refer to [*nmslib*](https:\/\/github.com\/nmslib\/nmslib\/blob\/master\/manual\/README.md)'s and [*faiss*](https:\/\/github.com\/facebookresearch\/faiss\/wiki)'s documentation.\\n\\n    Approximate k-NN is the best choice for searches over large indexes (that is, hundreds of thousands of vectors or more) that require low latency. You should not use approximate k-NN if you want to apply a filter on the index before the k-NN search, which greatly reduces the number of vectors to be searched. In this case, you should use either the script scoring method or Painless extensions.\\n\\n    For more details about this method, including recommendations for which engine to use, see [Approximate k-NN search]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/).\\n\\n2. **Script Score k-NN**\\n\\n    The second method extends OpenSearch's script scoring functionality to execute a brute force, exact k-NN search over \\\"knn_vector\\\" fields or fields that can represent binary objects. With this approach, you can run k-NN search on a subset of vectors in your index (sometimes referred to as a pre-filter search).\\n\\n    Use this approach for searches over smaller bodies of documents or when a pre-filter is needed. Using this approach on large indexes may lead to high latencies.\\n\\n    For more details about this method, see [Exact k-NN with scoring script]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-score-script\/).\\n\\n3. **Painless extensions**\\n\\n    The third method adds the distance functions as painless extensions that you can use in more complex combinations. Similar to the k-NN Script Score, you can use this method to perform a brute force, exact k-NN search across an index, which also supports pre-filtering.\\n\\n    This approach has slightly slower query performance compared to the k-NN Script Score. If your use case requires more customization over the final score, you should use this approach over Script Score k-NN.\\n\\n    For more details about this method, see [Painless scripting functions]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/painless-functions\/).\\n\\n\\nOverall, for larger data sets, you should generally choose the approximate nearest neighbor method because it scales significantly better. For smaller data sets, where you may want to apply a filter, you should choose the custom scoring approach. If you have a more complex use case where you need to use a distance function as part of their scoring method, you should use the painless scripting approach.\\n\\n\\n---\\nlayout: default\\ntitle: k-NN index\\nnav_order: 5\\nparent: k-NN search\\nhas_children: false\\n---\\n\\n# k-NN index\\n\\nThe k-NN plugin introduces a custom data type, the `knn_vector`, that allows users to ingest their k-NN vectors into an OpenSearch index and perform different kinds of k-NN search. The `knn_vector` field is highly configurable and can serve many different k-NN workloads. For more information, see [k-NN vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector\/).\\n\\nTo create a k-NN index, set the `settings.index.knn` parameter to `true`:\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 3,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"parameters\\\": {\\n            \\\"ef_construction\\\": 128,\\n            \\\"m\\\": 24\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Lucene byte vector\\n\\nStarting with k-NN plugin version 2.9, you can use `byte` vectors with the `lucene` engine to reduce the amount of storage space needed. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#lucene-byte-vector).\\n\\n## Binary vector\\n\\nStarting with k-NN plugin version 2.16, you can use `binary` vectors with the `faiss` engine to reduce the amount of required storage space. For more information, see [Binary k-NN vectors]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#binary-k-nn-vectors).\\n\\n## SIMD optimization for the Faiss engine\\n\\nStarting with version 2.13, the k-NN plugin supports [Single Instruction Multiple Data (SIMD)](https:\/\/en.wikipedia.org\/wiki\/Single_instruction,_multiple_data) processing if the underlying hardware supports SIMD instructions (AVX2 on x64 architecture and Neon on ARM64 architecture). SIMD is supported by default on Linux machines only for the Faiss engine. SIMD architecture helps boost overall performance by improving indexing throughput and reducing search latency.\\n\\nSIMD optimization is applicable only if the vector dimension is a multiple of 8.\\n{: .note}\\n\\n<!-- vale off -->\\n### x64 architecture\\n<!-- vale on -->\\n\\nFor the x64 architecture, two different versions of the Faiss library are built and shipped with the artifact:\\n\\n- `libopensearchknn_faiss.so`: The non-optimized Faiss library without SIMD instructions. \\n- `libopensearchknn_faiss_avx2.so`: The Faiss library that contains AVX2 SIMD instructions. \\n\\nIf your hardware supports AVX2, the k-NN plugin loads the `libopensearchknn_faiss_avx2.so` library at runtime.\\n\\nTo disable AVX2 and load the non-optimized Faiss library (`libopensearchknn_faiss.so`), specify the `knn.faiss.avx2.disabled` static setting as `true` in `opensearch.yml` (default is `false`). Note that to update a static setting, you must stop the cluster, change the setting, and restart the cluster. For more information, see [Static settings]({{site.url}}{{site.baseurl}}\/install-and-configure\/configuring-opensearch\/index\/#static-settings).\\n\\n### ARM64 architecture\\n\\nFor the ARM64 architecture, only one performance-boosting Faiss library (`libopensearchknn_faiss.so`) is built and shipped. The library contains Neon SIMD instructions and cannot be disabled. \\n\\n## Method definitions\\n\\nA method definition refers to the underlying configuration of the approximate k-NN algorithm you want to use. Method definitions are used to either create a `knn_vector` field (when the method does not require training) or [create a model during training]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/api#train-a-model) that can then be used to [create a `knn_vector` field]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/#building-a-k-nn-index-from-a-model).\\n\\nA method definition will always contain the name of the method, the space_type the method is built for, the engine\\n(the library) to use, and a map of parameters.\\n\\nMapping parameter | Required | Default | Updatable | Description\\n:--- | :--- | :--- | :--- | :---\\n`name` | true | n\/a | false | The identifier for the nearest neighbor method.\\n`space_type` | false | l2 | false | The vector space used to calculate the distance between vectors.\\n`engine` | false | nmslib | false | The approximate k-NN library to use for indexing and search. The available libraries are faiss, nmslib, and Lucene.\\n`parameters` | false | null | false | The parameters used for the nearest neighbor method.\\n\\n### Supported nmslib methods\\n\\nMethod name | Requires training | Supported spaces | Description\\n:--- | :--- | :--- | :---\\n`hnsw` | false | l2, innerproduct, cosinesimil, l1, linf | Hierarchical proximity graph approach to approximate k-NN search. For more details on the algorithm, see this [abstract](https:\/\/arxiv.org\/abs\/1603.09320).\\n\\n#### HNSW parameters\\n\\nParameter name | Required | Default | Updatable | Description\\n:--- | :--- | :--- | :--- | :---\\n`ef_construction` | false | 100 | false | The size of the dynamic list used during k-NN graph creation. Higher values result in a more accurate graph but slower indexing speed.\\n`m` | false | 16 | false | The number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100.\\n\\nFor nmslib, *ef_search* is set in the [index settings](#index-settings).\\n{: .note}\\n\\nAn index created in OpenSearch version 2.11 or earlier will still use the old `ef_construction` value (`512`).\\n{: .note}\\n\\n### Supported Faiss methods\\n\\nMethod name | Requires training | Supported spaces | Description\\n:--- | :--- |:---| :---\\n`hnsw` | false | l2, innerproduct, hamming | Hierarchical proximity graph approach to approximate k-NN search.\\n`ivf` | true | l2, innerproduct, hamming  | Stands for _inverted file index_. Bucketing approach where vectors are assigned different buckets based on clustering and, during search, only a subset of the buckets is searched.\\n\\nFor hnsw, \\\"innerproduct\\\" is not available when PQ is used.\\n{: .note}\\n\\nThe `hamming` space type is supported for binary vectors in OpenSearch version 2.16 and later. For more information, see [Binary k-NN vectors]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#binary-k-nn-vectors).\\n{: .note}\\n\\n#### HNSW parameters\\n\\nParameter name | Required | Default | Updatable | Description\\n:--- | :--- | :--- | :--- | :---\\n`ef_search` | false | 100 | false | The size of the dynamic list used during k-NN searches. Higher values result in more accurate but slower searches.\\n`ef_construction` | false | 100 | false | The size of the dynamic list used during k-NN graph creation. Higher values result in a more accurate graph but slower indexing speed.\\n`m` | false | 16 | false | The number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100.\\n`encoder` | false | flat | false | Encoder definition for encoding vectors. Encoders can reduce the memory footprint of your index, at the expense of search accuracy.\\n\\nAn index created in OpenSearch version 2.11 or earlier will still use the old `ef_construction` and `ef_search` values (`512`).\\n{: .note}\\n\\n#### IVF parameters\\n\\nParameter name | Required | Default | Updatable | Description\\n:--- | :--- | :--- | :--- | :---\\n`nlist` | false | 4 | false | Number of buckets to partition vectors into. Higher values may lead to more accurate searches at the expense of memory and training latency. For more information about choosing the right value, refer to [Guidelines to choose an index](https:\/\/github.com\/facebookresearch\/faiss\/wiki\/Guidelines-to-choose-an-index).\\n`nprobes` | false | 1 | false | Number of buckets to search during query. Higher values lead to more accurate but slower searches.\\n`encoder` | false | flat | false | Encoder definition for encoding vectors. Encoders can reduce the memory footprint of your index, at the expense of search accuracy.\\n\\nFor more information about setting these parameters, refer to the [Faiss documentation](https:\/\/github.com\/facebookresearch\/faiss\/wiki\/Faiss-indexes).\\n\\n#### IVF training requirements\\n\\nThe IVF algorithm requires a training step. To create an index that uses IVF, you need to train a model with the [Train API]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/api#train-a-model), passing the IVF method definition. IVF requires that, at a minimum, there are `nlist` training data points, but it is [recommended that you use more than this](https:\/\/github.com\/facebookresearch\/faiss\/wiki\/Guidelines-to-choose-an-index#how-big-is-the-dataset). Training data can be composed of either the same data that is going to be ingested or a separate dataset.\\n\\n### Supported Lucene methods\\n\\nMethod name | Requires training | Supported spaces | Description\\n:--- | :--- |:--------------------------------------------------------------------------------| :---\\n`hnsw` | false | l2, cosinesimil, innerproduct (supported in OpenSearch 2.13 and later) | Hierarchical proximity graph approach to approximate k-NN search.\\n\\n#### HNSW parameters\\n\\nParameter name | Required | Default | Updatable | Description\\n:--- | :--- | :--- | :--- | :---\\n`ef_construction` | false | 100 | false | The size of the dynamic list used during k-NN graph creation. Higher values result in a more accurate graph but slower indexing speed.<br>The Lucene engine uses the proprietary term \\\"beam_width\\\" to describe this function, which corresponds directly to \\\"ef_construction\\\". To be consistent throughout the OpenSearch documentation, we retain the term \\\"ef_construction\\\" for this parameter.\\n`m` | false | 16 | false | The number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100.<br>The Lucene engine uses the proprietary term \\\"max_connections\\\" to describe this function, which corresponds directly to \\\"m\\\". To be consistent throughout OpenSearch documentation, we retain the term \\\"m\\\" to label this parameter.\\n\\nLucene HNSW implementation ignores `ef_search`  and dynamically sets it to the value of \\\"k\\\" in the search request. Therefore, there is no need to make settings for `ef_search` when using the Lucene engine.\\n{: .note}\\n\\nAn index created in OpenSearch version 2.11 or earlier will still use the old `ef_construction` value (`512`).\\n{: .note}\\n\\n```json\\n\\\"method\\\": {\\n    \\\"name\\\":\\\"hnsw\\\",\\n    \\\"engine\\\":\\\"lucene\\\",\\n    \\\"space_type\\\": \\\"l2\\\",\\n    \\\"parameters\\\":{\\n        \\\"m\\\":2048,\\n        \\\"ef_construction\\\": 245\\n    }\\n}\\n```\\n\\n### Supported Faiss encoders\\n\\nYou can use encoders to reduce the memory footprint of a k-NN index at the expense of search accuracy. The k-NN plugin currently supports the `flat`, `pq`, and `sq` encoders in the Faiss library.\\n\\nThe following example method definition specifies the `hnsw` method and a `pq` encoder:\\n\\n```json\\n\\\"method\\\": {\\n  \\\"name\\\":\\\"hnsw\\\",\\n  \\\"engine\\\":\\\"faiss\\\",\\n  \\\"space_type\\\": \\\"l2\\\",\\n  \\\"parameters\\\":{\\n    \\\"encoder\\\":{\\n      \\\"name\\\":\\\"pq\\\",\\n      \\\"parameters\\\":{\\n        \\\"code_size\\\": 8,\\n        \\\"m\\\": 8\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThe `hnsw` method supports the `pq` encoder for OpenSearch versions 2.10 and later. The `code_size` parameter of a `pq` encoder with the `hnsw` method must be **8**.\\n{: .important}\\n\\nEncoder name | Requires training | Description\\n:--- | :--- | :---\\n`flat` (Default) | false | Encode vectors as floating-point arrays. This encoding does not reduce memory footprint.\\n`pq` | true | An abbreviation for _product quantization_, it is a lossy compression technique that uses clustering to encode a vector into a fixed size of bytes, with the goal of minimizing the drop in k-NN search accuracy. At a high level, vectors are broken up into `m` subvectors, and then each subvector is represented by a `code_size` code obtained from a code book produced during training. For more information about product quantization, see [this blog post](https:\/\/medium.com\/dotstar\/understanding-faiss-part-2-79d90b1e5388).\\n`sq` | false | An abbreviation for _scalar quantization_. Starting with k-NN plugin version 2.13, you can use the `sq` encoder to quantize 32-bit floating-point vectors into 16-bit floats. In version 2.13, the built-in `sq` encoder is the SQFP16 Faiss encoder. The encoder reduces memory footprint with a minimal loss of precision and improves performance by using SIMD optimization (using AVX2 on x86 architecture or Neon on ARM64 architecture). For more information, see [Faiss scalar quantization]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-vector-quantization#faiss-16-bit-scalar-quantization).\\n\\n#### PQ parameters\\n\\nParameter name | Required | Default | Updatable | Description\\n:--- | :--- | :--- | :--- | :---\\n`m` | false | 1 | false |  Determines the number of subvectors into which to break the vector. Subvectors are encoded independently of each other. This vector dimension must be divisible by `m`. Maximum value is 1,024.\\n`code_size` | false | 8 | false | Determines the number of bits into which to encode a subvector. Maximum value is 8. For IVF, this value must be less than or equal to 8. For HNSW, this value can only be 8.\\n\\n#### SQ parameters\\n\\nParameter name | Required | Default | Updatable | Description\\n:--- | :--- | :-- | :--- | :---\\n`type` | false | `fp16` | false |  The type of scalar quantization to be used to encode 32-bit float vectors into the corresponding type. As of OpenSearch 2.13, only the `fp16` encoder type is supported. For the `fp16` encoder, vector values must be in the [-65504.0, 65504.0] range. \\n`clip` | false | `false` | false | If `true`, then any vector values outside of the supported range for the specified vector type are rounded so that they are in the range. If `false`, then the request is rejected if any vector values are outside of the supported range. Setting `clip` to `true` may decrease recall.\\n\\nFor more information and examples, see [Using Faiss scalar quantization]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-vector-quantization\/#using-faiss-scalar-quantization).\\n\\n#### Examples\\n\\nThe following example uses the `ivf` method  without specifying an encoder (by default, OpenSearch uses the `flat` encoder):\\n\\n```json\\n\\\"method\\\": {\\n  \\\"name\\\":\\\"ivf\\\",\\n  \\\"engine\\\":\\\"faiss\\\",\\n  \\\"space_type\\\": \\\"l2\\\",\\n  \\\"parameters\\\":{\\n    \\\"nlist\\\": 4,\\n    \\\"nprobes\\\": 2\\n  }\\n}\\n```\\n\\nThe following example uses the `ivf` method with a `pq` encoder:\\n\\n```json\\n\\\"method\\\": {\\n  \\\"name\\\":\\\"ivf\\\",\\n  \\\"engine\\\":\\\"faiss\\\",\\n  \\\"space_type\\\": \\\"l2\\\",\\n  \\\"parameters\\\":{\\n    \\\"encoder\\\":{\\n      \\\"name\\\":\\\"pq\\\",\\n      \\\"parameters\\\":{\\n        \\\"code_size\\\": 8,\\n        \\\"m\\\": 8\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThe following example uses the `hnsw` method without specifying an encoder (by default, OpenSearch uses the `flat` encoder):\\n\\n```json\\n\\\"method\\\": {\\n  \\\"name\\\":\\\"hnsw\\\",\\n  \\\"engine\\\":\\\"faiss\\\",\\n  \\\"space_type\\\": \\\"l2\\\",\\n  \\\"parameters\\\":{\\n    \\\"ef_construction\\\": 256,\\n    \\\"m\\\": 8\\n  }\\n}\\n```\\n\\nThe following example uses the `hnsw` method with an `sq` encoder of type `fp16` with `clip` enabled:\\n\\n```json\\n\\\"method\\\": {\\n  \\\"name\\\":\\\"hnsw\\\",\\n  \\\"engine\\\":\\\"faiss\\\",\\n  \\\"space_type\\\": \\\"l2\\\",\\n  \\\"parameters\\\":{\\n    \\\"encoder\\\": {\\n      \\\"name\\\": \\\"sq\\\",\\n      \\\"parameters\\\": {\\n        \\\"type\\\": \\\"fp16\\\",\\n        \\\"clip\\\": true\\n      }  \\n    },    \\n    \\\"ef_construction\\\": 256,\\n    \\\"m\\\": 8\\n  }\\n}\\n```\\n\\nThe following example uses the `ivf` method with an `sq` encoder of type `fp16`:\\n\\n```json\\n\\\"method\\\": {\\n  \\\"name\\\":\\\"ivf\\\",\\n  \\\"engine\\\":\\\"faiss\\\",\\n  \\\"space_type\\\": \\\"l2\\\",\\n  \\\"parameters\\\":{\\n    \\\"encoder\\\": {\\n      \\\"name\\\": \\\"sq\\\",\\n      \\\"parameters\\\": {\\n        \\\"type\\\": \\\"fp16\\\",\\n        \\\"clip\\\": false\\n      }\\n    },\\n    \\\"nprobes\\\": 2\\n  }\\n}\\n```\\n\\n### Choosing the right method\\n\\nThere are several options to choose from when building your `knn_vector` field. To determine the correct methods and parameters, you should first understand the requirements of your workload and what trade-offs you are willing to make. Factors to consider are (1) query latency, (2) query quality, (3) memory limits, and (4) indexing latency.\\n\\nIf memory is not a concern, HNSW offers a strong query latency\/query quality trade-off.\\n\\nIf you want to use less memory and increase indexing speed as compared to HNSW while maintaining similar query quality, you should evaluate IVF.\\n\\nIf memory is a concern, consider adding a PQ encoder to your HNSW or IVF index. Because PQ is a lossy encoding, query quality will drop.\\n\\nYou can reduce the memory footprint by a factor of 2, with a minimal loss in search quality, by using the [`fp_16` encoder]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-vector-quantization\/#faiss-16-bit-scalar-quantization). If your vector dimensions are within the [-128, 127] byte range, we recommend using the [byte quantizer]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector\/#lucene-byte-vector) to reduce the memory footprint by a factor of 4. To learn more about vector quantization options, see [k-NN vector quantization]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-vector-quantization\/). \\n\\n### Memory estimation\\n\\nIn a typical OpenSearch cluster, a certain portion of RAM is reserved for the JVM heap. The k-NN plugin allocates native library indexes to a portion of the remaining RAM. This portion's size is determined by the `circuit_breaker_limit` cluster setting. By default, the limit is set to 50%.\\n\\nHaving a replica doubles the total number of vectors.\\n{: .note }\\n\\nFor information about using memory estimation with vector quantization, see the [vector quantization documentation]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-vector-quantization\/#memory-estimation).\\n{: .note }\\n\\n#### HNSW memory estimation\\n\\nThe memory required for HNSW is estimated to be `1.1 * (4 * dimension + 8 * M)` bytes\/vector.\\n\\nAs an example, assume you have a million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\\n\\n```\\n1.1 * (4 * 256 + 8 * 16) * 1,000,000 ~= 1.267 GB\\n```\\n\\n#### IVF memory estimation\\n\\nThe memory required for IVF is estimated to be `1.1 * (((4 * dimension) * num_vectors) + (4 * nlist * d))` bytes.\\n\\nAs an example, assume you have a million vectors with a dimension of 256 and nlist of 128. The memory requirement can be estimated as follows:\\n\\n```\\n1.1 * (((4 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 1.126 GB\\n\\n```\\n\\n## Index settings\\n\\nAdditionally, the k-NN plugin introduces several index settings that can be used to configure the k-NN structure as well.\\n\\nAt the moment, several parameters defined in the settings are in the deprecation process. Those parameters should be set in the mapping instead of the index settings. Parameters set in the mapping will override the parameters set in the index settings. Setting the parameters in the mapping allows an index to have multiple `knn_vector` fields with different parameters.\\n\\nSetting | Default | Updatable | Description\\n:--- | :--- | :--- | :---\\n`index.knn` | false | false | Whether the index should build native library indexes for the `knn_vector` fields. If set to false, the `knn_vector` fields will be stored in doc values, but approximate k-NN search functionality will be disabled.\\n`index.knn.algo_param.ef_search` | 100 | true | The size of the dynamic list used during k-NN searches. Higher values result in more accurate but slower searches. Only available for NMSLIB.\\n`index.knn.algo_param.ef_construction` | 100 | false | Deprecated in 1.0.0. Instead, use the [mapping parameters](https:\/\/opensearch.org\/docs\/latest\/search-plugins\/knn\/knn-index\/#method-definitions) to set this value.\\n`index.knn.algo_param.m` | 16 | false | Deprecated in 1.0.0. Use the [mapping parameters](https:\/\/opensearch.org\/docs\/latest\/search-plugins\/knn\/knn-index\/#method-definitions) to set this value instead.\\n`index.knn.space_type` | l2 | false | Deprecated in 1.0.0. Use the [mapping parameters](https:\/\/opensearch.org\/docs\/latest\/search-plugins\/knn\/knn-index\/#method-definitions) to set this value instead.\\n\\nAn index created in OpenSearch version 2.11 or earlier will still use the old `ef_construction` and `ef_search` values (`512`).\\n{: .note}\\n\\n\\n---\\nlayout: default\\ntitle: k-NN vector\\nnav_order: 58\\nhas_children: false\\nparent: Supported field types\\nhas_math: true\\n---\\n\\n# k-NN vector field type\\n\\nThe [k-NN plugin]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/index\/) introduces a custom data type, the `knn_vector`, that allows users to ingest their k-NN vectors into an OpenSearch index and perform different kinds of k-NN search. The `knn_vector` field is highly configurable and can serve many different k-NN workloads. In general, a `knn_vector` field can be built either by providing a method definition or specifying a model id.\\n\\n## Example\\n\\nFor example, to map `my_vector` as a `knn_vector`, use the following request:\\n\\n```json\\nPUT test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 3,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"parameters\\\": {\\n            \\\"ef_construction\\\": 128,\\n            \\\"m\\\": 24\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Method definitions\\n\\n[Method definitions]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index#method-definitions) are used when the underlying [approximate k-NN]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/) algorithm does not require training. For example, the following `knn_vector` field specifies that *nmslib*'s implementation of *hnsw* should be used for approximate k-NN search. During indexing, *nmslib* will build the corresponding *hnsw* segment files.\\n\\n```json\\n\\\"my_vector\\\": {\\n  \\\"type\\\": \\\"knn_vector\\\",\\n  \\\"dimension\\\": 4,\\n  \\\"method\\\": {\\n    \\\"name\\\": \\\"hnsw\\\",\\n    \\\"space_type\\\": \\\"l2\\\",\\n    \\\"engine\\\": \\\"nmslib\\\",\\n    \\\"parameters\\\": {\\n      \\\"ef_construction\\\": 128,\\n      \\\"m\\\": 24\\n    }\\n  }\\n}\\n```\\n\\n## Model IDs\\n\\nModel IDs are used when the underlying Approximate k-NN algorithm requires a training step. As a prerequisite, the model must be created with the [Train API]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/api#train-a-model). The\\nmodel contains the information needed to initialize the native library segment files.\\n\\n```json\\n  \\\"type\\\": \\\"knn_vector\\\",\\n  \\\"model_id\\\": \\\"my-model\\\"\\n}\\n```\\n\\nHowever, if you intend to use Painless scripting or a k-NN score script, you only need to pass the dimension.\\n ```json\\n   \\\"type\\\": \\\"knn_vector\\\",\\n   \\\"dimension\\\": 128\\n }\\n ```\\n\\n## Lucene byte vector\\n\\nBy default, k-NN vectors are `float` vectors, where each dimension is 4 bytes. If you want to save storage space, you can use `byte` vectors with the `lucene` engine. In a `byte` vector, each dimension is a signed 8-bit integer in the [-128, 127] range. \\n \\nByte vectors are supported only for the `lucene` engine. They are not supported for the `nmslib` and `faiss` engines.\\n{: .note}\\n\\nIn [k-NN benchmarking tests](https:\/\/github.com\/opensearch-project\/k-NN\/tree\/main\/benchmarks\/perf-tool), the use of `byte` rather than `float` vectors resulted in a significant reduction in storage and memory usage as well as improved indexing throughput and reduced query latency. Additionally, precision on recall was not greatly affected (note that recall can depend on various factors, such as the [quantization technique](#quantization-techniques) and data distribution). \\n\\nWhen using `byte` vectors, expect some loss of precision in the recall compared to using `float` vectors. Byte vectors are useful in large-scale applications and use cases that prioritize a reduced memory footprint in exchange for a minimal loss of recall.\\n{: .important}\\n \\nIntroduced in k-NN plugin version 2.9, the optional `data_type` parameter defines the data type of a vector. The default value of this parameter is `float`.\\n\\nTo use a `byte` vector, set the `data_type` parameter to `byte` when creating mappings for an index:\\n\\n ```json\\nPUT test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 3,\\n        \\\"data_type\\\": \\\"byte\\\",\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"parameters\\\": {\\n            \\\"ef_construction\\\": 128,\\n            \\\"m\\\": 24\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThen ingest documents as usual. Make sure each dimension in the vector is in the supported [-128, 127] range:\\n\\n```json\\nPUT test-index\/_doc\/1\\n{\\n  \\\"my_vector\\\": [-126, 28, 127]\\n}\\n```\\n{% include copy-curl.html %}\\n\\n```json\\nPUT test-index\/_doc\/2\\n{\\n  \\\"my_vector\\\": [100, -128, 0]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nWhen querying, be sure to use a `byte` vector:\\n\\n```json\\nGET test-index\/_search\\n{\\n  \\\"size\\\": 2,\\n  \\\"query\\\": {\\n    \\\"knn\\\": {\\n      \\\"my_vector\\\": {\\n        \\\"vector\\\": [26, -120, 99],\\n        \\\"k\\\": 2\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Quantization techniques\\n\\nIf your vectors are of the type `float`, you need to first convert them to the `byte` type before ingesting the documents. This conversion is accomplished by _quantizing the dataset_---reducing the precision of its vectors. There are many quantization techniques, such as scalar quantization or product quantization (PQ), which is used in the Faiss engine. The choice of quantization technique depends on the type of data you're using and can affect the accuracy of recall values. The following sections describe the scalar quantization algorithms that were used to quantize the [k-NN benchmarking test](https:\/\/github.com\/opensearch-project\/k-NN\/tree\/main\/benchmarks\/perf-tool) data for the [L2](#scalar-quantization-for-the-l2-space-type) and [cosine similarity](#scalar-quantization-for-the-cosine-similarity-space-type) space types. The provided pseudocode is for illustration purposes only.\\n\\n#### Scalar quantization for the L2 space type\\n\\nThe following example pseudocode illustrates the scalar quantization technique used for the benchmarking tests on Euclidean datasets with the L2 space type. Euclidean distance is shift invariant. If you shift both $$x$$ and $$y$$ by the same $$z$$, then the distance remains the same ($$\\\\lVert x-y\\\\rVert =\\\\lVert (x-z)-(y-z)\\\\rVert$$).\\n\\n```python\\n# Random dataset (Example to create a random dataset)\\ndataset = np.random.uniform(-300, 300, (100, 10))\\n# Random query set (Example to create a random queryset)\\nqueryset = np.random.uniform(-350, 350, (100, 10))\\n# Number of values\\nB = 256\\n\\n# INDEXING:\\n# Get min and max\\ndataset_min = np.min(dataset)\\ndataset_max = np.max(dataset)\\n# Shift coordinates to be non-negative\\ndataset -= dataset_min\\n# Normalize into [0, 1]\\ndataset *= 1. \/ (dataset_max - dataset_min)\\n# Bucket into 256 values\\ndataset = np.floor(dataset * (B - 1)) - int(B \/ 2)\\n\\n# QUERYING:\\n# Clip (if queryset range is out of datset range)\\nqueryset = queryset.clip(dataset_min, dataset_max)\\n# Shift coordinates to be non-negative\\nqueryset -= dataset_min\\n# Normalize\\nqueryset *= 1. \/ (dataset_max - dataset_min)\\n# Bucket into 256 values\\nqueryset = np.floor(queryset * (B - 1)) - int(B \/ 2)\\n```\\n{% include copy.html %}\\n\\n#### Scalar quantization for the cosine similarity space type\\n\\nThe following example pseudocode illustrates the scalar quantization technique used for the benchmarking tests on angular datasets with the cosine similarity space type. Cosine similarity is not shift invariant ($$cos(x, y) \\\\neq cos(x-z, y-z)$$). \\n\\nThe following pseudocode is for positive numbers:\\n\\n```python\\n# For Positive Numbers\\n\\n# INDEXING and QUERYING:\\n\\n# Get Max of train dataset\\nmax = np.max(dataset)\\nmin = 0\\nB = 127\\n\\n# Normalize into [0,1]\\nval = (val - min) \/ (max - min)\\nval = (val * B)\\n\\n# Get int and fraction values\\nint_part = floor(val)\\nfrac_part = val - int_part\\n\\nif 0.5 < frac_part:\\n bval = int_part + 1\\nelse:\\n bval = int_part\\n\\nreturn Byte(bval)\\n```\\n{% include copy.html %}\\n\\nThe following pseudocode is for negative numbers:\\n\\n```python\\n# For Negative Numbers\\n\\n# INDEXING and QUERYING:\\n\\n# Get Min of train dataset\\nmin = 0\\nmax = -np.min(dataset)\\nB = 128\\n\\n# Normalize into [0,1]\\nval = (val - min) \/ (max - min)\\nval = (val * B)\\n\\n# Get int and fraction values\\nint_part = floor(var)\\nfrac_part = val - int_part\\n\\nif 0.5 < frac_part:\\n bval = int_part + 1\\nelse:\\n bval = int_part\\n\\nreturn Byte(bval)\\n```\\n{% include copy.html %}\\n\\n## Binary k-NN vectors\\n\\nYou can reduce memory costs by a factor of 32 by switching from float to binary vectors.\\nUsing binary vector indexes can lower operational costs while maintaining high recall performance, making large-scale deployment more economical and efficient.\\n\\nBinary format is available for the following k-NN search types:\\n\\n- [Approximate k-NN]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/): Supports binary vectors only for the Faiss engine with the HNSW and IVF algorithms.\\n- [Script score k-NN]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-score-script\/): Enables the use of binary vectors in script scoring.\\n- [Painless extensions]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/painless-functions\/): Allows the use of binary vectors with Painless scripting extensions.\\n\\n### Requirements \\n\\nThere are several requirements for using binary vectors in the OpenSearch k-NN plugin:\\n\\n- The `data_type` of the binary vector index must be `binary`.\\n- The `space_type` of the binary vector index must be `hamming`.\\n- The `dimension` of the binary vector index must be a multiple of 8.\\n- You must convert your binary data into 8-bit signed integers (`int8`) in the [-128, 127] range. For example, the binary sequence of 8 bits `0, 1, 1, 0, 0, 0, 1, 1` must be converted into its equivalent byte value of `99` to be used as a binary vector input.\\n\\n### Example: HNSW\\n\\nTo create a binary vector index with the Faiss engine and HNSW algorithm, send the following request:\\n\\n```json\\nPUT \/test-binary-hnsw\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 8,\\n        \\\"data_type\\\": \\\"binary\\\",\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"space_type\\\": \\\"hamming\\\",\\n          \\\"engine\\\": \\\"faiss\\\",\\n          \\\"parameters\\\": {\\n            \\\"ef_construction\\\": 128,\\n            \\\"m\\\": 24\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThen ingest some documents containing binary vectors:\\n\\n```json\\nPUT _bulk\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-hnsw\\\", \\\"_id\\\": \\\"1\\\"}}\\n{\\\"my_vector\\\": [7], \\\"price\\\": 4.4}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-hnsw\\\", \\\"_id\\\": \\\"2\\\"}}\\n{\\\"my_vector\\\": [10], \\\"price\\\": 14.2}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-hnsw\\\", \\\"_id\\\": \\\"3\\\"}}\\n{\\\"my_vector\\\": [15], \\\"price\\\": 19.1}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-hnsw\\\", \\\"_id\\\": \\\"4\\\"}}\\n{\\\"my_vector\\\": [99], \\\"price\\\": 1.2}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-hnsw\\\", \\\"_id\\\": \\\"5\\\"}}\\n{\\\"my_vector\\\": [80], \\\"price\\\": 16.5}\\n```\\n{% include copy-curl.html %}\\n\\nWhen querying, be sure to use a binary vector:\\n\\n```json\\nGET \/test-binary-hnsw\/_search\\n{\\n  \\\"size\\\": 2,\\n  \\\"query\\\": {\\n    \\\"knn\\\": {\\n      \\\"my_vector\\\": {\\n        \\\"vector\\\": [9],\\n        \\\"k\\\": 2\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the two vectors closest to the query vector:\\n\\n<details markdown=\\\"block\\\">\\n  <summary>\\n    Response\\n  <\/summary>\\n  {: .text-delta}\\n\\n```json\\n{\\n  \\\"took\\\": 8,\\n  \\\"timed_out\\\": false,\\n  \\\"_shards\\\": {\\n    \\\"total\\\": 1,\\n    \\\"successful\\\": 1,\\n    \\\"skipped\\\": 0,\\n    \\\"failed\\\": 0\\n  },\\n  \\\"hits\\\": {\\n    \\\"total\\\": {\\n      \\\"value\\\": 2,\\n      \\\"relation\\\": \\\"eq\\\"\\n    },\\n    \\\"max_score\\\": 0.5,\\n    \\\"hits\\\": [\\n      {\\n        \\\"_index\\\": \\\"test-binary-hnsw\\\",\\n        \\\"_id\\\": \\\"2\\\",\\n        \\\"_score\\\": 0.5,\\n        \\\"_source\\\": {\\n          \\\"my_vector\\\": [\\n            10\\n          ],\\n          \\\"price\\\": 14.2\\n        }\\n      },\\n      {\\n        \\\"_index\\\": \\\"test-binary-hnsw\\\",\\n        \\\"_id\\\": \\\"5\\\",\\n        \\\"_score\\\": 0.25,\\n        \\\"_source\\\": {\\n          \\\"my_vector\\\": [\\n            80\\n          ],\\n          \\\"price\\\": 16.5\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n<\/details>\\n\\n### Example: IVF\\n\\nThe IVF method requires a training step that creates and trains the model used to initialize the native library index during segment creation. For more information, see [Building a k-NN index from a model]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/#building-a-k-nn-index-from-a-model). \\n\\nFirst, create an index that will contain binary vector training data. Specify the Faiss engine and IVF algorithm and make sure that the `dimension` matches the dimension of the model you want to create:\\n\\n```json\\nPUT train-index\\n{\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"train-field\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 8,\\n        \\\"data_type\\\": \\\"binary\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nIngest training data containing binary vectors into the training index:\\n\\n<details markdown=\\\"block\\\">\\n  <summary>\\n    Bulk ingest request\\n  <\/summary>\\n  {: .text-delta}\\n\\n```json\\nPUT _bulk\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"1\\\" } }\\n{ \\\"train-field\\\": [1] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"2\\\" } }\\n{ \\\"train-field\\\": [2] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"3\\\" } }\\n{ \\\"train-field\\\": [3] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"4\\\" } }\\n{ \\\"train-field\\\": [4] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"5\\\" } }\\n{ \\\"train-field\\\": [5] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"6\\\" } }\\n{ \\\"train-field\\\": [6] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"7\\\" } }\\n{ \\\"train-field\\\": [7] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"8\\\" } }\\n{ \\\"train-field\\\": [8] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"9\\\" } }\\n{ \\\"train-field\\\": [9] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"10\\\" } }\\n{ \\\"train-field\\\": [10] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"11\\\" } }\\n{ \\\"train-field\\\": [11] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"12\\\" } }\\n{ \\\"train-field\\\": [12] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"13\\\" } }\\n{ \\\"train-field\\\": [13] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"14\\\" } }\\n{ \\\"train-field\\\": [14] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"15\\\" } }\\n{ \\\"train-field\\\": [15] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"16\\\" } }\\n{ \\\"train-field\\\": [16] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"17\\\" } }\\n{ \\\"train-field\\\": [17] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"18\\\" } }\\n{ \\\"train-field\\\": [18] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"19\\\" } }\\n{ \\\"train-field\\\": [19] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"20\\\" } }\\n{ \\\"train-field\\\": [20] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"21\\\" } }\\n{ \\\"train-field\\\": [21] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"22\\\" } }\\n{ \\\"train-field\\\": [22] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"23\\\" } }\\n{ \\\"train-field\\\": [23] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"24\\\" } }\\n{ \\\"train-field\\\": [24] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"25\\\" } }\\n{ \\\"train-field\\\": [25] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"26\\\" } }\\n{ \\\"train-field\\\": [26] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"27\\\" } }\\n{ \\\"train-field\\\": [27] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"28\\\" } }\\n{ \\\"train-field\\\": [28] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"29\\\" } }\\n{ \\\"train-field\\\": [29] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"30\\\" } }\\n{ \\\"train-field\\\": [30] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"31\\\" } }\\n{ \\\"train-field\\\": [31] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"32\\\" } }\\n{ \\\"train-field\\\": [32] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"33\\\" } }\\n{ \\\"train-field\\\": [33] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"34\\\" } }\\n{ \\\"train-field\\\": [34] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"35\\\" } }\\n{ \\\"train-field\\\": [35] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"36\\\" } }\\n{ \\\"train-field\\\": [36] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"37\\\" } }\\n{ \\\"train-field\\\": [37] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"38\\\" } }\\n{ \\\"train-field\\\": [38] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"39\\\" } }\\n{ \\\"train-field\\\": [39] }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"train-index\\\", \\\"_id\\\": \\\"40\\\" } }\\n{ \\\"train-field\\\": [40] }\\n```\\n{% include copy-curl.html %}\\n<\/details>\\n\\nThen, create and train the model named `test-binary-model`. The model will be trained using the training data from the `train_field` in the `train-index`. Specify the `binary` data type and `hamming` space type:\\n\\n```json\\nPOST _plugins\/_knn\/models\/test-binary-model\/_train\\n{\\n  \\\"training_index\\\": \\\"train-index\\\",\\n  \\\"training_field\\\": \\\"train-field\\\",\\n  \\\"dimension\\\": 8,\\n  \\\"description\\\": \\\"model with binary data\\\",\\n  \\\"data_type\\\": \\\"binary\\\",\\n  \\\"method\\\": {\\n    \\\"name\\\": \\\"ivf\\\",\\n    \\\"engine\\\": \\\"faiss\\\",\\n    \\\"space_type\\\": \\\"hamming\\\",\\n    \\\"parameters\\\": {\\n      \\\"nlist\\\": 1,\\n      \\\"nprobes\\\": 1\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nTo check the model training status, call the Get Model API:\\n\\n```json\\nGET _plugins\/_knn\/models\/test-binary-model?filter_path=state\\n```\\n{% include copy-curl.html %}\\n\\nOnce the training is complete, the `state` changes to `created`.\\n\\nNext, create an index that will initialize its native library indexes using the trained model:\\n\\n```json\\nPUT test-binary-ivf\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"model_id\\\": \\\"test-binary-model\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nIngest the data containing the binary vectors that you want to search into the created index:\\n\\n```json\\nPUT _bulk?refresh=true\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-ivf\\\", \\\"_id\\\": \\\"1\\\"}}\\n{\\\"my_vector\\\": [7], \\\"price\\\": 4.4}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-ivf\\\", \\\"_id\\\": \\\"2\\\"}}\\n{\\\"my_vector\\\": [10], \\\"price\\\": 14.2}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-ivf\\\", \\\"_id\\\": \\\"3\\\"}}\\n{\\\"my_vector\\\": [15], \\\"price\\\": 19.1}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-ivf\\\", \\\"_id\\\": \\\"4\\\"}}\\n{\\\"my_vector\\\": [99], \\\"price\\\": 1.2}\\n{\\\"index\\\": {\\\"_index\\\": \\\"test-binary-ivf\\\", \\\"_id\\\": \\\"5\\\"}}\\n{\\\"my_vector\\\": [80], \\\"price\\\": 16.5}\\n```\\n{% include copy-curl.html %}\\n\\nFinally, search the data. Be sure to provide a binary vector in the k-NN vector field:\\n\\n```json\\nGET test-binary-ivf\/_search\\n{\\n  \\\"size\\\": 2,\\n  \\\"query\\\": {\\n    \\\"knn\\\": {\\n      \\\"my_vector\\\": {\\n        \\\"vector\\\": [8],\\n        \\\"k\\\": 2\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the two vectors closest to the query vector:\\n\\n<details markdown=\\\"block\\\">\\n  <summary>\\n    Response\\n  <\/summary>\\n  {: .text-delta}\\n\\n```json\\nGET \/_plugins\/_knn\/models\/my-model?filter_path=state\\n{\\n  \\\"took\\\": 7,\\n  \\\"timed_out\\\": false,\\n  \\\"_shards\\\": {\\n    \\\"total\\\": 1,\\n    \\\"successful\\\": 1,\\n    \\\"skipped\\\": 0,\\n    \\\"failed\\\": 0\\n  },\\n  \\\"hits\\\": {\\n    \\\"total\\\": {\\n      \\\"value\\\": 2,\\n      \\\"relation\\\": \\\"eq\\\"\\n    },\\n    \\\"max_score\\\": 0.5,\\n    \\\"hits\\\": [\\n      {\\n        \\\"_index\\\": \\\"test-binary-ivf\\\",\\n        \\\"_id\\\": \\\"2\\\",\\n        \\\"_score\\\": 0.5,\\n        \\\"_source\\\": {\\n          \\\"my_vector\\\": [\\n            10\\n          ],\\n          \\\"price\\\": 14.2\\n        }\\n      },\\n      {\\n        \\\"_index\\\": \\\"test-binary-ivf\\\",\\n        \\\"_id\\\": \\\"3\\\",\\n        \\\"_score\\\": 0.25,\\n        \\\"_source\\\": {\\n          \\\"my_vector\\\": [\\n            15\\n          ],\\n          \\\"price\\\": 19.1\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n<\/details>\\n\\n\\n\"","generated_answer":"To use vector search in OpenSearch:\n\n1. Create a k-NN index by setting `index.knn` to `true` in the index settings.\n2. Specify a `knn_vector` field type and configure the method definition, such as the algorithm, space type, and engine to use for approximate k-NN search.\n3. Ingest your vector data into the k-NN index.\n4. Perform vector search by using the `knn` query and providing a vector to search for the k-nearest neighbors.","usage":"{\"inputTokens\": 16947, \"outputTokens\": 135, \"totalTokens\": 17082}","latency":2680,"reasoning":"None","score":null,"faithfulness":0.375,"answer_relevancy":0.124,"cost":0.0044055}
{"query_text":"How do I understand the memory requirements for using hnsw?","groundtruth_answer":"<question_answer>\nThe memory requirements for using HNSW with vector quantization depend on the quantization method used:\n\nFor Lucene scalar quantization, the memory required for the HNSW graph is estimated as 1.1 * (dimension + 8 * M) bytes\/vector, where M is the maximum number of bidirectional links created for each element during graph construction.\n\nFor Faiss 16-bit scalar quantization, the memory required for HNSW is estimated as 1.1 * (2 * dimension + 8 * M) bytes\/vector.\n\nFor Faiss product quantization, the memory required for HNSW is estimated as 1.1 * (((pq_code_size \/ 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_code_size is the number of bits used to encode each subvector, pq_m is the number of subvectors, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, num_segments is the number of segments, and d is the vector dimension.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: k-NN vector quantization\\nnav_order: 27\\nparent: k-NN search\\nhas_children: false\\nhas_math: true\\n---\\n\\n# k-NN vector quantization\\n\\nBy default, the k-NN plugin supports the indexing and querying of vectors of type `float`, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping `float` vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native `nmslib` and `faiss` engines). To reduce the memory footprint, you can use vector quantization.\\n\\nOpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, 16-bit scalar quantization, and product quantization (PQ).\\n\\n## Lucene byte vector\\n\\nStarting with k-NN plugin version 2.9, you can use `byte` vectors with the Lucene engine in order to reduce the amount of required memory. This requires quantizing the vectors outside of OpenSearch before ingesting them into an OpenSearch index. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#lucene-byte-vector).\\n\\n## Lucene scalar quantization\\n\\nStarting with version 2.16, the k-NN plugin supports built-in scalar quantization for the Lucene engine. Unlike the [Lucene byte vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#lucene-byte-vector), which requires you to quantize vectors before ingesting the documents, the Lucene scalar quantizer quantizes input vectors in OpenSearch during ingestion. The Lucene scalar quantizer converts 32-bit floating-point input vectors into 7-bit integer vectors in each segment using the minimum and maximum quantiles computed based on the [`confidence_interval`](#confidence-interval) parameter. During search, the query vector is quantized in each segment using the segment's minimum and maximum quantiles in order to compute the distance between the query vector and the segment's quantized input vectors. \\n\\nQuantization can decrease the memory footprint by a factor of 4 in exchange for some loss in recall. Additionally, quantization slightly increases disk usage because it requires storing both the raw input vectors and the quantized vectors.\\n\\n### Using Lucene scalar quantization\\n\\nTo use the Lucene scalar quantizer, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 2,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\"\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Confidence interval\\n\\nOptionally, you can specify the `confidence_interval` parameter in the `method.parameters.encoder` object.\\nThe `confidence_interval` is used to compute the minimum and maximum quantiles in order to quantize the vectors:\\n- If you set the `confidence_interval` to a value in the `0.9` to `1.0` range, inclusive, then the quantiles are calculated statically. For example, setting the `confidence_interval` to `0.9` specifies to compute the minimum and maximum quantiles based on the middle 90% of the vector values, excluding the minimum 5% and maximum 5% of the values. \\n- Setting `confidence_interval` to `0` specifies to compute the quantiles dynamically, which involves oversampling and additional computations performed on the input data.\\n- When `confidence_interval` is not set, it is computed based on the vector dimension $$d$$ using the formula $$max(0.9, 1 - \\\\frac{1}{1 + d})$$.\\n\\nLucene scalar quantization is applied only to `float` vectors. If you change the default value of the `data_type` parameter from `float` to `byte` or any other type when mapping a [k-NN vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector\/), then the request is rejected.\\n{: .warning}\\n\\nThe following example method definition specifies the Lucene `sq` encoder with the `confidence_interval` set to `1.0`. This `confidence_interval` specifies to consider all the input vectors when computing the minimum and maximum quantiles. Vectors are quantized to 7 bits by default:\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 2,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\",\\n              \\\"parameters\\\": {\\n                \\\"confidence_interval\\\": 1.0\\n              }\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThere are no changes to ingestion or query mapping and no range limitations for the input vectors. \\n\\n### Memory estimation\\n\\nIn the ideal scenario, 7-bit vectors created by the Lucene scalar quantizer use only 25% of the memory required by 32-bit vectors.\\n\\n#### HNSW memory estimation\\n\\nThe memory required for the Hierarchical Navigable Small World (HNSW) graph can be estimated as `1.1 * (dimension + 8 * M)` bytes\/vector, where `M` is the maximum number of bidirectional links created for each element during the construction of the graph.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * (256 + 8 * 16) * 1,000,000 ~= 0.4 GB\\n```\\n\\n## Faiss 16-bit scalar quantization \\n \\nStarting with version 2.13, the k-NN plugin supports performing scalar quantization for the Faiss engine within OpenSearch. Within the Faiss engine, a scalar quantizer (SQfp16) performs the conversion between 32-bit and 16-bit vectors. At ingestion time, when you upload 32-bit floating-point vectors to OpenSearch, SQfp16 quantizes them into 16-bit floating-point vectors and stores the quantized vectors in a k-NN index. \\n\\nAt search time, SQfp16 decodes the vector values back into 32-bit floating-point values for distance computation. The SQfp16 quantization can decrease the memory footprint by a factor of 2. Additionally, it leads to a minimal loss in recall when differences between vector values are large compared to the error introduced by eliminating their two least significant bits. When used with [SIMD optimization]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index#simd-optimization-for-the-faiss-engine), SQfp16 quantization can also significantly reduce search latencies and improve indexing throughput. \\n\\nSIMD optimization is not supported on Windows. Using Faiss scalar quantization on Windows can lead to a significant drop in performance, including decreased indexing throughput and increased search latencies.\\n{: .warning} \\n\\n### Using Faiss scalar quantization\\n\\nTo use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 3,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"faiss\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\"\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nOptionally, you can specify the parameters in `method.parameters.encoder`. For more information about `encoder` object parameters, see [SQ parameters]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index\/#sq-parameters).\\n\\nThe `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the [-65504.0, 65504.0] range. To define how to handle out-of-range values, the preceding request specifies the `clip` parameter. By default, this parameter is `false`, and any vectors containing out-of-range values are rejected. \\n\\nWhen `clip` is set to `true` (as in the preceding request), out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is `[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.\\n\\nWe recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.\\n{: .note}\\n\\nThe following example method definition specifies the Faiss SQfp16 encoder, which rejects any indexing request that contains out-of-range vector values (because the `clip` parameter is `false` by default):\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 3,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"faiss\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\",\\n              \\\"parameters\\\": {\\n                \\\"type\\\": \\\"fp16\\\"\\n              }\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nDuring ingestion, make sure each vector dimension is in the supported range ([-65504.0, 65504.0]).\\n\\n```json\\nPUT test-index\/_doc\/1\\n{\\n  \\\"my_vector1\\\": [-65504.0, 65503.845, 55.82]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nDuring querying, the query vector has no range limitation:\\n\\n```json\\nGET test-index\/_search\\n{\\n  \\\"size\\\": 2,\\n  \\\"query\\\": {\\n    \\\"knn\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"vector\\\": [265436.876, -120906.256, 99.84],\\n        \\\"k\\\": 2\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Memory estimation\\n\\nIn the best-case scenario, 16-bit vectors produced by the Faiss SQfp16 quantizer require 50% of the memory that 32-bit vectors require. \\n\\n#### HNSW memory estimation\\n\\nThe memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes\/vector.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB\\n```\\n\\n#### IVF memory estimation\\n\\nThe memory required for IVF is estimated to be `1.1 * (((2 * dimension) * num_vectors) + (4 * nlist * d))` bytes\/vector.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256 and `nlist` of 128. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * (((2 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 0.525 GB\\n```\\n\\n## Faiss product quantization\\n\\nPQ is a technique used to represent a vector in a configurable amount of bits. In general, it can be used to achieve a higher level of compression as compared to byte or scalar quantization. PQ works by separating vectors into _m_ subvectors and encoding each subvector with _code_size_ bits. Thus, the total amount of memory for the vector is `m*code_size` bits, plus overhead. For details about the parameters, see [PQ parameters]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index\/#pq-parameters). PQ is only supported for the _Faiss_ engine and can be used with either the _HNSW_ or _IVF_ approximate nearest neighbor (ANN) algorithms.\\n\\n### Using Faiss product quantization\\n\\nTo minimize loss in accuracy, PQ requires a _training_ step that builds a model based on the distribution of the data that will be searched.\\n\\nThe product quantizer is trained by running k-means clustering on a set of training vectors for each subvector space and extracts the centroids to be used for encoding. The training vectors can be either a subset of the vectors to be ingested or vectors that have the same distribution and dimension as the vectors to be ingested.\\n\\nIn OpenSearch, the training vectors need to be present in an index. In general, the amount of training data will depend on which ANN algorithm is used and how much data will be stored in the index. For IVF-based indexes, a recommended number of training vectors is `max(1000*nlist, 2^code_size * 1000)`. For HNSW-based indexes, a recommended number is `2^code_size*1000`. See the [Faiss documentation](https:\/\/github.com\/facebookresearch\/faiss\/wiki\/FAQ#how-many-training-points-do-i-need-for-k-means) for more information about the methodology used to calculate these figures.\\n\\nFor PQ, both _m_ and _code_size_ need to be selected. _m_ determines the number of subvectors into which vectors should be split for separate encoding. Consequently, the _dimension_ needs to be divisible by _m_. _code_size_ determines the number of bits used to encode each subvector. In general, we recommend a setting of `code_size = 8` and then tuning _m_ to get the desired trade-off between memory footprint and recall.\\n\\nFor an example of setting up an index with PQ, see the [Building a k-NN index from a model]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/#building-a-k-nn-index-from-a-model) tutorial.\\n\\n### Memory estimation\\n\\nWhile PQ is meant to represent individual vectors with `m*code_size` bits, in reality, the indexes consume more space. This is mainly due to the overhead of storing certain code tables and auxiliary data structures.\\n\\nSome of the memory formulas depend on the number of segments present. This is not typically known beforehand, but a recommended default value is 300.\\n{: .note}\\n\\n#### HNSW memory estimation\\n\\nThe memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size \/ 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * ((8 \/ 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB\\n```\\n\\n#### IVF memory estimation\\n\\nThe memory required for IVF with PQ is estimated to be `1.1*(((pq_code_size \/ 8) * pq_m + 24) * num_vectors  + num_segments * (2^code_size * 4 * d + 4 * ivf_nlist * d))` bytes.\\n\\nFor example, assume that you have 1 million vectors with a dimension of 256, `ivf_nlist` of 512, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1*((8 \/ 8 * 64 + 24) * 1000000  + 100 * (2^8 * 4 * 256 + 4 * 512 * 256))  ~= 0.171 GB\\n```\\n\\n\\n\"","generated_answer":"To understand the memory requirements for using HNSW, the report extract provides the following information:\n\nFor HNSW with Lucene scalar quantization (SQ):\nThe memory required can be estimated as 1.1 * (dimension + 8 * M) bytes\/vector, where M is the maximum number of bidirectional links created for each element during the construction of the graph.\n\nFor HNSW with Faiss 16-bit scalar quantization (SQfp16):\nThe memory required can be estimated as 1.1 * (2 * dimension + 8 * M) bytes\/vector.\n\nFor HNSW with Faiss product quantization (PQ):\nThe memory required can be estimated as 1.1 * (((pq_code_size \/ 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d)) bytes, where pq_m is the number of subvectors, pq_code_size is the number of bits used to encode each subvector, hnsw_m is the maximum number of bidirectional links, num_vectors is the number of vectors, and num_segments is the number of segments (typically 300).","usage":"{\"inputTokens\": 4636, \"outputTokens\": 315, \"totalTokens\": 4951}","latency":3433,"reasoning":"None","score":null,"faithfulness":0.333,"answer_relevancy":0.092,"cost":0.00155275}
{"query_text":"Can you show me some different examples of using different quantization methods for vectors?","groundtruth_answer":"<question_answer>\nThere is not enough information available to answer this question. The context provides details on different quantization methods for vectors supported by the OpenSearch k-NN plugin, such as Lucene byte vectors, Lucene scalar quantization, Faiss 16-bit scalar quantization, and Faiss product quantization. However, it does not provide specific examples of using these methods.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: k-NN vector quantization\\nnav_order: 27\\nparent: k-NN search\\nhas_children: false\\nhas_math: true\\n---\\n\\n# k-NN vector quantization\\n\\nBy default, the k-NN plugin supports the indexing and querying of vectors of type `float`, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping `float` vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native `nmslib` and `faiss` engines). To reduce the memory footprint, you can use vector quantization.\\n\\nOpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, 16-bit scalar quantization, and product quantization (PQ).\\n\\n## Lucene byte vector\\n\\nStarting with k-NN plugin version 2.9, you can use `byte` vectors with the Lucene engine in order to reduce the amount of required memory. This requires quantizing the vectors outside of OpenSearch before ingesting them into an OpenSearch index. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#lucene-byte-vector).\\n\\n## Lucene scalar quantization\\n\\nStarting with version 2.16, the k-NN plugin supports built-in scalar quantization for the Lucene engine. Unlike the [Lucene byte vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector#lucene-byte-vector), which requires you to quantize vectors before ingesting the documents, the Lucene scalar quantizer quantizes input vectors in OpenSearch during ingestion. The Lucene scalar quantizer converts 32-bit floating-point input vectors into 7-bit integer vectors in each segment using the minimum and maximum quantiles computed based on the [`confidence_interval`](#confidence-interval) parameter. During search, the query vector is quantized in each segment using the segment's minimum and maximum quantiles in order to compute the distance between the query vector and the segment's quantized input vectors. \\n\\nQuantization can decrease the memory footprint by a factor of 4 in exchange for some loss in recall. Additionally, quantization slightly increases disk usage because it requires storing both the raw input vectors and the quantized vectors.\\n\\n### Using Lucene scalar quantization\\n\\nTo use the Lucene scalar quantizer, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 2,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\"\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Confidence interval\\n\\nOptionally, you can specify the `confidence_interval` parameter in the `method.parameters.encoder` object.\\nThe `confidence_interval` is used to compute the minimum and maximum quantiles in order to quantize the vectors:\\n- If you set the `confidence_interval` to a value in the `0.9` to `1.0` range, inclusive, then the quantiles are calculated statically. For example, setting the `confidence_interval` to `0.9` specifies to compute the minimum and maximum quantiles based on the middle 90% of the vector values, excluding the minimum 5% and maximum 5% of the values. \\n- Setting `confidence_interval` to `0` specifies to compute the quantiles dynamically, which involves oversampling and additional computations performed on the input data.\\n- When `confidence_interval` is not set, it is computed based on the vector dimension $$d$$ using the formula $$max(0.9, 1 - \\\\frac{1}{1 + d})$$.\\n\\nLucene scalar quantization is applied only to `float` vectors. If you change the default value of the `data_type` parameter from `float` to `byte` or any other type when mapping a [k-NN vector]({{site.url}}{{site.baseurl}}\/field-types\/supported-field-types\/knn-vector\/), then the request is rejected.\\n{: .warning}\\n\\nThe following example method definition specifies the Lucene `sq` encoder with the `confidence_interval` set to `1.0`. This `confidence_interval` specifies to consider all the input vectors when computing the minimum and maximum quantiles. Vectors are quantized to 7 bits by default:\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 2,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"lucene\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\",\\n              \\\"parameters\\\": {\\n                \\\"confidence_interval\\\": 1.0\\n              }\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThere are no changes to ingestion or query mapping and no range limitations for the input vectors. \\n\\n### Memory estimation\\n\\nIn the ideal scenario, 7-bit vectors created by the Lucene scalar quantizer use only 25% of the memory required by 32-bit vectors.\\n\\n#### HNSW memory estimation\\n\\nThe memory required for the Hierarchical Navigable Small World (HNSW) graph can be estimated as `1.1 * (dimension + 8 * M)` bytes\/vector, where `M` is the maximum number of bidirectional links created for each element during the construction of the graph.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * (256 + 8 * 16) * 1,000,000 ~= 0.4 GB\\n```\\n\\n## Faiss 16-bit scalar quantization \\n \\nStarting with version 2.13, the k-NN plugin supports performing scalar quantization for the Faiss engine within OpenSearch. Within the Faiss engine, a scalar quantizer (SQfp16) performs the conversion between 32-bit and 16-bit vectors. At ingestion time, when you upload 32-bit floating-point vectors to OpenSearch, SQfp16 quantizes them into 16-bit floating-point vectors and stores the quantized vectors in a k-NN index. \\n\\nAt search time, SQfp16 decodes the vector values back into 32-bit floating-point values for distance computation. The SQfp16 quantization can decrease the memory footprint by a factor of 2. Additionally, it leads to a minimal loss in recall when differences between vector values are large compared to the error introduced by eliminating their two least significant bits. When used with [SIMD optimization]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index#simd-optimization-for-the-faiss-engine), SQfp16 quantization can also significantly reduce search latencies and improve indexing throughput. \\n\\nSIMD optimization is not supported on Windows. Using Faiss scalar quantization on Windows can lead to a significant drop in performance, including decreased indexing throughput and increased search latencies.\\n{: .warning} \\n\\n### Using Faiss scalar quantization\\n\\nTo use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 3,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"faiss\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\"\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nOptionally, you can specify the parameters in `method.parameters.encoder`. For more information about `encoder` object parameters, see [SQ parameters]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index\/#sq-parameters).\\n\\nThe `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the [-65504.0, 65504.0] range. To define how to handle out-of-range values, the preceding request specifies the `clip` parameter. By default, this parameter is `false`, and any vectors containing out-of-range values are rejected. \\n\\nWhen `clip` is set to `true` (as in the preceding request), out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is `[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.\\n\\nWe recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.\\n{: .note}\\n\\nThe following example method definition specifies the Faiss SQfp16 encoder, which rejects any indexing request that contains out-of-range vector values (because the `clip` parameter is `false` by default):\\n\\n```json\\nPUT \/test-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index\\\": {\\n      \\\"knn\\\": true,\\n      \\\"knn.algo_param.ef_search\\\": 100\\n    }\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"type\\\": \\\"knn_vector\\\",\\n        \\\"dimension\\\": 3,\\n        \\\"method\\\": {\\n          \\\"name\\\": \\\"hnsw\\\",\\n          \\\"engine\\\": \\\"faiss\\\",\\n          \\\"space_type\\\": \\\"l2\\\",\\n          \\\"parameters\\\": {\\n            \\\"encoder\\\": {\\n              \\\"name\\\": \\\"sq\\\",\\n              \\\"parameters\\\": {\\n                \\\"type\\\": \\\"fp16\\\"\\n              }\\n            },\\n            \\\"ef_construction\\\": 256,\\n            \\\"m\\\": 8\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nDuring ingestion, make sure each vector dimension is in the supported range ([-65504.0, 65504.0]).\\n\\n```json\\nPUT test-index\/_doc\/1\\n{\\n  \\\"my_vector1\\\": [-65504.0, 65503.845, 55.82]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nDuring querying, the query vector has no range limitation:\\n\\n```json\\nGET test-index\/_search\\n{\\n  \\\"size\\\": 2,\\n  \\\"query\\\": {\\n    \\\"knn\\\": {\\n      \\\"my_vector1\\\": {\\n        \\\"vector\\\": [265436.876, -120906.256, 99.84],\\n        \\\"k\\\": 2\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Memory estimation\\n\\nIn the best-case scenario, 16-bit vectors produced by the Faiss SQfp16 quantizer require 50% of the memory that 32-bit vectors require. \\n\\n#### HNSW memory estimation\\n\\nThe memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes\/vector.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB\\n```\\n\\n#### IVF memory estimation\\n\\nThe memory required for IVF is estimated to be `1.1 * (((2 * dimension) * num_vectors) + (4 * nlist * d))` bytes\/vector.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256 and `nlist` of 128. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * (((2 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 0.525 GB\\n```\\n\\n## Faiss product quantization\\n\\nPQ is a technique used to represent a vector in a configurable amount of bits. In general, it can be used to achieve a higher level of compression as compared to byte or scalar quantization. PQ works by separating vectors into _m_ subvectors and encoding each subvector with _code_size_ bits. Thus, the total amount of memory for the vector is `m*code_size` bits, plus overhead. For details about the parameters, see [PQ parameters]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/knn-index\/#pq-parameters). PQ is only supported for the _Faiss_ engine and can be used with either the _HNSW_ or _IVF_ approximate nearest neighbor (ANN) algorithms.\\n\\n### Using Faiss product quantization\\n\\nTo minimize loss in accuracy, PQ requires a _training_ step that builds a model based on the distribution of the data that will be searched.\\n\\nThe product quantizer is trained by running k-means clustering on a set of training vectors for each subvector space and extracts the centroids to be used for encoding. The training vectors can be either a subset of the vectors to be ingested or vectors that have the same distribution and dimension as the vectors to be ingested.\\n\\nIn OpenSearch, the training vectors need to be present in an index. In general, the amount of training data will depend on which ANN algorithm is used and how much data will be stored in the index. For IVF-based indexes, a recommended number of training vectors is `max(1000*nlist, 2^code_size * 1000)`. For HNSW-based indexes, a recommended number is `2^code_size*1000`. See the [Faiss documentation](https:\/\/github.com\/facebookresearch\/faiss\/wiki\/FAQ#how-many-training-points-do-i-need-for-k-means) for more information about the methodology used to calculate these figures.\\n\\nFor PQ, both _m_ and _code_size_ need to be selected. _m_ determines the number of subvectors into which vectors should be split for separate encoding. Consequently, the _dimension_ needs to be divisible by _m_. _code_size_ determines the number of bits used to encode each subvector. In general, we recommend a setting of `code_size = 8` and then tuning _m_ to get the desired trade-off between memory footprint and recall.\\n\\nFor an example of setting up an index with PQ, see the [Building a k-NN index from a model]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/approximate-knn\/#building-a-k-nn-index-from-a-model) tutorial.\\n\\n### Memory estimation\\n\\nWhile PQ is meant to represent individual vectors with `m*code_size` bits, in reality, the indexes consume more space. This is mainly due to the overhead of storing certain code tables and auxiliary data structures.\\n\\nSome of the memory formulas depend on the number of segments present. This is not typically known beforehand, but a recommended default value is 300.\\n{: .note}\\n\\n#### HNSW memory estimation\\n\\nThe memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size \/ 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.\\n\\nAs an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1 * ((8 \/ 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB\\n```\\n\\n#### IVF memory estimation\\n\\nThe memory required for IVF with PQ is estimated to be `1.1*(((pq_code_size \/ 8) * pq_m + 24) * num_vectors  + num_segments * (2^code_size * 4 * d + 4 * ivf_nlist * d))` bytes.\\n\\nFor example, assume that you have 1 million vectors with a dimension of 256, `ivf_nlist` of 512, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:\\n\\n```r\\n1.1*((8 \/ 8 * 64 + 24) * 1000000  + 100 * (2^8 * 4 * 256 + 4 * 512 * 256))  ~= 0.171 GB\\n```\\n\\n\\n\"","generated_answer":"Here are some examples of using different quantization methods for vectors:\n\n1. Lucene byte vector:\n   - Quantizes vectors to 8-bit (byte) values before ingesting them into the OpenSearch index.\n   - Reduces the memory footprint but requires quantizing the vectors outside of OpenSearch.\n\n2. Lucene scalar quantization (SQ):\n   - Quantizes 32-bit floating-point input vectors to 7-bit integer vectors during ingestion.\n   - Reduces the memory footprint by a factor of 4 in exchange for some loss in recall.\n\n3. Faiss 16-bit scalar quantization (SQfp16):\n   - Quantizes 32-bit floating-point vectors to 16-bit floating-point vectors during ingestion.\n   - Reduces the memory footprint by a factor of 2 with minimal loss in recall.\n\n4. Faiss product quantization (PQ):\n   - Splits vectors into subvectors and encodes each subvector with a configurable number of bits.\n   - Provides a higher level of compression compared to byte or scalar quantization, but requires a training step to build a model based on the data distribution.","usage":"{\"inputTokens\": 4639, \"outputTokens\": 286, \"totalTokens\": 4925}","latency":3620,"reasoning":"None","score":null,"faithfulness":0.167,"answer_relevancy":0.1,"cost":0.00151725}
{"query_text":"Would you recommend I use ReRank? If so, what type of model would you recommend for ReRank? ","groundtruth_answer":"<question_answer>\nBased on the information provided, I would recommend using ReRank with a cross-encoder model for improving search relevance. The report suggests configuring a search pipeline with the \"rerank\" processor that uses a cross-encoder model to rerank the search results. The specific model to use would depend on your use case, but the report mentions using either an OpenSearch-provided cross-encoder model or a custom local model.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Reranking search results\\nparent: Search relevance\\nhas_children: false\\nnav_order: 60\\n---\\n\\n# Reranking search results\\nIntroduced 2.12\\n{: .label .label-purple }\\n\\nYou can rerank search results using a cross-encoder reranker in order to improve search relevance. To implement reranking, you need to configure a [search pipeline]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/index\/) that runs at search time. The search pipeline intercepts search results and applies the [`rerank` processor]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/rerank-processor\/) to them. The `rerank` processor evaluates the search results and sorts them based on the new scores provided by the cross-encoder model. \\n\\n**PREREQUISITE**<br>\\nBefore configuring a reranking pipeline, you must set up a cross-encoder model. For information about using an OpenSearch-provided model, see [Cross-encoder models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/pretrained-models\/#cross-encoder-models). For information about using a custom model, see [Custom local models]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/custom-local-models\/).\\n{: .note}\\n\\n## Running a search with reranking\\n\\nTo run a search with reranking, follow these steps:\\n\\n1. [Configure a search pipeline](#step-1-configure-a-search-pipeline).\\n1. [Create an index for ingestion](#step-2-create-an-index-for-ingestion).\\n1. [Ingest documents into the index](#step-3-ingest-documents-into-the-index).\\n1. [Search using reranking](#step-4-search-using-reranking).\\n\\n## Step 1: Configure a search pipeline\\n\\nNext, configure a search pipeline with a [`rerank` processor]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/rerank-processor\/).\\n\\nThe following example request creates a search pipeline with an `ml_opensearch` rerank processor. In the request, provide a model ID for the cross-encoder model and the document fields to use as context:\\n\\n```json\\nPUT \/_search\/pipeline\/my_pipeline\\n{\\n  \\\"description\\\": \\\"Pipeline for reranking with a cross-encoder\\\",\\n  \\\"response_processors\\\": [\\n    {\\n      \\\"rerank\\\": {\\n        \\\"ml_opensearch\\\": {\\n          \\\"model_id\\\": \\\"gnDIbI0BfUsSoeNT_jAw\\\"\\n        },\\n        \\\"context\\\": {\\n          \\\"document_fields\\\": [\\n            \\\"passage_text\\\"\\n          ]\\n        }\\n      }\\n    }\\n  ]\\n}\\n```\\n{% include copy-curl.html %}\\n\\nFor more information about the request fields, see [Request fields]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/rerank-processor\/#request-fields).\\n\\n## Step 2: Create an index for ingestion\\n\\nIn order to use the rerank processor defined in your pipeline, create an OpenSearch index and add the pipeline created in the previous step as the default pipeline:\\n\\n```json\\nPUT \/my-index\\n{\\n  \\\"settings\\\": {\\n    \\\"index.search.default_pipeline\\\" : \\\"my_pipeline\\\"\\n  },\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"passage_text\\\": {\\n        \\\"type\\\": \\\"text\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Step 3: Ingest documents into the index\\n\\nTo ingest documents into the index created in the previous step, send the following bulk request:\\n\\n```json\\nPOST \/_bulk\\n{ \\\"index\\\": { \\\"_index\\\": \\\"my-index\\\" } }\\n{ \\\"passage_text\\\" : \\\"I said welcome to them and we entered the house\\\" }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"my-index\\\" } }\\n{ \\\"passage_text\\\" : \\\"I feel welcomed in their family\\\" }\\n{ \\\"index\\\": { \\\"_index\\\": \\\"my-index\\\" } }\\n{ \\\"passage_text\\\" : \\\"Welcoming gifts are great\\\" }\\n\\n```\\n{% include copy-curl.html %}\\n\\n## Step 4: Search using reranking\\n\\nTo perform reranking search on your index, use any OpenSearch query and provide an additional `ext.rerank` field:\\n\\n```json\\nPOST \/my-index\/_search\\n{\\n  \\\"query\\\": {\\n    \\\"match\\\": {\\n      \\\"passage_text\\\": \\\"how to welcome in family\\\"\\n    }\\n  },\\n  \\\"ext\\\": {\\n    \\\"rerank\\\": {\\n      \\\"query_context\\\": {\\n         \\\"query_text\\\": \\\"how to welcome in family\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nAlternatively, you can provide the full path to the field containing the context. For more information, see [Rerank processor example]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/rerank-processor\/#example).\\n\\n## Using rerank and normalization processors together\\n\\nWhen you use a rerank processor in conjunction with a [normalization processor]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/normalization-processor\/) and a hybrid query, the rerank processor alters the final document scores. This is because the rerank processor operates after the normalization processor in the search pipeline.\\n{: .note}\\n\\nThe processing order is as follows: \\n\\n- Normalization processor: This processor normalizes the document scores based on the configured normalization method. For more information, see [Normalization processor]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/normalization-processor\/).\\n- Rerank processor: Following normalization, the rerank processor further adjusts the document scores. This adjustment can significantly impact the final ordering of search results.\\n\\nThis processing order has the following implications:\\n\\n- Score modification: The rerank processor modifies the scores that were initially adjusted by the normalization processor, potentially leading to different ranking results than initially expected.\\n- Hybrid queries: In the context of hybrid queries, where multiple types of queries and scoring mechanisms are combined, this behavior is particularly noteworthy. The combined scores from the initial query are normalized first and then reranked, resulting in a two-stage scoring modification.\\n\\n\"","generated_answer":"Based on the information provided in the context, I would recommend using ReRank. The context indicates that ReRank can improve search relevance by applying a cross-encoder reranker to the search results. For the type of model, the context suggests using an OpenSearch-provided cross-encoder model or a custom local model. The specific model to recommend would depend on the requirements and performance of the available models.","usage":"{\"inputTokens\": 1709, \"outputTokens\": 100, \"totalTokens\": 1809}","latency":2292,"reasoning":"None","score":null,"faithfulness":0.25,"answer_relevancy":0.115,"cost":0.00055225}
{"query_text":"How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?","groundtruth_answer":"<question_answer>\nTo set up a tiered cache with an on-heap and disk tier for the index request cache:\n\n1. Enable the experimental pluggable caching feature by setting `opensearch.experimental.feature.pluggable.caching.enabled: true` in opensearch.yml.\n2. Install the `cache-ehcache` plugin to provide the disk cache implementation.\n3. Set the cache store name to `tiered_spillover`: `indices.request.cache.store.name: tiered_spillover`\n4. Set the on-heap and disk store tiers: `indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap` and `indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk`\n5. Configure the on-heap and disk store settings, such as size, expiration, and disk storage path.\n\nTo prevent the tiered cache from consuming excessive resources, you can:\n- Set appropriate cache sizes for the on-heap and disk tiers based on your workload and available resources.\n- Configure cache expiration policies to evict stale entries.\n- Monitor cache usage and performance using the Node Stats API and adjust settings as needed.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Tiered cache\\nparent: Caching\\ngrand_parent: Improving search performance\\nnav_order: 10\\n---\\n\\n# Tiered cache\\n\\nThis is an experimental feature and is not recommended for use in a production environment. For updates on the progress of the feature or if you want to leave feedback, see the associated [GitHub issue](https:\/\/github.com\/opensearch-project\/OpenSearch\/issues\/10024).    \\n{: .warning}\\n\\nA tiered cache is a multi-level cache in which each tier has its own characteristics and performance levels. By combining different tiers, you can achieve a balance between cache performance and size.\\n\\n## Types of tiered caches\\n\\nOpenSearch provides an implementation of a `_tiered` spillover `cache_`. This implementation spills any items removed from the upper tiers to the lower tiers of cache. The upper tier, such as the on-heap tier, is smaller in size but offers better latency. The lower tier, such as the disk cache, is larger in size but slower in terms of latency. OpenSearch offers both on-heap and disk tiers. \\n\\n## Enabling a tiered cache\\n\\nTo enable a tiered cache, configure the following setting in `opensearch.yml`:\\n\\n```yaml\\nopensearch.experimental.feature.pluggable.caching.enabled: true\\n```\\n{% include copy.html %}\\n\\nFor more information about ways to enable experimental features, see [Experimental feature flags]({{site.url}}{{site.baseurl}}\/install-and-configure\/configuring-opensearch\/experimental\/).\\n\\n## Installing required plugins\\n\\nTo use tiered caching, install a tiered cache plugin. As of OpenSearch 2.13, the only available cache plugin is the `cache-ehcache` plugin. This plugin provides a disk cache implementation that can be used as a disk tier within a tiered cache. For more information about installing non-bundled plugins, see [Additional plugins]({{site.url}}{{site.baseurl}}\/install-and-configure\/plugins\/#additional-plugins).\\n\\nA tiered cache will fail to initialize if the `cache-ehcache` plugin is not installed or if disk cache properties are not set. \\n{: .warning}\\n\\n## Tiered cache settings\\n\\nIn OpenSearch 2.14, a request cache can be used in a tiered cache. To begin, configure the following settings in the `opensearch.yml` file.\\n\\n### Cache store name\\n\\nTo use the OpenSearch-provided tiered spillover cache implementation, set the cache store name to `tiered_spillover`, as shown in the following example:\\n\\n```yaml\\nindices.request.cache.store.name: tiered_spillover\\n```\\n{% include copy.html %}\\n\\n### Setting on-heap and disk store tiers\\n\\nSet the on-heap and disk store tiers to `opensearch_onheap` and `ehcache_disk`, as shown in the following example:\\n\\n```yaml\\nindices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap\\nindices.request.cache.tiered_spillover.disk.store.name: ehcache_disk\\n```\\nThe `opensearch_onheap` setting uses the built-in on-heap cache available in OpenSearch. \\n\\nThe `ehcache_disk` setting is the disk cache implementation from [Ehcache](https:\/\/www.ehcache.org\/) and requires installing the `cache-ehcache` plugin.\\n\\n{% include copy.html %}\\n\\n### Configuring on-heap and disk stores\\n\\nThe following table lists the cache store settings for the `opensearch_onheap` store.\\n\\nSetting | Data type | Default | Description\\n:--- | :--- | :--- | :---\\n`indices.request.cache.opensearch_onheap.size` | Percentage | 1% of the heap size | The size of the on-heap cache. Optional.\\n`indices.request.cache.opensearch_onheap.expire` | Time unit | `MAX_VALUE` (disabled) | Specifies a time-to-live (TTL) for the cached results. Optional.\\n\\nThe following table lists the disk cache store settings for the `ehcache_disk` store.\\n\\nSetting | Data type | Default | Description\\n:--- | :--- | :--- | :---\\n`indices.request.cache.ehcache_disk.max_size_in_bytes` | Long | `1073741824` (1 GB)  | Defines the size of the disk cache. Optional.\\n`indices.request.cache.ehcache_disk.storage.path` | String | `\\\"\\\"` | Defines the storage path for the disk cache. Required.\\n`indices.request.cache.ehcache_disk.expire_after_access` | Time unit | `MAX_VALUE` (disabled) | Specifies a TTL for the cached results. Optional.\\n`indices.request.cache.ehcache_disk.alias` | String | `ehcacheDiskCache#INDICES_REQUEST_CACHE` | Specifies an alias for the disk cache. Optional.\\n`indices.request.cache.ehcache_disk.segments` | Integer | `16` | Defines the number of segments into which the disk cache is separated. Used for concurrency. Optional.\\n`indices.request.cache.ehcache_disk.concurrency` | Integer | `1` | Defines the number of distinct write queues created for the disk store, where a group of segments shares a write queue. Optional.\\n\\n### Additional settings for the `tiered_spillover` store\\n\\nThe following table lists additional settings for the `tiered_spillover` store setting.\\n\\nSetting | Data type | Default | Description\\n:--- | :--- | :--- | :---\\n`indices.request.cache.tiered_spillover.disk.store.policies.took_time.threshold` | Time unit | `10ms` | A policy used to determine whether to cache a query into a disk cache based on its took time. This is a dynamic setting. Optional.\\n`indices.request.cache.tiered_spillover.disk.store.enabled` | Boolean | `True` | Enables or disables the disk cache dynamically within a tiered spillover cache. Note: After disabling a disk cache, entries are not removed automatically and requires the cache to be manually cleared. Optional.\\n\\n### Delete stale entries settings\\n\\nThe following table lists the settings related to the deletion of stale entries from the cache.\\n\\nSetting | Data type | Default | Description\\n:--- | :--- |:--------| :---\\n`indices.requests.cache.cleanup.staleness_threshold` | String | `0%`    | Defines the percentage of stale keys in the cache post. After identification, all stale cache entries are deleted. Optional.\\n`indices.requests.cache.cleanup.interval` | Time unit | `1m`  | Defines the frequency at which the request cache's stale entries are deleted. Optional.\\n\\n## Getting statistics for the `tiered_spillover` store \\n\\nTo assess the impact of using the tiered spillover cache, use the [Node Stats API]({{site.url}}{{site.baseurl}}\/api-reference\/nodes-apis\/nodes-stats\/#caches), as shown in the following example: \\n\\n```json\\nGET \/_nodes\/stats\/caches\/request_cache?level=tier\\n```\\n\\n\\n\\n---\\nlayout: default\\ntitle: Index request cache\\nparent: Caching\\ngrand_parent: Improving search performance\\nnav_order: 5\\n---\\n\\n# Index request cache\\n\\nThe OpenSearch index request cache is a specialized caching mechanism designed to enhance search performance by storing the results of frequently executed search queries at the shard level. This reduces cluster load and improves response times for repeated searches. This cache is enabled by default and is particularly useful for read-heavy workloads where certain queries are executed frequently.\\n\\nThe cache is automatically invalidated at the configured refresh interval. The invalidation includes document updates (including document deletions) and changes to index settings. This ensures that stale results are never returned from the cache. When the cache size exceeds its configured limit, the least recently used entries are evicted to make room for new entries.\\n\\nSearch requests with `size=0` are cached in the request cache by default. Search requests with non-deterministic characteristics (such as `Math.random()`) or relative times (such as `now` or `new Date()`) are ineligible for caching.\\n{: .note}\\n\\n## Configuring request caching\\n\\nYou can configure the index request cache by setting the parameters in the `opensearch.yml` configuration file or using the REST API. For more information, see [Index settings]({{site.url}}{{site.baseurl}}\/install-and-configure\/configuring-opensearch\/index-settings\/).\\n\\n### Settings\\n\\nThe following table lists the index request cache settings. For more information about dynamic settings, see [Index settings]({{site.url}}{{site.baseurl}}\/install-and-configure\/configuring-opensearch\/index-settings\/).\\n\\nSetting | Data type  | Default | Level | Static\/Dynamic | Description\\n:--- |:-----------|:--------| :--- | :--- | :---\\n`indices.cache.cleanup_interval` | Time unit  | `1m` (1 minute)  | Cluster | Static | Schedules a recurring background task that cleans up expired entries from the cache at the specified interval. \\n`indices.requests.cache.size` | Percentage | `1%`      | Cluster | Static | The cache size as a percentage of the heap size (for example, to use 1% of the heap, specify `1%`). \\n`index.requests.cache.enable` | Boolean    | `true`    | Index | Dynamic | Enables or disables the request cache. \\n\\n### Example\\n\\nTo disable the request cache for an index, send the following request:\\n\\n```json\\nPUT \/my_index\/_settings\\n{\\n  \\\"index.requests.cache.enable\\\": false\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Caching specific requests\\n\\nIn addition to providing index-level or cluster-level settings for the request cache, you can also cache specific search requests selectively by setting the `request_cache` query parameter to `true`:\\n\\n```json\\nGET \/students\/_search?request_cache=true\\n{\\n  \\\"query\\\": {\\n    \\\"match\\\": {\\n      \\\"name\\\": \\\"doe john\\\"\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n## Monitoring the request cache\\n\\nMonitoring cache usage and performance is crucial to maintaining an efficient caching strategy. OpenSearch provides several APIs to help monitor the cache.\\n\\n### Retrieving cache statistics for all nodes\\n\\nThe [Nodes Stats API]({{site.url}}{{site.baseurl}}\/api-reference\/nodes-apis\/nodes-stats\/) returns cache statistics for all nodes in a cluster:\\n\\n```json\\nGET \/_nodes\/stats\/indices\/request_cache\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the request cache statistics:\\n\\n```json\\n{\\n  \\\"nodes\\\": {\\n    \\\"T7aqO6zaQX-lt8XBWBYLsA\\\": {\\n      \\\"indices\\\": {\\n        \\\"request_cache\\\": {\\n          \\\"memory_size_in_bytes\\\": 10240,\\n          \\\"evictions\\\": 0,\\n          \\\"hit_count\\\": 50,\\n          \\\"miss_count\\\": 10\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n### Retrieving cache statistics for a specific index\\n\\nThe [Index Stats API]({{site.url}}{{site.baseurl}}\/api-reference\/index-apis\/stats\/) returns cache statistics for a specific index:\\n\\n```json\\nGET \/my_index\/_stats\/request_cache\\n```\\n{% include copy-curl.html %}\\n\\nThe response contains the request cache statistics:\\n\\n```json\\n{\\n  \\\"_shards\\\": {\\n    \\\"total\\\": 5,\\n    \\\"successful\\\": 5,\\n    \\\"failed\\\": 0\\n  },\\n  \\\"_all\\\": {\\n    \\\"primaries\\\": {\\n      \\\"request_cache\\\": {\\n        \\\"memory_size_in_bytes\\\": 2048,\\n        \\\"evictions\\\": 1,\\n        \\\"hit_count\\\": 30,\\n        \\\"miss_count\\\": 5\\n      }\\n    },\\n    \\\"total\\\": {\\n      \\\"request_cache\\\": {\\n        \\\"memory_size_in_bytes\\\": 4096,\\n        \\\"evictions\\\": 2,\\n        \\\"hit_count\\\": 60,\\n        \\\"miss_count\\\": 10\\n      }\\n    }\\n  },\\n  \\\"indices\\\": {\\n    \\\"my_index\\\": {\\n      \\\"primaries\\\": {\\n        \\\"request_cache\\\": {\\n          \\\"memory_size_in_bytes\\\": 2048,\\n          \\\"evictions\\\": 1,\\n          \\\"hit_count\\\": 30,\\n          \\\"miss_count\\\": 5\\n        }\\n      },\\n      \\\"total\\\":{\\n        \\\"request_cache\\\": {\\n          \\\"memory_size_in_bytes\\\": 4096,\\n          \\\"evictions\\\": 2,\\n          \\\"hit_count\\\": 60,\\n          \\\"miss_count\\\": 10\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\n## Best practices\\n\\nWhen using the index request cache, consider the following best practices:\\n\\n- **Appropriate cache size**: Configure the cache size based on your query patterns. A larger cache can store more results but may consume significant resources.\\n- **Query optimization**: Ensure that frequently executed queries are optimized so that they can benefit from caching.\\n- **Monitoring**: Regularly monitor cache hit and cache miss rates to understand cache efficiency and make necessary adjustments.\\n\\n\"","generated_answer":"To set up a tiered cache with an on-heap and disk tier for your index request cache, you can follow these steps:\n\n1. Enable the experimental feature for pluggable caching in your `opensearch.yml` file:\n```yaml\nopensearch.experimental.feature.pluggable.caching.enabled: true\n```\n\n2. Install the `cache-ehcache` plugin, which provides the disk cache implementation for the tiered cache.\n\n3. Configure the following settings in your `opensearch.yml` file:\n```yaml\nindices.request.cache.store.name: tiered_spillover\nindices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap\nindices.request.cache.tiered_spillover.disk.store.name: ehcache_disk\n```\n\n4. Customize the settings for the on-heap and disk tiers:\n   - For the on-heap tier (`opensearch_onheap`), you can set the cache size as a percentage of the heap size using the `indices.request.cache.opensearch_onheap.size` setting.\n   - For the disk tier (`ehcache_disk`), you can set the maximum size in bytes using the `indices.request.cache.ehcache_disk.max_size_in_bytes` setting and the storage path using the `indices.request.cache.ehcache_disk.storage.path` setting.\n\nTo ensure the tiered cache doesn't eat all your resources, you can adjust the following settings:\n- `indices.request.cache.opensearch_onheap.size`: Set this to a reasonable percentage of your heap size, such as 1%.\n- `indices.request.cache.ehcache_disk.max_size_in_bytes`: Set this to a size that fits your disk capacity and expected cache usage.\n- `indices.requests.cache.cleanup.staleness_threshold`: Set this to a non-zero value to automatically delete stale cache entries.\n- `indices.requests.cache.cleanup.interval`: Adjust the frequency at which stale entries are deleted.","usage":"{\"inputTokens\": 3352, \"outputTokens\": 493, \"totalTokens\": 3845}","latency":5712,"reasoning":"None","score":null,"faithfulness":0.545,"answer_relevancy":0.109,"cost":0.00145425}
{"query_text":"I need to combine relevance scores from different types of searches.","groundtruth_answer":"<question_answer>\nA hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. It contains an array of one or more query clauses, and the relevance scores from each clause are combined using a search pipeline.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Hybrid\\nparent: Compound queries\\nnav_order: 70\\n---\\n\\n# Hybrid query\\n\\nYou can use a hybrid query to combine relevance scores from multiple queries into one score for a given document. A hybrid query contains a list of one or more queries and independently calculates document scores at the shard level for each subquery. The subquery rewriting is performed at the coordinating node level in order to avoid duplicate computations.\\n\\n## Example\\n\\nLearn how to use the `hybrid` query by following the steps in [Using hybrid search]({{site.url}}{{site.baseurl}}\/search-plugins\/hybrid-search\/#using-hybrid-search).\\n\\nFor a comprehensive example, follow the [Neural search tutorial]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/semantic-search#tutorial).\\n\\n## Parameters\\n\\nThe following table lists all top-level parameters supported by `hybrid` queries.\\n\\nParameter | Description\\n:--- | :---\\n`queries` | An array of one or more query clauses that are used to match documents. A document must match at least one query clause in order to be returned in the results. The documents' relevance scores from all query clauses are combined into one score by applying a [search pipeline]({{site.url}}{{site.baseurl}}\/search-plugins\/search-pipelines\/index\/). The maximum number of query clauses is 5. Required.\\n\\n## Disabling hybrid queries\\n\\nBy default, hybrid queries are enabled. To disable hybrid queries in your cluster, set the `plugins.neural_search.hybrid_search_disabled` setting to `true` in `opensearch.yml`. \\n\\n\"","generated_answer":"According to the context provided, to combine relevance scores from different types of searches, you can use the \"hybrid\" query. The hybrid query allows you to combine relevance scores from multiple queries into one score for a given document. The hybrid query contains a list of one or more queries and independently calculates document scores at the shard level for each subquery.","usage":"{\"inputTokens\": 570, \"outputTokens\": 89, \"totalTokens\": 659}","latency":1075,"reasoning":"None","score":null,"faithfulness":0.667,"answer_relevancy":0.125,"cost":0.00025375}
{"query_text":"What metrics does query metrics give me out of the box? ","groundtruth_answer":"<question_answer>\nAccording to the report, query metrics provide the following out-of-the-box metrics:\n\n- Number of queries per query type (e.g. match, regex)\n- Number of queries per aggregation type (e.g. terms)\n- Number of queries per sort order (e.g. ascending, descending)\n- Histograms of latency for each query type, aggregation type, and sort order\n- Histograms of CPU usage for each query type, aggregation type, and sort order\n- Histograms of memory usage for each query type, aggregation type, and sort order\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Query metrics\\nparent: Query insights\\nnav_order: 20\\n---\\n\\n# Query metrics\\n\\nKey query [metrics](#metrics), such as aggregation types, query types, latency, and resource usage per query type, are captured along the search path by using the OpenTelemetry (OTel) instrumentation framework. The telemetry data can be consumed using OTel metrics [exporters]({{site.url}}{{site.baseurl}}\/observing-your-data\/trace\/distributed-tracing\/#exporters).\\n\\n## Configuring query metric generation\\n\\nTo configure query metric generation, use the following steps.\\n\\n### Step 1: Install the Query Insights plugin\\n\\nFor information about installing the Query Insights plugin, see [Installing the Query Insights plugin]({{site.url}}{{site.baseurl}}\/observing-your-data\/query-insights\/index\/#installing-the-query-insights-plugin).\\n\\n### Step 2: Install the OpenTelemetry plugin\\n\\nFor information about installing the OpenTelemetry plugin, see [Distributed tracing]({{site.url}}{{site.baseurl}}\/observing-your-data\/trace\/distributed-tracing\/).\\n\\n### Step 3: Enable query metrics\\n\\nEnable query metrics by configuring the following `opensearch.yml` settings:\\n\\n```yaml\\ntelemetry.feature.metrics.enabled: true\\nsearch.query.metrics.enabled: true\\n```\\n{% include copy.html %}\\n\\nThe following is a complete sample configuration that includes a telemetry configuration:\\n\\n```yaml\\n# Enable query metrics feature\\nsearch.query.metrics.enabled: true\\ntelemetry.feature.metrics.enabled: true\\n\\n# OTel-related configuration\\nopensearch.experimental.feature.telemetry.enabled: true\\ntelemetry.tracer.sampler.probability: 1.0\\ntelemetry.feature.tracer.enabled: true\\n```\\n{% include copy.html %}\\n\\nAlternatively, you can configure query metric generation using the API:\\n\\n```json\\nPUT _cluster\/settings\\n{\\n  \\\"persistent\\\" : {\\n    \\\"search.query.metrics.enabled\\\" : true\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nConfigure the export of metrics and traces using a gRPC exporter. For more information, see [Exporters]({{site.url}}{{site.baseurl}}\/observing-your-data\/trace\/distributed-tracing\/#exporters). You can skip this step if you use the [default logging exporter](#default-logging-exporter):\\n\\n```yaml\\ntelemetry.otel.tracer.span.exporter.class: io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter\\ntelemetry.otel.metrics.exporter.class: io.opentelemetry.exporter.otlp.metrics.OtlpGrpcMetricExporter\\n```\\n{% include copy.html %}\\n\\n## Metrics\\n\\nQuery metrics provide the following measurements:\\n\\n- The number of queries per query type (for example, the number of `match` or `regex` queries)\\n- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)\\n- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)\\n- Histograms of `latency` for each query type, aggregation type, and sort order\\n- Histograms of `cpu` for each query type, aggregation type, and sort order\\n- Histograms of `memory` for each query type, aggregation type, and sort order\\n\\n## Default logging exporter\\n\\nBy default, if no gRPC exporters are configured, then the metrics and traces are exported to log files. The data is saved in the `opensearch\/logs` directory in the following files:\\n\\n- `opensearch_otel_metrics.log`\\n- `opensearch_otel_traces.log`\\n\\n\\n\"","generated_answer":"The query metrics provide the following out-of-the-box metrics:\n- Number of queries per query type (e.g. match, regex)\n- Number of queries per aggregation type (e.g. terms)\n- Number of queries per sort order (e.g. ascending, descending)\n- Histograms of latency, CPU, and memory usage for each query type, aggregation type, and sort order","usage":"{\"inputTokens\": 1106, \"outputTokens\": 108, \"totalTokens\": 1214}","latency":1340,"reasoning":"None","score":null,"faithfulness":0.25,"answer_relevancy":0.109,"cost":0.0004115}
{"query_text":"Does opensearch work with open telemetry?","groundtruth_answer":"<question_answer>There is not enough information available to answer this question<\/question_answer>\n\nThe context provided information about OpenSearch's Metric Analytics feature, including how to ingest and visualize metrics from OpenTelemetry and Prometheus. However, it did not explicitly mention whether OpenSearch works with OpenTelemetry or not.","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Metric analytics\\nnav_order: 40\\n---\\n\\n# Metric analytics\\nIntroduced 2.4\\n{: .label .label-purple }\\n\\nWith the release of OpenSearch 2.4, you can now ingest and visualize metric data stored directly in OpenSearch using the **Metrics** tool. This equips you with tools to analyze and correlate data across logs, traces, and metrics.\\n\\nBefore the introduction of this feature, you could only ingest and visualize logs and traces from your monitored environments. With the **Metrics** tool, you can now observe your digital assets with more granularity, gain deeper insight into the health of your infrastructure, and better inform your root cause analysis.\\n\\nThe **Metrics** tool offers federated visualization capabilities in addition to the following:\\n\\n - An OpenSearch cluster containing an [OpenTelemetry (OTel)-compatible metrics index](https:\/\/github.com\/opensearch-project\/opensearch-catalog\/tree\/main\/docs\/schema\/observability\/metrics) with OTel-based signals. See [What is OpenTelemetry?](https:\/\/opentelemetry.io\/docs\/what-is-opentelemetry\/) for an overview of OTel.\\n - An OpenSearch cluster containing a [Prometheus data source](https:\/\/github.com\/opensearch-project\/sql\/blob\/main\/docs\/dev\/datasource-prometheus.md) connected to a Prometheus server. \\n\\nThe following image displays the flow for retrieving metrics from Prometheus and displaying them on a visualization dashboard.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/prom-metrics.png\\\" alt=\\\"Prometheus data source\\\" width=\\\"700\\\"\/>\\n\\nThe following image displays an observability dashboard that visualizes metric data from the OpenSearch index using OTel queries.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/otel-metrics.png\\\" alt=\\\"OTel data source\\\" width=\\\"700\\\"\/>\\n\\n---\\n\\n## Configuring Prometheus to send metric data to OpenSearch\\n\\nYou must first create a connection from [Prometheus](https:\/\/prometheus.io\/) to OpenSearch using the [SQL plugin](https:\/\/github.com\/opensearch-project\/sql). You can then configure a connection to Prometheus by using the `_datasources` API endpoint. \\n\\nThe following example shows a request that configures a Prometheus data source without any authentication:\\n\\n```json\\nPOST _plugins\/_query\/_datasources \\n{\\n    \\\"name\\\" : \\\"my_prometheus\\\",\\n    \\\"connector\\\": \\\"prometheus\\\",\\n    \\\"properties\\\" : {\\n        \\\"prometheus.uri\\\" : \\\"http:\/\/localhost:9090\\\"\\n    }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nThe following example shows how to configure a Prometheus data source using AWS Signature Version 4 authentication:\\n\\n```json\\nPOST _plugins\/_query\/_datasources\\n{\\n    \\\"name\\\" : \\\"my_prometheus\\\",\\n    \\\"connector\\\": \\\"prometheus\\\",\\n    \\\"properties\\\" : {\\n        \\\"prometheus.uri\\\" : \\\"http:\/\/localhost:8080\\\",\\n        \\\"prometheus.auth.type\\\" : \\\"awssigv4\\\",\\n        \\\"prometheus.auth.region\\\" : \\\"us-east-1\\\",\\n        \\\"prometheus.auth.access_key\\\" : \\\"{{accessKey}}\\\"\\n        \\\"prometheus.auth.secret_key\\\" : \\\"{{secretKey}}\\\"\\n    }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nAfter configuring the connection, you can view Prometheus metrics in OpenSearch Dashboards by going to the **Observability** > **Metrics** page, as shown in the following image.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/metrics1.png\\\" alt=\\\"Prometheus metrics displayed on a dashboard\\\" width=\\\"700\\\"\/>\\n\\n### Developer resources\\n\\nSee the following developer resources for sample code, articles, tutorials, and API references:\\n\\n* [Datasource Settings](https:\/\/github.com\/opensearch-project\/sql\/blob\/main\/docs\/user\/ppl\/admin\/datasources.rst), which contains information about authentication and authorization of data source APIs.\\n* [Prometheus Connector](https:\/\/github.com\/opensearch-project\/sql\/blob\/main\/docs\/user\/ppl\/admin\/connectors\/prometheus_connector.rst), which contains configuration information.\\n* [Simple Schema for Observability](https:\/\/github.com\/opensearch-project\/opensearch-catalog\/tree\/main\/docs\/schema\/observability), which contains information about the OTel schema and ingest pipeline.\\n* [OTel Metrics Source](https:\/\/github.com\/opensearch-project\/data-prepper\/tree\/main\/data-prepper-plugins\/otel-metrics-source), which contains information about the Data Prepper metrics pipeline and ingestion.\\n\\n---\\n\\n## Experimenting with OpenTelemetry Metrics in the OpenSearch demo environment \\n\\nThe OpenSearch [`opentelemetry-demo` repository](https:\/\/github.com\/opensearch-project\/opentelemetry-demo) provides a practical demonstration of collecting, processing, and visualizing metric data through **OpenTelemetry Metrics** from OpenTelemetry and using the **Metrics** tool in OpenSearch Dashboards.\\n\\n### Visualizing OTel metrics in OpenSearch\\n\\nTo visualize OTel metric data in OpenSearch, follow these steps: \\n\\n1. Install the [`opentelemetry-demo` repository](https:\/\/github.com\/opensearch-project\/opentelemetry-demo). See the [Getting Started](https:\/\/github.com\/opensearch-project\/opentelemetry-demo\/blob\/main\/tutorial\/GettingStarted.md) guide for instructions.\\n2. Collect the OTel signals, including metric signals. See the [OTel Collector](https:\/\/opentelemetry.io\/docs\/collector\/) guide for instructions.  \\n3. Configure the OTel pipeline to emit metric signals. See the [OTel Collector Pipeline](https:\/\/github.com\/opensearch-project\/opentelemetry-demo\/tree\/main\/src\/otelcollector) guide for instructions.\\n\\n#### Example YAML config file\\n\\n```yaml\\n    service:\\n      extensions: [basicauth\/client]\\n      pipelines:\\n        traces:\\n          receivers: [otlp]\\n          processors: [batch]\\n          exporters: [otlp, debug, spanmetrics, otlp\/traces, opensearch\/traces]\\n        metrics:\\n          receivers: [otlp, spanmetrics]\\n          processors: [filter\/ottl, transform, batch]\\n          exporters: [otlphttp\/prometheus, otlp\/metrics, debug]\\n        logs:\\n          receivers: [otlp]\\n          processors: [batch]\\n          exporters: [otlp\/logs,  opensearch\/logs, debug]\\n```\\n{% include copy-curl.html %}\\n    \\n4. Configure the [Data Prepper pipeline](https:\/\/github.com\/opensearch-project\/opentelemetry-demo\/blob\/main\/src\/dataprepper\/pipelines.yaml) to emit the collected metric signals into the OpenSearch metrics index.\\n\\n#### Example YAML config file\\n\\n```yaml\\n    otel-metrics-pipeline:\\n      workers: 8\\n      delay: 3000\\n      source:\\n        otel_metrics_source:\\n          health_check_service: true\\n          ssl: false\\n      buffer:\\n        bounded_blocking:\\n          buffer_size: 1024 # max number of records the buffer accepts\\n          batch_size: 1024 # max number of records the buffer drains after each read\\n      processor:\\n        - otel_metrics:\\n            calculate_histogram_buckets: true\\n            calculate_exponential_histogram_buckets: true\\n            exponential_histogram_max_allowed_scale: 10\\n            flatten_attributes: false\\n      sink:\\n        - opensearch:\\n            hosts: [\\\"https:\/\/opensearch-node1:9200\\\"]\\n            username: \\\"admin\\\"\\n            password: \\\"my_%New%_passW0rd!@#\\\"\\n            insecure: true\\n            index_type: custom\\n            template_file: \\\"templates\/ss4o_metrics.json\\\"\\n            index: ss4o_metrics-otel-%{yyyy.MM.dd}\\n            bulk_size: 4\\n```\\n{% include copy-curl.html %}\\n\\n5. Ingest metric data into OpenSearch. As the demo starts generating data, the metric signals will be added to the OpenSearch index that supports the OpenTelemetry Metrics schema format.\\n6. On the **Metrics** page, choose `Otel-Index` from the **Data sources** dropdown menu and `Simple Schema for Observability Index` from the **OTel index** dropdown menu. A visualization is displayed, as shown in the following image.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/otel-metrics.png\\\" alt=\\\"OTel metrics dashboard\\\" width=\\\"700\\\"\/>\\n\\n---\\n\\n## Visualizing metrics in remote clusters\\nIntroduced 2.14\\n{: .label .label-purple }\\n\\nYou can view metrics from remote OpenSearch clusters by using the **Metrics** tool. Select the database icon on the upper-right toolbar and choose a cluster from the **DATA SOURCES** dropdown menu, as shown in the following image. You can switch from a local cluster to a remote cluster.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/remote-cluster-selection.png\\\" alt=\\\"Switching clusters using the Metrics analytics tool\\\" width=\\\"700\\\"\/>\\n\\nYou can also view metric visualizations from other sources alongside local metric visualizations. From the **DATA SOURCES** dropdown menu, choose the remote metric visualization to add it to the group of visualizations already shown on the dashboard. An example dashboard is shown in the following image.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/otel-metrics-remote-cluster-selection.png\\\" alt=\\\"Metrics dashboard\\\" width=\\\"700\\\"\/>\\n\\nTo learn about multi-cluster support for data sources, see [Enable OpenSearch Dashboards to support multiple OpenSearch clusters](https:\/\/github.com\/opensearch-project\/OpenSearch-Dashboards\/issues\/1388).\\n\\n## Creating visualizations based on custom metrics\\n\\nYou can create visualizations using the metric data collected by your OpenSearch cluster, including Prometheus metrics and custom metrics.\\n\\nTo create these visualizations, follow these steps:\\n\\n1. From the OpenSearch Dashboards main menu, navigate to **Observability** > **Metrics** > **Available Metrics**.\\n2. Choose the metrics to add to your visualization and then select **Save**.\\n3. When prompted for a **Custom operational dashboards\/application**, choose one of the listed options. You can edit the predefined name values in the **Metric Name** field.\\n4. Select **Save** to save your visualization. An example visualization is shown in the following image.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/metrics2.png\\\" alt=\\\"Metrics analytics dashboard with visualizations\\\" width=\\\"700\\\"\/>\\n\\n## Defining PPL queries for Prometheus metrics\\n\\nYou can define [Piped Processing Language (PPL)]({{site.url}}{{site.baseurl}}\/search-plugins\/sql\/ppl\/index\/) queries to interact with metrics collected by Prometheus. The following is an example PPL query for a Prometheus metric:\\n\\n```\\nsource = my_prometheus.prometheus_http_requests_total | stats avg(@value) by span(@timestamp,15s), handler, code\\n```\\n{% include copy-curl.html %}\\n\\n### Creating a custom visualization based on the PPL query\\n\\nTo create a custom visualization based on the PPL query, follow these steps:\\n\\n1. From the **Logs** page, select > **Event Explorer**.\\n2. On the **Explorer** page,  enter your PPL query and select **Run**. Then select **Save**.\\n3. When prompted to choose a **Custom Operational Dashboards\/Application**, select one of the listed options. Optionally, you can edit the predefined name values in the **Metric Name** fields and can choose to save the visualization as a metric.\\n5. Select **Save** to save your custom visualization. \\n\\nOnly queries that include a time-series visualization and statistics or span information can be saved as a metric, as shown in the following image.\\n\\n<img src=\\\"{{site.url}}{{site.baseurl}}\/images\/metrics\/metrics3.png\\\" alt=\\\"Saving queries as metrics\\\" width=\\\"700\\\"\/>\\n\\n\\n\"","generated_answer":"There is not enough information available to answer this question. The report extract does not mention whether OpenSearch works with OpenTelemetry.","usage":"{\"inputTokens\": 3081, \"outputTokens\": 43, \"totalTokens\": 3124}","latency":1279,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.064,"cost":0.000824}
{"query_text":"How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?","groundtruth_answer":"<question_answer>\nTo update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, send a PUT request to the \/_plugins\/_notifications\/configs\/<config_id> endpoint with the new configuration details in the request body, including the updated webhook URL and description. For example:\n\nPUT \/_plugins\/_notifications\/configs\/<config_id>\n{\n  \"config\": {\n    \"name\": \"Slack Channel\",\n    \"description\": \"This is an updated channel configuration\",\n    \"config_type\": \"slack\",\n    \"is_enabled\": true,\n    \"slack\": {\n      \"url\": \"https:\/\/hooks.slack.com\/new-webhook-url\"\n    }\n  }\n}\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: API\\nnav_order: 50\\nparent: Notifications\\nredirect_from:\\n  - \/notifications-plugin\/api\/\\n---\\n\\n# Notifications API\\n\\nIf you want to programmatically define your notification channels and sources for versioning and reuse, you can use the Notifications REST API to define, configure, and delete notification channels and send test messages.\\n\\n---\\n\\n#### Table of contents\\n1. TOC\\n{:toc}\\n\\n---\\n\\n## List supported channel configurations\\n\\nTo retrieve a list of all supported notification configuration types, send a GET request to the `features` resource.\\n\\n#### Example request\\n\\n```json\\nGET \/_plugins\/_notifications\/features\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"allowed_config_type_list\\\" : [\\n    \\\"slack\\\",\\n    \\\"chime\\\",\\n    \\\"webhook\\\",\\n    \\\"email\\\",\\n    \\\"sns\\\",\\n    \\\"ses_account\\\",\\n    \\\"smtp_account\\\",\\n    \\\"email_group\\\"\\n  ],\\n  \\\"plugin_features\\\" : {\\n    \\\"tooltip_support\\\" : \\\"true\\\"\\n  }\\n}\\n```\\n\\n## List all notification channels\\n\\nTo retrieve a list of all notification channels, send a GET request to the `channels` resource.\\n\\n#### Example request\\n\\n```json\\nGET \/_plugins\/_notifications\/channels\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"start_index\\\" : 0,\\n  \\\"total_hits\\\" : 2,\\n  \\\"total_hit_relation\\\" : \\\"eq\\\",\\n  \\\"channel_list\\\" : [\\n    {\\n      \\\"config_id\\\" : \\\"sample-id\\\",\\n      \\\"name\\\" : \\\"Sample Slack Channel\\\",\\n      \\\"description\\\" : \\\"This is a Slack channel\\\",\\n      \\\"config_type\\\" : \\\"slack\\\",\\n      \\\"is_enabled\\\" : true\\n    },\\n    {\\n      \\\"config_id\\\" : \\\"sample-id2\\\",\\n      \\\"name\\\" : \\\"Test chime channel\\\",\\n      \\\"description\\\" : \\\"A test chime channel\\\",\\n      \\\"config_type\\\" : \\\"chime\\\",\\n      \\\"is_enabled\\\" : true\\n    }\\n  ]\\n}\\n```\\n\\n## List all notification configurations\\n\\nTo retrieve a list of all notification configurations, send a GET request to the `configs` resource.\\n\\n#### Example request\\n\\n```json\\nGET _plugins\/_notifications\/configs\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"start_index\\\" : 0,\\n  \\\"total_hits\\\" : 2,\\n  \\\"total_hit_relation\\\" : \\\"eq\\\",\\n  \\\"config_list\\\" : [\\n    {\\n      \\\"config_id\\\" : \\\"sample-id\\\",\\n      \\\"last_updated_time_ms\\\" : 1652760532774,\\n      \\\"created_time_ms\\\" : 1652760532774,\\n      \\\"config\\\" : {\\n        \\\"name\\\" : \\\"Sample Slack Channel\\\",\\n        \\\"description\\\" : \\\"This is a Slack channel\\\",\\n        \\\"config_type\\\" : \\\"slack\\\",\\n        \\\"is_enabled\\\" : true,\\n        \\\"slack\\\" : {\\n          \\\"url\\\" : \\\"https:\/\/sample-slack-webhook\\\"\\n        }\\n      }\\n    },\\n    {\\n      \\\"config_id\\\" : \\\"sample-id2\\\",\\n      \\\"last_updated_time_ms\\\" : 1652760735380,\\n      \\\"created_time_ms\\\" : 1652760735380,\\n      \\\"config\\\" : {\\n        \\\"name\\\" : \\\"Test chime channel\\\",\\n        \\\"description\\\" : \\\"A test chime channel\\\",\\n        \\\"config_type\\\" : \\\"chime\\\",\\n        \\\"is_enabled\\\" : true,\\n        \\\"chime\\\" : {\\n          \\\"url\\\" : \\\"https:\/\/sample-chime-webhook\\\"\\n        }\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nTo filter the notification configuration types this request returns, you can refine your query with the following optional path parameters.\\n\\nParameter\\t| Description\\n:--- | :---\\nconfig_id | Specifies the channel identifier.\\nconfig_id_list | Specifies a comma-separated list of channel IDs.\\nfrom_index | The starting index to search from.\\nmax_items | The maximum amount of items to return in your request.\\nsort_order | Specifies the direction to sort results in. Valid options are `asc` and `desc`.\\nsort_field | Field to sort results with.\\nlast_updated_time_ms | The Unix time in milliseconds of when the channel was last updated.\\ncreated_time_ms | The Unix time in milliseconds of when the channel was created.\\nis_enabled | Indicates whether the channel is enabled.\\nconfig_type | The channel type. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`.\\nname | The channel name.\\ndescription\\t| The channel description.\\nemail.email_account_id | The sender email addresses the channel uses.\\nemail.email_group_id_list | The email groups the channel uses.\\nemail.recipient_list | The channel recipient list.\\nemail_group.recipient_list | The channel list of email recipient groups.\\nsmtp_account.method | The email encryption method.\\nslack.url\\t| The Slack channel URL.\\nchime.url\\t| The Amazon Chime connection URL.\\nwebhook.url\\t| The webhook URL.\\nsmtp_account.host\\t| The domain of the SMTP account.\\nsmtp_account.from_address\\t| The email account's sender address.\\nsmtp_account.method | The SMTP account's encryption method.\\nsns.topic_arn\\t| The Amazon Simple Notification Service (SNS) topic's ARN.\\nsns.role_arn | The Amazon SNS topic's role ARN.\\nses_account.region | The Amazon Simple Email Service (SES) account's AWS Region.\\nses_account.role_arn | The Amazon SES account's role ARN.\\nses_account.from_address | The Amazon SES account's sender email address.\\n\\n## Create channel configuration\\n\\nTo create a notification channel configuration, send a POST request to the `configs` resource.\\n\\n#### Example request\\n\\n```json\\nPOST \/_plugins\/_notifications\/configs\/\\n{\\n  \\\"config_id\\\": \\\"sample-id\\\",\\n  \\\"name\\\": \\\"sample-name\\\",\\n  \\\"config\\\": {\\n    \\\"name\\\": \\\"Sample Slack Channel\\\",\\n    \\\"description\\\": \\\"This is a Slack channel\\\",\\n    \\\"config_type\\\": \\\"slack\\\",\\n    \\\"is_enabled\\\": true,\\n    \\\"slack\\\": {\\n      \\\"url\\\": \\\"https:\/\/sample-slack-webhook\\\"\\n    }\\n  }\\n}\\n```\\n\\nThe create channel API operation accepts the following fields in its request body:\\n\\nField |\\tData type |\\tDescription |\\tRequired\\n:--- | :--- | :--- | :---\\nconfig_id | String | The configuration's custom ID. | No\\nconfig | Object |\\tContains all relevant information, such as channel name, configuration type, and plugin source. |\\tYes\\nname | String |\\tName of the channel. | Yes\\ndescription |\\tString | The channel's description. | No\\nconfig_type |\\tString | The destination of your notification. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`. | Yes\\nis_enabled | Boolean | Indicates whether the channel is enabled for sending and receiving notifications. Default is `true`.\\t| No\\n\\nThe create channel operation accepts multiple `config_types` as possible notification destinations, so follow the format for your preferred `config_type`.\\n\\n```json\\n\\\"sns\\\": {\\n  \\\"topic_arn\\\": \\\"<arn>\\\",\\n  \\\"role_arn\\\": \\\"<arn>\\\" \/\/optional\\n}\\n\\\"slack\\\": {\\n  \\\"url\\\": \\\"https:\/\/sample-chime-webhoook\\\"\\n}\\n\\\"chime\\\": {\\n  \\\"url\\\": \\\"https:\/\/sample-amazon-chime-webhoook\\\"\\n}\\n\\\"webhook\\\": {\\n      \\\"url\\\": \\\"https:\/\/custom-webhook-test-url.com:8888\/test-path?params1=value1&params2=value2\\\"\\n}\\n\\\"smtp_account\\\": {\\n  \\\"host\\\": \\\"test-host.com\\\",\\n  \\\"port\\\": 123,\\n  \\\"method\\\": \\\"start_tls\\\",\\n  \\\"from_address\\\": \\\"test@email.com\\\"\\n}\\n\\\"ses_account\\\": {\\n  \\\"region\\\": \\\"us-east-1\\\",\\n  \\\"role_arn\\\": \\\"arn:aws:iam::012345678912:role\/NotificationsSESRole\\\",\\n  \\\"from_address\\\": \\\"test@email.com\\\"\\n}\\n\\\"email_group\\\": { \/\/Email recipient group\\n  \\\"recipient_list\\\": [\\n    {\\n      \\\"recipient\\\": \\\"test-email1@test.com\\\"\\n    },\\n    {\\n      \\\"recipient\\\": \\\"test-email2@test.com\\\"\\n    }\\n  ]\\n}\\n\\\"email\\\": { \/\/The channel that sends emails\\n  \\\"email_account_id\\\": \\\"<smtp or ses account config id>\\\",\\n  \\\"recipient_list\\\": [\\n    {\\n      \\\"recipient\\\": \\\"custom.email@test.com\\\"\\n    }\\n  ],\\n  \\\"email_group_id_list\\\": []\\n}\\n```\\n\\nThe following example demonstrates how to create a channel using email as a `config_type`:\\n\\n```json\\nPOST \/_plugins\/_notifications\/configs\/\\n{\\n  \\\"id\\\": \\\"sample-email-id\\\",\\n  \\\"name\\\": \\\"sample-name\\\",\\n  \\\"config\\\": {\\n    \\\"name\\\": \\\"Sample Email Channel\\\",\\n    \\\"description\\\": \\\"Sample email description\\\",\\n    \\\"config_type\\\": \\\"email\\\",\\n    \\\"is_enabled\\\": true,\\n    \\\"email\\\": {\\n      \\\"email_account_id\\\": \\\"<email_account_id>\\\",\\n      \\\"recipient_list\\\": [\\n        \\\"sample@email.com\\\"\\n      ]\\n    }\\n  }\\n}\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"config_id\\\" : \\\"<config_id>\\\"\\n}\\n```\\n\\n\\n## Get channel configuration\\n\\nTo get a channel configuration by `config_id`, send a GET request and specify the `config_id` as a path parameter.\\n\\n#### Example request\\n\\n```json\\nGET _plugins\/_notifications\/configs\/<config_id>\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"start_index\\\" : 0,\\n  \\\"total_hits\\\" : 1,\\n  \\\"total_hit_relation\\\" : \\\"eq\\\",\\n  \\\"config_list\\\" : [\\n    {\\n      \\\"config_id\\\" : \\\"sample-id\\\",\\n      \\\"last_updated_time_ms\\\" : 1652760532774,\\n      \\\"created_time_ms\\\" : 1652760532774,\\n      \\\"config\\\" : {\\n        \\\"name\\\" : \\\"Sample Slack Channel\\\",\\n        \\\"description\\\" : \\\"This is a Slack channel\\\",\\n        \\\"config_type\\\" : \\\"slack\\\",\\n        \\\"is_enabled\\\" : true,\\n        \\\"slack\\\" : {\\n          \\\"url\\\" : \\\"https:\/\/sample-slack-webhook\\\"\\n        }\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n\\n## Update channel configuration\\n\\nTo update a channel configuration, send a POST request to the `configs` resource and specify the channel's `config_id` as a path parameter. Specify the new configuration details in the request body.\\n\\n#### Example request\\n\\n```json\\nPUT _plugins\/_notifications\/configs\/<config_id>\\n{\\n  \\\"config\\\": {\\n    \\\"name\\\": \\\"Slack Channel\\\",\\n    \\\"description\\\": \\\"This is an updated channel configuration\\\",\\n    \\\"config_type\\\": \\\"slack\\\",\\n    \\\"is_enabled\\\": true,\\n    \\\"slack\\\": {\\n      \\\"url\\\": \\\"https:\/\/hooks.slack.com\/sample-url\\\"\\n    }\\n  }\\n}\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"config_id\\\" : \\\"<config_id>\\\"\\n}\\n```\\n\\n\\n## Delete channel configuration\\n\\nTo delete a channel configuration, send a DELETE request to the `configs` resource and specify the `config_id` as a path parameter.\\n\\n#### Example request\\n\\n```json\\nDELETE \/_plugins\/_notifications\/configs\/<config_id>\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"delete_response_list\\\" : {\\n  \\\"<config_id>\\\" : \\\"OK\\\"\\n  }\\n}\\n```\\n\\nYou can also submit a comma-separated list of channel IDs you want to delete, and OpenSearch deletes all of the specified notification channels.\\n\\n#### Example request\\n\\n```json\\nDELETE \/_plugins\/_notifications\/configs\/?config_id_list=<config_id1>,<config_id2>,<config_id3>...\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"delete_response_list\\\" : {\\n  \\\"<config_id1>\\\" : \\\"OK\\\",\\n  \\\"<config_id2>\\\" : \\\"OK\\\",\\n  \\\"<config_id3>\\\" : \\\"OK\\\"\\n  }\\n}\\n```\\n\\n\\n## Send test notification\\n\\nTo send a test notification, send a GET request to `\/feature\/test\/` and specify the channel configuration's `config_id` as a path parameter.\\n\\n#### Example request\\n\\n```json\\nGET _plugins\/_notifications\/feature\/test\/<config_id>\\n```\\n\\n#### Example response\\n\\n```json\\n{\\n  \\\"event_source\\\" : {\\n    \\\"title\\\" : \\\"Test Message Title-0Jnlh4ABa4TCWn5C5H2G\\\",\\n    \\\"reference_id\\\" : \\\"0Jnlh4ABa4TCWn5C5H2G\\\",\\n    \\\"severity\\\" : \\\"info\\\",\\n    \\\"tags\\\" : [ ]\\n  },\\n  \\\"status_list\\\" : [\\n    {\\n      \\\"config_id\\\" : \\\"0Jnlh4ABa4TCWn5C5H2G\\\",\\n      \\\"config_type\\\" : \\\"slack\\\",\\n      \\\"config_name\\\" : \\\"sample-id\\\",\\n      \\\"email_recipient_status\\\" : [ ],\\n      \\\"delivery_status\\\" : {\\n        \\\"status_code\\\" : \\\"200\\\",\\n        \\\"status_text\\\" : \\\"\\\"\\\"<!doctype html>\\n<html>\\n<head>\\n<\/head>\\n<body>\\n<div>\\n    <h1>Example Domain<\/h1>\\n    <p>Sample paragraph.<\/p>\\n    <p><a href=\\\"sample.example.com\\\">TO BE OR NOT TO BE, THAT IS THE QUESTION<\/a><\/p>\\n<\/div>\\n<\/body>\\n<\/html>\\n\\\"\\\"\\\"\\n      }\\n    }\\n  ]\\n}\\n\\n```\\n\\n\\n\"","generated_answer":"To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, you can send a PUT request to the `configs` resource and specify the channel's `config_id` as a path parameter. In the request body, include the updated configuration details:\n\n{\n  \"config\": {\n    \"name\": \"Slack Channel\",\n    \"description\": \"This is an updated channel configuration\",\n    \"config_type\": \"slack\",\n    \"is_enabled\": true,\n    \"slack\": {\n      \"url\": \"https:\/\/hooks.slack.com\/sample-url\"\n    }\n  }\n}","usage":"{\"inputTokens\": 3560, \"outputTokens\": 156, \"totalTokens\": 3716}","latency":1741,"reasoning":"None","score":null,"faithfulness":1.0,"answer_relevancy":0.2,"cost":0.001085}
{"query_text":"What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?","groundtruth_answer":"<question_answer>\nThe specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\n\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\njavax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\n\nThis permission applies to the org.apache.commons.pool2.impl.GenericObjectPool object.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Installing plugins\\nnav_order: 90\\nhas_children: true\\nredirect_from:\\n   - \/opensearch\/install\/plugins\/\\n   - \/install-and-configure\/install-opensearch\/plugins\/\\n---\\n\\n# Installing plugins\\n\\nOpenSearch comprises of a number of plugins that add features and capabilities to the core platform. The plugins available to you are dependent on how OpenSearch was installed and which plugins were subsequently added or removed. For example, the minimal distribution of OpenSearch enables only core functionality, such as indexing and search. Using the minimal distribution of OpenSearch is beneficial when you are working in a testing environment, have custom plugins, or are intending to integrate OpenSearch with other services.\\n\\nThe standard distribution of OpenSearch has much more functionality included. You can choose to add additional plugins or remove any of the plugins you don't need. \\n\\nFor a list of the available plugins, see [Available plugins](#available-plugins).\\n\\nFor a plugin to work properly with OpenSearch, it may request certain permissions as part of the installation process. Review the requested permissions and proceed accordingly. It is important that you understand a plugin's functionality before installation. When opting for a community-provided plugin, ensure that the source is trustworthy and reliable.\\n{: .warning}\\n\\n## Managing plugins\\n\\nTo manage plugins in OpenSearch, you can use a command line tool called `opensearch-plugin`. This tool allows you to perform the following actions:\\n\\n- [List](#list) installed plugins.\\n- [Install](#install) plugins.\\n- [Remove](#remove) an installed plugin.\\n\\nYou can print help text by passing `-h` or `--help`. Depending on your host configuration, you might also need to run the command with `sudo` privileges.\\n\\nIf you're running OpenSearch in a Docker container, plugins must be installed, removed, and configured by modifying the Docker image. For more information, see [Working with plugins]({{site.url}}{{site.baseurl}}\/install-and-configure\/install-opensearch\/docker#working-with-plugins).\\n{: .note}\\n\\n## List\\n\\nUse `list` to see a list of plugins that have already been installed.\\n\\n#### Usage\\n```bash\\nbin\/opensearch-plugin list\\n```\\n\\n#### Example\\n```bash\\n$ .\/opensearch-plugin list\\nopensearch-alerting\\nopensearch-anomaly-detection\\nopensearch-asynchronous-search\\nopensearch-cross-cluster-replication\\nopensearch-geospatial\\nopensearch-index-management\\nopensearch-job-scheduler\\nopensearch-knn\\nopensearch-ml\\nopensearch-notifications\\nopensearch-notifications-core\\nopensearch-observability\\nopensearch-performance-analyzer\\nopensearch-reports-scheduler\\nopensearch-security\\nopensearch-sql\\n```\\n\\n## List (with CAT API)\\nYou can also list installed plugins by using the [CAT API]({{site.url}}{{site.baseurl}}\/api-reference\/cat\/cat-plugins\/).\\n\\n#### Usage\\n\\n```bash\\nGET _cat\/plugins\\n```\\n\\n#### Example response\\n\\n```bash\\nopensearch-node1 opensearch-alerting                  2.0.1.0\\nopensearch-node1 opensearch-anomaly-detection         2.0.1.0\\nopensearch-node1 opensearch-asynchronous-search       2.0.1.0\\nopensearch-node1 opensearch-cross-cluster-replication 2.0.1.0\\nopensearch-node1 opensearch-index-management          2.0.1.0\\nopensearch-node1 opensearch-job-scheduler             2.0.1.0\\nopensearch-node1 opensearch-knn                       2.0.1.0\\nopensearch-node1 opensearch-ml                        2.0.1.0\\nopensearch-node1 opensearch-notifications             2.0.1.0\\nopensearch-node1 opensearch-notifications-core        2.0.1.0\\n```\\n\\n## Install\\n\\nThere are three ways to install plugins using the `opensearch-plugin` tool:\\n\\n- [Install a plugin by name](#install-a-plugin-by-name).\\n- [Install a plugin from a zip file](#install-a-plugin-from-a-zip-file).\\n- [Install a plugin using Maven coordinates](#install-a-plugin-using-maven-coordinates).\\n\\n### Install a plugin by name\\n\\nYou can install plugins that aren't already preinstalled in your installation by using the plugin name. For a list of plugins that may not be preinstalled, see [Additional plugins](#additional-plugins).\\n\\n#### Usage\\n```bash\\nbin\/opensearch-plugin install <plugin-name>\\n```\\n\\n#### Example\\n```bash\\n$ sudo .\/opensearch-plugin install analysis-icu\\n-> Installing analysis-icu\\n-> Downloading analysis-icu from opensearch\\n[=================================================] 100%   \\n-> Installed analysis-icu with folder name analysis-icu\\n```\\n\\n### Install a plugin from a zip file\\n\\nYou can install remote zip files by replacing `<zip-file>` with the URL of the hosted file. The tool supports downloading over HTTP\/HTTPS protocols only. For local zip files, replace `<zip-file>` with `file:` followed by the absolute or relative path to the plugin zip file, as shown in the second example that follows.\\n\\n#### Usage\\n```bash\\nbin\/opensearch-plugin install <zip-file>\\n```\\n\\n#### Example\\n<details markdown=\\\"block\\\">\\n  <summary>\\n    Select to expand the example \\n  <\/summary>\\n  {: .text-delta}\\n  \\n```bash\\n# Zip file is hosted on a remote server - in this case, Maven central repository.\\n$ sudo .\/opensearch-plugin install https:\/\/repo1.maven.org\/maven2\/org\/opensearch\/plugin\/opensearch-anomaly-detection\/2.2.0.0\/opensearch-anomaly-detection-2.2.0.0.zip\\n-> Installing https:\/\/repo1.maven.org\/maven2\/org\/opensearch\/plugin\/opensearch-anomaly-detection\/2.2.0.0\/opensearch-anomaly-detection-2.2.0.0.zip\\n-> Downloading https:\/\/repo1.maven.org\/maven2\/org\/opensearch\/plugin\/opensearch-anomaly-detection\/2.2.0.0\/opensearch-anomaly-detection-2.2.0.0.zip\\n[=================================================] 100%   \\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n@     WARNING: plugin requires additional permissions     @\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n* java.lang.RuntimePermission accessClassInPackage.sun.misc\\n* java.lang.RuntimePermission accessDeclaredMembers\\n* java.lang.RuntimePermission getClassLoader\\n* java.lang.RuntimePermission setContextClassLoader\\n* java.lang.reflect.ReflectPermission suppressAccessChecks\\n* java.net.SocketPermission * connect,resolve\\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n* javax.management.MBeanServerPermission createMBeanServer\\n* javax.management.MBeanTrustPermission register\\nSee http:\/\/docs.oracle.com\/javase\/8\/docs\/technotes\/guides\/security\/permissions.html\\nfor descriptions of what these permissions allow and the associated risks.\\n\\nContinue with installation? [y\/N]y\\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\\n\\n# Zip file in a local directory.\\n$ sudo .\/opensearch-plugin install file:\/home\/user\/opensearch-anomaly-detection-2.2.0.0.zip\\n-> Installing file:\/home\/user\/opensearch-anomaly-detection-2.2.0.0.zip\\n-> Downloading file:\/home\/user\/opensearch-anomaly-detection-2.2.0.0.zip\\n[=================================================] 100%   \\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n@     WARNING: plugin requires additional permissions     @\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n* java.lang.RuntimePermission accessClassInPackage.sun.misc\\n* java.lang.RuntimePermission accessDeclaredMembers\\n* java.lang.RuntimePermission getClassLoader\\n* java.lang.RuntimePermission setContextClassLoader\\n* java.lang.reflect.ReflectPermission suppressAccessChecks\\n* java.net.SocketPermission * connect,resolve\\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n* javax.management.MBeanServerPermission createMBeanServer\\n* javax.management.MBeanTrustPermission register\\nSee http:\/\/docs.oracle.com\/javase\/8\/docs\/technotes\/guides\/security\/permissions.html\\nfor descriptions of what these permissions allow and the associated risks.\\n\\nContinue with installation? [y\/N]y\\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\\n```\\n<\/details>\\n\\n### Install a plugin using Maven coordinates\\n\\nThe `opensearch-plugin install` tool also allows you to specify Maven coordinates for available artifacts and versions hosted on [Maven Central](https:\/\/search.maven.org\/search?q=org.opensearch.plugin). The tool parses the Maven coordinates you provide and constructs a URL. As a result, the host must be able to connect directly to the Maven Central site. The plugin installation fails if you pass coordinates to a proxy or local repository.\\n\\n#### Usage\\n```bash\\nbin\/opensearch-plugin install <groupId>:<artifactId>:<version>\\n```\\n\\n#### Example\\n\\n<details markdown=\\\"block\\\">\\n  <summary>\\n    Select to expand the example \\n  <\/summary>\\n  {: .text-delta}\\n\\n```console\\n$ sudo .\/opensearch-plugin install org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0\\n-> Installing org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0\\n-> Downloading org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 from maven central\\n[=================================================] 100%   \\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n@     WARNING: plugin requires additional permissions     @\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n* java.lang.RuntimePermission accessClassInPackage.sun.misc\\n* java.lang.RuntimePermission accessDeclaredMembers\\n* java.lang.RuntimePermission getClassLoader\\n* java.lang.RuntimePermission setContextClassLoader\\n* java.lang.reflect.ReflectPermission suppressAccessChecks\\n* java.net.SocketPermission * connect,resolve\\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\\n* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\\n* javax.management.MBeanServerPermission createMBeanServer\\n* javax.management.MBeanTrustPermission register\\nSee http:\/\/docs.oracle.com\/javase\/8\/docs\/technotes\/guides\/security\/permissions.html\\nfor descriptions of what these permissions allow and the associated risks.\\n\\nContinue with installation? [y\/N]y\\n-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection\\n```\\n<\/details>\\n\\nRestart your OpenSearch node after installing a plugin.\\n{: .note}\\n\\n## Installing multiple plugins\\n\\nMultiple plugins can be installed in a single invocation.\\n\\n#### Usage\\n```bash\\nbin\/opensearch-plugin install <plugin-name> <plugin-name> ... <plugin-name>\\n```\\n\\n#### Example\\n```console\\n$ sudo .\/opensearch-plugin install analysis-nori repository-s3\\n```\\n\\n## Remove\\n\\nYou can remove a plugin that has already been installed with the `remove` option. \\n\\n#### Usage\\n```bash\\nbin\/opensearch-plugin remove <plugin-name>\\n```\\n\\n#### Example\\n```console\\n$ sudo .\/opensearch-plugin remove opensearch-anomaly-detection\\n-> removing [opensearch-anomaly-detection]...\\n```\\n\\nRestart your OpenSearch node after removing a plugin.\\n{: .note}\\n\\n## Batch mode\\n\\nWhen installing a plugin that requires additional privileges that are not included by default, the plugin will prompt you for confirmation of the required privileges. To grant all requested privileges, use batch mode to skip the confirmation prompt.\\n\\nTo force batch mode when installing plugins, add the `-b` or `--batch` option:\\n```bash\\nbin\/opensearch-plugin install --batch <plugin-name>\\n```\\n\\n## Available plugins\\n\\nOpenSearch provides several bundled plugins that are available for immediate use with all OpenSearch distributions except for the minimal distribution. Additional plugins are available but must be installed separately using one of the installation options.  \\n\\n### Bundled plugins\\n\\nThe following plugins are bundled with all OpenSearch distributions except for the minimal distribution. If you are using the minimal distribution, you can add these plugins by using one of the installation methods.\\n\\n| Plugin name | Repository | Earliest available version |\\n| :--- | :--- | :--- |\\n| Alerting | [opensearch-alerting](https:\/\/github.com\/opensearch-project\/alerting) | 1.0.0 |\\n| Anomaly Detection | [opensearch-anomaly-detection](https:\/\/github.com\/opensearch-project\/anomaly-detection) | 1.0.0 |\\n| Asynchronous Search | [opensearch-asynchronous-search](https:\/\/github.com\/opensearch-project\/asynchronous-search) | 1.0.0 |\\n| Cross Cluster Replication | [opensearch-cross-cluster-replication](https:\/\/github.com\/opensearch-project\/cross-cluster-replication) | 1.1.0 |\\n| Custom Codecs | [opensearch-custom-codecs](https:\/\/github.com\/opensearch-project\/custom-codecs) | 2.10.0 |\\n| Flow Framework | [flow-framework](https:\/\/github.com\/opensearch-project\/flow-framework) | 2.12.0 |\\n| Notebooks<sup>1<\/sup> | [opensearch-notebooks](https:\/\/github.com\/opensearch-project\/dashboards-notebooks) | 1.0.0 to 1.1.0 |\\n| Notifications | [notifications](https:\/\/github.com\/opensearch-project\/notifications) | 2.0.0\\n| Reports Scheduler | [opensearch-reports-scheduler](https:\/\/github.com\/opensearch-project\/dashboards-reports) | 1.0.0 |\\n| Geospatial | [opensearch-geospatial](https:\/\/github.com\/opensearch-project\/geospatial) | 2.2.0 |\\n| Index Management | [opensearch-index-management](https:\/\/github.com\/opensearch-project\/index-management) | 1.0.0 |\\n| Job Scheduler | [opensearch-job-scheduler](https:\/\/github.com\/opensearch-project\/job-scheduler) | 1.0.0 |\\n| k-NN | [opensearch-knn](https:\/\/github.com\/opensearch-project\/k-NN) | 1.0.0 |\\n| ML Commons | [opensearch-ml](https:\/\/github.com\/opensearch-project\/ml-commons) | 1.3.0 |\\n| Skills | [opensearch-skills](https:\/\/github.com\/opensearch-project\/skills) | 2.12.0 |\\n| Neural Search | [neural-search](https:\/\/github.com\/opensearch-project\/neural-search) | 2.4.0 |\\n| Observability | [opensearch-observability](https:\/\/github.com\/opensearch-project\/observability) | 1.2.0 |\\n| Performance Analyzer<sup>2<\/sup> | [opensearch-performance-analyzer](https:\/\/github.com\/opensearch-project\/performance-analyzer) | 1.0.0 |\\n| Security | [opensearch-security](https:\/\/github.com\/opensearch-project\/security) | 1.0.0 |\\n| Security Analytics | [opensearch-security-analytics](https:\/\/github.com\/opensearch-project\/security-analytics) | 2.4.0 |\\n| SQL | [opensearch-sql](https:\/\/github.com\/opensearch-project\/sql) | 1.0.0 |\\n\\n_<sup>1<\/sup>Dashboard Notebooks was merged in to the Observability plugin with the release of OpenSearch 1.2.0._<br>\\n_<sup>2<\/sup>Performance Analyzer is not available on Windows._\\n\\n\\n### Additional plugins\\n\\nThere are many more plugins available in addition to those provided by the default distribution. These additional plugins have been built by OpenSearch developers or members of the OpenSearch community. For a list of additional plugins you can install, see [Additional plugins]({{site.url}}{{site.baseurl}}\/install-and-configure\/additional-plugins\/index\/).\\n\\n## Plugin compatibility\\n\\nYou can specify plugin compatibility with a particular OpenSearch version in the `plugin-descriptor.properties` file. For example, a plugin with the following property is compatible only with OpenSearch 2.3.0:\\n\\n```properties\\nopensearch.version=2.3.0\\n```\\nAlternatively, you can specify a range of compatible OpenSearch versions by setting the `dependencies` property in the `plugin-descriptor.properties` file to one of the following notations:\\n- `dependencies={ opensearch: \\\"2.3.0\\\" }`: The plugin is compatible only with OpenSearch version 2.3.0.\\n- `dependencies={ opensearch: \\\"=2.3.0\\\" }`: The plugin is compatible only with OpenSearch version 2.3.0.\\n- `dependencies={ opensearch: \\\"~2.3.0\\\" }`: The plugin is compatible with all versions from 2.3.0 up to the next minor version, in this example, 2.4.0 (exclusive).\\n- `dependencies={ opensearch: \\\"^2.3.0\\\" }`: The plugin is compatible with all versions from 2.3.0 up to the next major version, in this example, 3.0.0 (exclusive).\\n\\nYou can specify only one of the `opensearch.version` or `dependencies` properties.\\n{: .note}\\n\\n## Related links\\n\\n- [Observability]({{site.url}}{{site.baseurl}}\/observability-plugin\/index\/)\\n- [Security Analytics]({{site.url}}{{site.baseurl}}\/security-analytics\/index\/)\\n- [Security]({{site.url}}{{site.baseurl}}\/security\/index\/)\\n- [Alerting]({{site.url}}{{site.baseurl}}\/monitoring-plugins\/alerting\/index\/)\\n- [Anomaly detection]({{site.url}}{{site.baseurl}}\/monitoring-plugins\/ad\/index\/)\\n- [Asynchronous search]({{site.url}}{{site.baseurl}}\/search-plugins\/async\/index\/)\\n- [Cross-cluster replication]({{site.url}}{{site.baseurl}}\/replication-plugin\/index\/)\\n- [Index State Management]({{site.url}}{{site.baseurl}}\/im-plugin\/ism\/index\/)\\n- [k-NN search]({{site.url}}{{site.baseurl}}\/search-plugins\/knn\/index\/)\\n- [ML Commons]({{site.url}}{{site.baseurl}}\/ml-commons-plugin\/index\/)\\n- [Neural search]({{site.url}}{{site.baseurl}}\/neural-search-plugin\/index\/)\\n- [Notifications]({{site.url}}{{site.baseurl}}\/notifications-plugin\/index\/)\\n- [OpenSearch Dashboards]({{site.url}}{{site.baseurl}}\/dashboards\/index\/)\\n- [Performance Analyzer]({{site.url}}{{site.baseurl}}\/monitoring-plugins\/pa\/index\/)\\n- [SQL]({{site.url}}{{site.baseurl}}\/search-plugins\/sql\/index\/)\\n\\n\\n\"","generated_answer":"The specific Java permission required to register and unregister MBeans when installing the anomaly detection plugin is:\n- javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean\n- javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean\nThese permissions apply to the org.apache.commons.pool2.impl.GenericObjectPool MBean.","usage":"{\"inputTokens\": 5220, \"outputTokens\": 168, \"totalTokens\": 5388}","latency":1821,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.123,"cost":0.001515}
{"query_text":"What environment variable and YAML structure must be used to customize the admin password in a helm chart? ","groundtruth_answer":"<question_answer>\nTo customize the admin password in a Helm chart, you need to use the environment variable OPENSEARCH_INITIAL_ADMIN_PASSWORD under the extraEnvs section in the values.yaml file, for example:\n\nextraEnvs:\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\n    value: <custom-admin-password>\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Helm\\nparent: Installing OpenSearch\\nnav_order: 6\\nredirect_from:\\n  - \/opensearch\/install\/helm\/\\n---\\n\\n# Helm\\n\\nHelm is a package manager that allows you to easily install and manage OpenSearch in a Kubernetes cluster. You can define your OpenSearch configurations in a YAML file and use Helm to deploy your applications in a version-controlled and reproducible way.\\n\\nThe Helm chart contains the resources described in the following table.\\n\\nResource | Description\\n:--- | :---\\n`Chart.yaml` |  Information about the chart.\\n`values.yaml` |  Default configuration values for the chart.\\n`templates` |  Templates that combine with values to generate the Kubernetes manifest files.\\n\\nThe specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC).\\n\\nFor information about the default configuration, steps to configure security, and configurable parameters, see the\\n[README](https:\/\/github.com\/opensearch-project\/helm-charts\/blob\/main\/README.md).\\n\\nThe instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the [Kubernetes documentation](https:\/\/kubernetes.io\/docs\/setup\/) for steps to configure a Kubernetes cluster and the [Helm documentation](https:\/\/helm.sh\/docs\/intro\/install\/) to install Helm.\\n{: .note }\\n\\n## Prerequisites\\n\\nThe default Helm chart deploys a three-node cluster. We recommend that you have at least 8 GiB of memory available for this deployment. You can expect the deployment to fail if, say, you have less than 4 GiB of memory available.\\n\\n## Install OpenSearch using Helm\\n\\n1. Add `opensearch` [helm-charts](https:\/\/github.com\/opensearch-project\/helm-charts) repository to Helm:\\n\\n   ```bash\\n   helm repo add opensearch https:\/\/opensearch-project.github.io\/helm-charts\/\\n   ```\\n   {% include copy.html %}\\n\\n1. Update the available charts locally from charts repositories:\\n\\n   ```bash\\n   helm repo update\\n   ```\\n   {% include copy.html %}\\n\\n1. To search for the OpenSearch-related Helm charts:\\n\\n   ```bash\\n   helm search repo opensearch\\n   ```\\n   {% include copy.html %}\\n\\n   ```bash\\n   NAME                            \\tCHART VERSION\\tAPP VERSION\\tDESCRIPTION                           \\n   opensearch\/opensearch           \\t1.0.7        \\t1.0.0      \\tA Helm chart for OpenSearch           \\n   opensearch\/opensearch-dashboards\\t1.0.4        \\t1.0.0      \\tA Helm chart for OpenSearch Dashboards\\n   ```\\n\\n1. Deploy OpenSearch:\\n\\n   ```bash\\n   helm install my-deployment opensearch\/opensearch\\n   ```\\n   {% include copy.html %}\\n\\nYou can also build the `opensearch-1.0.0.tgz` file manually:\\n\\n1. Change to the `opensearch` directory:\\n\\n   ```bash\\n   cd charts\/opensearch\\n   ```\\n   {% include copy.html %}\\n\\n1. Package the Helm chart:\\n\\n   ```bash\\n   helm package .\\n   ```\\n   {% include copy.html %}\\n\\n1. Deploy OpenSearch:\\n\\n   ```bash\\n   helm install --generate-name opensearch-1.0.0.tgz\\n   ```\\n   {% include copy.html %}\\n\\n   The output shows you the specifications instantiated from the install.\\n   To customize the deployment, pass in the values that you want to override with a custom YAML file:\\n\\n   ```bash\\n   helm install --values=customvalues.yaml opensearch-1.0.0.tgz\\n   ```\\n   {% include copy.html %}\\n\\nFor OpenSearch 2.12 or greater, customize the admin password in `values.yaml` under `extraEnvs`, as shown in the following example:\\n\\n```yaml\\nextraEnvs:\\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\\n    value: <custom-admin-password>\\n```\\n\\n#### Sample output\\n\\n  ```yaml\\n  NAME: opensearch-1-1629223146\\n  LAST DEPLOYED: Tue Aug 17 17:59:07 2021\\n  NAMESPACE: default\\n  STATUS: deployed\\n  REVISION: 1\\n  TEST SUITE: None\\n  NOTES:\\n  Watch all cluster members come up.\\n    $ kubectl get pods --namespace=default -l app=opensearch-cluster-master -w\\n  ```\\n\\nTo make sure your OpenSearch pod is up and running, run the following command:\\n\\n```bash\\n$ kubectl get pods\\nNAME                                                  READY   STATUS    RESTARTS   AGE\\nopensearch-cluster-master-0                           1\/1     Running   0          3m56s\\nopensearch-cluster-master-1                           1\/1     Running   0          3m56s\\nopensearch-cluster-master-2                           1\/1     Running   0          3m56s\\n```\\n\\nTo access the OpenSearch shell:\\n\\n```bash\\n$ kubectl exec -it opensearch-cluster-master-0 -- \/bin\/bash\\n```\\n{% include copy.html %}\\n\\nYou can send requests to the pod to verify that OpenSearch is up and running:\\n\\n```json\\n$ curl -XGET https:\/\/localhost:9200 -u 'admin:admin' --insecure\\n{\\n  \\\"name\\\" : \\\"opensearch-cluster-master-1\\\",\\n  \\\"cluster_name\\\" : \\\"opensearch-cluster\\\",\\n  \\\"cluster_uuid\\\" : \\\"hP2gq5bPS3SLp8Z7wXm8YQ\\\",\\n  \\\"version\\\" : {\\n    \\\"distribution\\\" : \\\"opensearch\\\",\\n    \\\"number\\\" : <version>,\\n    \\\"build_type\\\" : <build-type>,\\n    \\\"build_hash\\\" : <build-hash>,\\n    \\\"build_date\\\" : <build-date>,\\n    \\\"build_snapshot\\\" : false,\\n    \\\"lucene_version\\\" : <lucene-version>,\\n    \\\"minimum_wire_compatibility_version\\\" : \\\"6.8.0\\\",\\n    \\\"minimum_index_compatibility_version\\\" : \\\"6.0.0-beta1\\\"\\n  },\\n  \\\"tagline\\\" : \\\"The OpenSearch Project: https:\/\/opensearch.org\/\\\"\\n}\\n```\\n\\n## Uninstall using Helm\\n\\nTo identify the OpenSearch deployment that you want to delete:\\n\\n```bash\\n$ helm list\\nNAME  NAMESPACEREVISIONUPDATED  STATUS  CHART APP VERSION\\nopensearch-1-1629223146 default 1 2021-08-17 17:59:07.664498239 +0000 UTCdeployedopensearch-1.0.0    1.0.0       \\n```\\n\\nTo delete or uninstall a deployment, run the following command:\\n\\n```bash\\nhelm delete opensearch-1-1629223146\\n```\\n{% include copy.html %}\\n\\nFor steps to install OpenSearch Dashboards, see [Helm to install OpenSearch Dashboards]({{site.url}}{{site.baseurl}}\/dashboards\/install\/helm\/).\\n\\n\"","generated_answer":"To customize the admin password in a Helm chart, you must use the `OPENSEARCH_INITIAL_ADMIN_PASSWORD` environment variable and the following YAML structure in the `values.yaml` file:\n\n```yaml\nextraEnvs:\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\n    value: <custom-admin-password>\n```","usage":"{\"inputTokens\": 1874, \"outputTokens\": 98, \"totalTokens\": 1972}","latency":1134,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.135,"cost":0.000591}
{"query_text":"How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?","groundtruth_answer":"<question_answer>\nThe context does not provide enough information to answer how to group data by a nested field while calculating metrics on a parent-level field in the same query. The context explains the reverse_nested aggregation, which allows aggregating values from nested documents to their parent, but does not specifically address grouping by nested fields and calculating parent-level metrics in the same query.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Reverse nested\\nparent: Bucket aggregations\\nnav_order: 160\\nredirect_from:\\n  - \/query-dsl\/aggregations\/bucket\/reverse-nested\/\\n---\\n\\n# Reverse nested aggregations\\n\\nYou can aggregate values from nested documents to their parent; this aggregation is called `reverse_nested`.\\nYou can use `reverse_nested` to aggregate a field from the parent document after grouping by the field from the nested object. The `reverse_nested` aggregation \\\"joins back\\\" the root page and gets the `load_time` for each for your variations.\\n\\nThe `reverse_nested` aggregation is a sub-aggregation inside a nested aggregation. It accepts a single option named `path`. This option defines how many steps backwards in the document hierarchy OpenSearch takes to calculate the aggregations.\\n\\n```json\\nGET logs\/_search\\n{\\n  \\\"query\\\": {\\n    \\\"match\\\": { \\\"response\\\": \\\"200\\\" }\\n  },\\n  \\\"aggs\\\": {\\n    \\\"pages\\\": {\\n      \\\"nested\\\": {\\n        \\\"path\\\": \\\"pages\\\"\\n      },\\n      \\\"aggs\\\": {\\n        \\\"top_pages_per_load_time\\\": {\\n          \\\"terms\\\": {\\n            \\\"field\\\": \\\"pages.load_time\\\"\\n          },\\n          \\\"aggs\\\": {\\n            \\\"comment_to_logs\\\": {\\n              \\\"reverse_nested\\\": {},\\n              \\\"aggs\\\": {\\n                \\\"min_load_time\\\": {\\n                  \\\"min\\\": {\\n                    \\\"field\\\": \\\"pages.load_time\\\"\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\n#### Example response\\n\\n```json\\n...\\n\\\"aggregations\\\" : {\\n  \\\"pages\\\" : {\\n    \\\"doc_count\\\" : 2,\\n    \\\"top_pages_per_load_time\\\" : {\\n      \\\"doc_count_error_upper_bound\\\" : 0,\\n      \\\"sum_other_doc_count\\\" : 0,\\n      \\\"buckets\\\" : [\\n        {\\n          \\\"key\\\" : 200.0,\\n          \\\"doc_count\\\" : 1,\\n          \\\"comment_to_logs\\\" : {\\n            \\\"doc_count\\\" : 1,\\n            \\\"min_load_time\\\" : {\\n              \\\"value\\\" : null\\n            }\\n          }\\n        },\\n        {\\n          \\\"key\\\" : 500.0,\\n          \\\"doc_count\\\" : 1,\\n          \\\"comment_to_logs\\\" : {\\n            \\\"doc_count\\\" : 1,\\n            \\\"min_load_time\\\" : {\\n              \\\"value\\\" : null\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  }\\n }\\n}\\n```\\n\\nThe response shows the logs index has one page with a `load_time` of 200 and one with a `load_time` of 500.\\n\\n\"","generated_answer":"To group your data by a nested field but still calculate metrics on a parent-level field in the same query, you can use the `reverse_nested` aggregation. The `reverse_nested` aggregation allows you to \"join back\" to the parent document after grouping by the nested field, enabling you to calculate metrics on the parent-level field.\n\nIn the provided example, the `reverse_nested` aggregation is used within the `top_pages_per_load_time` aggregation to calculate the `min_load_time` on the parent-level `pages` field after grouping by the nested `load_time` field.","usage":"{\"inputTokens\": 894, \"outputTokens\": 152, \"totalTokens\": 1046}","latency":1490,"reasoning":"None","score":null,"faithfulness":0.333,"answer_relevancy":0.229,"cost":0.0004135}
{"query_text":"Make it simpler like what happens when I have missing values in an agg?","groundtruth_answer":"<question_answer>\nWhen there are missing values in an aggregation, the `missing` parameter can be used to specify a bucket name for documents that do not have the aggregating field or have a null value for that field. For example, `\"missing\": \"N\/A\"` will place missing values in a bucket named \"N\/A\". However, by default (`min_doc_count=1`), this bucket will not be included in the response. To see the missing bucket, `min_doc_count` needs to be set to 0.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Missing\\nparent: Bucket aggregations\\nnav_order: 120\\nredirect_from:\\n  - \/query-dsl\/aggregations\/bucket\/missing\/\\n---\\n\\n# Missing aggregations\\n\\nIf you have documents in your index that don\\u2019t contain the aggregating field at all or the aggregating field has a value of NULL, use the `missing` parameter to specify the name of the bucket such documents should be placed in.\\n\\nThe following example adds any missing values to a bucket named \\\"N\/A\\\":\\n\\n```json\\nGET opensearch_dashboards_sample_data_logs\/_search\\n{\\n  \\\"size\\\": 0,\\n  \\\"aggs\\\": {\\n    \\\"response_codes\\\": {\\n      \\\"terms\\\": {\\n        \\\"field\\\": \\\"response.keyword\\\",\\n        \\\"size\\\": 10,\\n        \\\"missing\\\": \\\"N\/A\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n{% include copy-curl.html %}\\n\\nBecause the default value for the `min_doc_count` parameter is 1, the `missing` parameter doesn't return any buckets in its response. Set `min_doc_count` parameter to 0 to see the \\\"N\/A\\\" bucket in the response:\\n\\n```json\\nGET opensearch_dashboards_sample_data_logs\/_search\\n{\\n  \\\"size\\\": 0,\\n  \\\"aggs\\\": {\\n    \\\"response_codes\\\": {\\n      \\\"terms\\\": {\\n        \\\"field\\\": \\\"response.keyword\\\",\\n        \\\"size\\\": 10,\\n        \\\"missing\\\": \\\"N\/A\\\",\\n        \\\"min_doc_count\\\": 0\\n      }\\n    }\\n  }\\n}\\n```\\n\\n#### Example response\\n\\n```json\\n...\\n\\\"aggregations\\\" : {\\n  \\\"response_codes\\\" : {\\n    \\\"doc_count_error_upper_bound\\\" : 0,\\n    \\\"sum_other_doc_count\\\" : 0,\\n    \\\"buckets\\\" : [\\n      {\\n        \\\"key\\\" : \\\"200\\\",\\n        \\\"doc_count\\\" : 12832\\n      },\\n      {\\n        \\\"key\\\" : \\\"404\\\",\\n        \\\"doc_count\\\" : 801\\n      },\\n      {\\n        \\\"key\\\" : \\\"503\\\",\\n        \\\"doc_count\\\" : 441\\n      },\\n      {\\n        \\\"key\\\" : \\\"N\/A\\\",\\n        \\\"doc_count\\\" : 0\\n      }\\n    ]\\n  }\\n }\\n}\\n```\\n\\n\"","generated_answer":"When you have missing values in an aggregation, you can use the \"missing\" parameter to specify the name of the bucket that the documents with missing values should be placed in. This allows you to account for the missing values in your analysis.","usage":"{\"inputTokens\": 764, \"outputTokens\": 64, \"totalTokens\": 828}","latency":905,"reasoning":"None","score":null,"faithfulness":0.5,"answer_relevancy":0.087,"cost":0.000271}
{"query_text":"Can you explain compound queries to me? ","groundtruth_answer":"<question_answer>\nCompound queries are used to combine multiple leaf or compound queries together using Boolean logic or other operations. Some examples of compound queries include:\n\n- bool: Combines multiple queries with Boolean logic (AND, OR, NOT)\n- boosting: Boosts the relevance score of documents matching a positive query, and downgrades the score of documents matching a negative query\n- constant_score: Wraps a query and assigns a constant relevance score to all matching documents\n- dis_max: Returns documents matching one or more queries, with higher scores for documents matching multiple queries\n- function_score: Recalculates the relevance score of documents using a custom function\n- hybrid: Combines relevance scores from multiple queries into one score for each document\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Compound queries\\nhas_children: true\\nhas_toc: false\\nnav_order: 40\\nredirect_from: \\n  - \/opensearch\/query-dsl\/compound\/index\/\\n  - \/query-dsl\/compound\/index\/\\n  - \/query-dsl\/query-dsl\/compound\/\\n  - \/query-dsl\/compound\/\\n---\\n\\n# Compound queries\\n\\nCompound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. \\n\\nThe following table lists all compound query types.\\n\\nQuery type | Description\\n:--- | :---\\n[`bool`]({{site.url}}{{site.baseurl}}\/query-dsl\/compound\/bool\/) (Boolean)| Combines multiple query clauses with Boolean logic. \\n[`boosting`]({{site.url}}{{site.baseurl}}\/query-dsl\/compound\/boosting\/) | Changes the relevance score of documents without removing them from the search results. Returns documents that match a `positive` query, but downgrades the relevance of documents in the results that match a `negative` query.\\n[`constant_score`]({{site.url}}{{site.baseurl}}\/query-dsl\/compound\/constant-score\/) | Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the `boost` value.\\n[`dis_max`]({{site.url}}{{site.baseurl}}\/query-dsl\/compound\/disjunction-max\/) (disjunction max) | Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value.\\n[`function_score`]({{site.url}}{{site.baseurl}}\/query-dsl\/compound\/function-score\/) | Recalculates the relevance score of documents that are returned by a query using a function that you define.\\n[`hybrid`]({{site.url}}{{site.baseurl}}\/query-dsl\/compound\/hybrid\/) | Combines relevance scores from multiple queries into one score for a given document.\\n\\n\\n\"","generated_answer":"Compound queries are a type of query in Elasticsearch that serve as wrappers for multiple leaf or compound clauses. They are used to combine the results of multiple queries or modify their behavior. Some examples of compound queries include:\n\n- `bool`: Combines multiple query clauses using Boolean logic (AND, OR, NOT).\n- `boosting`: Changes the relevance score of documents without removing them from the search results.\n- `constant_score`: Assigns a constant score to all matching documents.\n- `dis_max`: Returns documents that match one or more query clauses and assigns a higher relevance score to documents that match multiple clauses.\n- `function_score`: Recalculates the relevance score of documents using a custom function.","usage":"{\"inputTokens\": 730, \"outputTokens\": 176, \"totalTokens\": 906}","latency":1707,"reasoning":"None","score":null,"faithfulness":0.429,"answer_relevancy":0.128,"cost":0.0004025}
{"query_text":"What is the difference between vector search and full text search? ","groundtruth_answer":"<question_answer>\nThe main difference between vector search and full text search is:\n\nVector search is used to find similar or semantically related documents to a given query, based on vector representations of the text. It calculates the similarity between the query vector and document vectors, and returns the most similar documents.\n\nFull text search is used to find documents that contain the exact words or terms in the query. It analyzes the query text and matches it against an inverted index of terms in the documents, returning documents that contain those terms.\n\nThere is not enough information available in the given context to explain the difference between vector search and full text search.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Term-level and full-text queries compared\\nnav_order: 10\\nredirect_from:\\n  - \/query-dsl\/query-dsl\/term-vs-full-text\/\\n  - \/opensearch\/query-dsl\/term-vs-full-text\/\\n---\\n\\n# Term-level and full-text queries compared\\n\\nYou can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries [analyze]({{site.url}}{{site.baseurl}}\/analyzers\/) the query string. The following table summarizes the differences between term-level and full-text queries.\\n\\n| | Term-level queries | Full-text queries\\n:--- | :--- | :---\\n*Description* | Term-level queries answer which documents match a query. | Full-text queries answer how well the documents match a query.\\n*Analyzer* | The search term isn't analyzed. This means that the term query searches for your search term as it is.  | The search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document's field.\\n*Relevance* | Term-level queries simply return documents that match without sorting them based on the relevance score. They still calculate the relevance score, but this score is the same for all the documents that are returned. | Full-text queries calculate a relevance score for each match and sort the results by decreasing order of relevance.\\n*Use Case* | Use term-level queries when you want to match exact values such as numbers, dates, or tags and don't need the matches to be sorted by relevance. | Use full-text queries to match text fields and sort by relevance after taking into account factors like casing and stemming variants.\\n\\nOpenSearch uses the BM25 ranking algorithm to calculate relevance scores. To learn more, see [Okapi BM25](https:\/\/en.wikipedia.org\/wiki\/Okapi_BM25).\\n{: .note }\\n\\n## Should I use a full-text or a term-level query?\\n\\nTo clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster.\\n\\n### Example: Phrase search\\n\\nIn this example, you'll search the complete works of Shakespeare for the phrase \\\"To be, or not to be\\\" in the `text_entry` field. \\n\\nFirst, use a **term-level query** for this search:\\n\\n```json\\nGET shakespeare\/_search\\n{\\n  \\\"query\\\": {\\n    \\\"term\\\": {\\n      \\\"text_entry\\\": \\\"To be, or not to be\\\"\\n    }\\n  }\\n}\\n```\\n\\nThe response contains no matches, indicated by zero `hits`:\\n\\n```json\\n{\\n  \\\"took\\\" : 3,\\n  \\\"timed_out\\\" : false,\\n  \\\"_shards\\\" : {\\n    \\\"total\\\" : 1,\\n    \\\"successful\\\" : 1,\\n    \\\"skipped\\\" : 0,\\n    \\\"failed\\\" : 0\\n  },\\n  \\\"hits\\\" : {\\n    \\\"total\\\" : {\\n      \\\"value\\\" : 0,\\n      \\\"relation\\\" : \\\"eq\\\"\\n    },\\n    \\\"max_score\\\" : null,\\n    \\\"hits\\\" : [ ]\\n  }\\n}\\n```\\n\\nThis is because the term \\u201cTo be, or not to be\\u201d is searched literally in the inverted index, where only the analyzed values of the text fields are stored. Term-level queries aren\\u2019t suited for searching analyzed text fields because they often yield unexpected results. When working with text data, use term-level queries only for fields mapped as `keyword`.\\n\\nNow search for the same phrase using a **full-text query**:\\n\\n```json\\nGET shakespeare\/_search\\n{\\n  \\\"query\\\": {\\n    \\\"match\\\": {\\n      \\\"text_entry\\\": \\\"To be, or not to be\\\"\\n    }\\n  }\\n}\\n```\\n\\nThe search query \\u201cTo be, or not to be\\u201d is analyzed and tokenized into an array of tokens just like the `text_entry` field of the documents. The full-text query takes an intersection of tokens between the search query and the `text_entry` fields for all the documents, and then sorts the results by relevance score:\\n\\n```json\\n{\\n  \\\"took\\\" : 19,\\n  \\\"timed_out\\\" : false,\\n  \\\"_shards\\\" : {\\n    \\\"total\\\" : 1,\\n    \\\"successful\\\" : 1,\\n    \\\"skipped\\\" : 0,\\n    \\\"failed\\\" : 0\\n  },\\n  \\\"hits\\\" : {\\n    \\\"total\\\" : {\\n      \\\"value\\\" : 10000,\\n      \\\"relation\\\" : \\\"gte\\\"\\n    },\\n    \\\"max_score\\\" : 17.419369,\\n    \\\"hits\\\" : [\\n      {\\n        \\\"_index\\\" : \\\"shakespeare\\\",\\n        \\\"_id\\\" : \\\"34229\\\",\\n        \\\"_score\\\" : 17.419369,\\n        \\\"_source\\\" : {\\n          \\\"type\\\" : \\\"line\\\",\\n          \\\"line_id\\\" : 34230,\\n          \\\"play_name\\\" : \\\"Hamlet\\\",\\n          \\\"speech_number\\\" : 19,\\n          \\\"line_number\\\" : \\\"3.1.64\\\",\\n          \\\"speaker\\\" : \\\"HAMLET\\\",\\n          \\\"text_entry\\\" : \\\"To be, or not to be: that is the question:\\\"\\n        }\\n      },\\n      {\\n        \\\"_index\\\" : \\\"shakespeare\\\",\\n        \\\"_id\\\" : \\\"109930\\\",\\n        \\\"_score\\\" : 14.883024,\\n        \\\"_source\\\" : {\\n          \\\"type\\\" : \\\"line\\\",\\n          \\\"line_id\\\" : 109931,\\n          \\\"play_name\\\" : \\\"A Winters Tale\\\",\\n          \\\"speech_number\\\" : 23,\\n          \\\"line_number\\\" : \\\"4.4.153\\\",\\n          \\\"speaker\\\" : \\\"PERDITA\\\",\\n          \\\"text_entry\\\" : \\\"Not like a corse; or if, not to be buried,\\\"\\n        }\\n      },\\n      {\\n        \\\"_index\\\" : \\\"shakespeare\\\",\\n        \\\"_id\\\" : \\\"103117\\\",\\n        \\\"_score\\\" : 14.782743,\\n        \\\"_source\\\" : {\\n          \\\"type\\\" : \\\"line\\\",\\n          \\\"line_id\\\" : 103118,\\n          \\\"play_name\\\" : \\\"Twelfth Night\\\",\\n          \\\"speech_number\\\" : 53,\\n          \\\"line_number\\\" : \\\"1.3.95\\\",\\n          \\\"speaker\\\" : \\\"SIR ANDREW\\\",\\n          \\\"text_entry\\\" : \\\"will not be seen; or if she be, its four to one\\\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n...\\n```\\n\\nFor a list of all full-text queries, see [Full-text queries]({{site.url}}{{site.baseurl}}\/opensearch\/query-dsl\/full-text\/index).\\n\\n### Example: Exact term search\\n\\nIf you want to search for an exact term like \\u201cHAMLET\\u201d in the `speaker` field and don't need the results to be sorted by relevance score, a term-level query is more efficient:\\n\\n```json\\nGET shakespeare\/_search\\n{\\n  \\\"query\\\": {\\n    \\\"term\\\": {\\n      \\\"speaker\\\": \\\"HAMLET\\\"\\n    }\\n  }\\n}\\n```\\n\\nThe response contains document matches:\\n\\n```json\\n{\\n  \\\"took\\\" : 5,\\n  \\\"timed_out\\\" : false,\\n  \\\"_shards\\\" : {\\n    \\\"total\\\" : 1,\\n    \\\"successful\\\" : 1,\\n    \\\"skipped\\\" : 0,\\n    \\\"failed\\\" : 0\\n  },\\n  \\\"hits\\\" : {\\n    \\\"total\\\" : {\\n      \\\"value\\\" : 1582,\\n      \\\"relation\\\" : \\\"eq\\\"\\n    },\\n    \\\"max_score\\\" : 4.2540946,\\n    \\\"hits\\\" : [\\n      {\\n        \\\"_index\\\" : \\\"shakespeare\\\",\\n        \\\"_id\\\" : \\\"32700\\\",\\n        \\\"_score\\\" : 4.2540946,\\n        \\\"_source\\\" : {\\n          \\\"type\\\" : \\\"line\\\",\\n          \\\"line_id\\\" : 32701,\\n          \\\"play_name\\\" : \\\"Hamlet\\\",\\n          \\\"speech_number\\\" : 9,\\n          \\\"line_number\\\" : \\\"1.2.66\\\",\\n          \\\"speaker\\\" : \\\"HAMLET\\\",\\n          \\\"text_entry\\\" : \\\"[Aside]  A little more than kin, and less than kind.\\\"\\n        }\\n      },\\n      {\\n        \\\"_index\\\" : \\\"shakespeare\\\",\\n        \\\"_id\\\" : \\\"32702\\\",\\n        \\\"_score\\\" : 4.2540946,\\n        \\\"_source\\\" : {\\n          \\\"type\\\" : \\\"line\\\",\\n          \\\"line_id\\\" : 32703,\\n          \\\"play_name\\\" : \\\"Hamlet\\\",\\n          \\\"speech_number\\\" : 11,\\n          \\\"line_number\\\" : \\\"1.2.68\\\",\\n          \\\"speaker\\\" : \\\"HAMLET\\\",\\n          \\\"text_entry\\\" : \\\"Not so, my lord; I am too much i' the sun.\\\"\\n        }\\n      },\\n      {\\n        \\\"_index\\\" : \\\"shakespeare\\\",\\n        \\\"_id\\\" : \\\"32709\\\",\\n        \\\"_score\\\" : 4.2540946,\\n        \\\"_source\\\" : {\\n          \\\"type\\\" : \\\"line\\\",\\n          \\\"line_id\\\" : 32710,\\n          \\\"play_name\\\" : \\\"Hamlet\\\",\\n          \\\"speech_number\\\" : 13,\\n          \\\"line_number\\\" : \\\"1.2.75\\\",\\n          \\\"speaker\\\" : \\\"HAMLET\\\",\\n          \\\"text_entry\\\" : \\\"Ay, madam, it is common.\\\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n...\\n```\\n\\nThe term-level queries provide exact matches. So if you search for \\u201cHamlet\\u201d, you don\\u2019t receive any matches, because \\u201cHAMLET\\u201d is a keyword field and is stored in OpenSearch literally and not in an analyzed form.\\nThe search query \\u201cHAMLET\\u201d is also searched literally. So to get a match for this field, we need to enter the exact same characters.\\n\\n\\n\\n\\n\"","generated_answer":"The main difference between vector search and full text search is:\n- Vector search looks for documents that are similar to the query based on the vector representation of the text, while full text search looks for documents that contain the exact words or phrases in the query.\n- Vector search can find documents that are semantically similar to the query, even if they don't contain the exact words, while full text search can only find documents that contain the exact words or phrases in the query.\n- Vector search provides a relevance score for each document based on the similarity between the document and the query, while full text search simply returns documents that match the query without sorting them by relevance.","usage":"{\"inputTokens\": 2587, \"outputTokens\": 153, \"totalTokens\": 2740}","latency":2148,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.186,"cost":0.000838}
{"query_text":"Why can't my coworker see my async searches, but I can see and delete theirs? This role thing is messing everything up!","groundtruth_answer":"<question_answer>\nBased on the information provided, the reason why your coworker cannot see your async searches but you can see and delete theirs is likely due to the backend roles assigned to each user. The context explains that when users have different backend roles, they can only see and manage async searches submitted by users with a subset of their own backend roles. So if you have a backend role that is a superset of your coworker's role, you can see and delete their async searches, but not vice versa.\n<\/question_answer>","retrieved_chunks":"\"---\\nlayout: default\\ntitle: Asynchronous search security\\nnav_order: 2\\nparent: Asynchronous search\\ngrand_parent: Improving search performance\\nhas_children: false\\nredirect_from:\\n - \/search-plugins\/async\/security\/\\n---\\n\\n# Asynchronous search security\\n\\nYou can use the Security plugin with asynchronous searches to limit non-admin users to specific actions. For example, you might want some users to only be able to submit or delete asynchronous searches, while you might want others to only view the results.\\n\\nAll asynchronous search indexes are protected as system indexes. Only a super admin user or an admin user with a Transport Layer Security (TLS) certificate can access system indexes. For more information, see [System indexes]({{site.url}}{{site.baseurl}}\/security\/configuration\/system-indices\/).\\n\\n## Basic permissions\\n\\nAs an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see [Asynchronous search]({{site.url}}{{site.baseurl}}\/).\\n\\nThe Security plugin has two built-in roles that cover most asynchronous search use cases: `asynchronous_search_full_access` and `asynchronous_search_read_access`. For descriptions of each, see [Predefined roles]({{site.url}}{{site.baseurl}}\/security\/access-control\/users-roles#predefined-roles).\\n\\nIf these roles don\\u2019t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the `cluster:admin\/opensearch\/asynchronous_search\/delete` permission lets you delete a previously submitted asynchronous search.\\n\\n### A note on Asynchronous Search and fine-grained access control\\n\\nBy design, the Asynchronous Search plugin extracts data from a target index and stores the data in a separate index to make search results available to users with the proper permissions. Although a user with either the `asynchronous_search_read_access` or `cluster:admin\/opensearch\/asynchronous_search\/get` permission cannot submit the asynchronous search request itself, that user can get and view the search results using the associated search ID. [Document-level security]({{site.url}}{{site.baseurl}}\/security\/access-control\/document-level-security) (DLS) and [field-level security]({{site.url}}{{site.baseurl}}\/security\/access-control\/field-level-security) (FLS) access controls are designed to protect the data in the target index. But once the data is stored outside this index, users with these access permissions are able to use search IDs to get and view asynchronous search results, which may include data that is otherwise concealed by DLS and FLS access control in the target index.\\n\\nTo reduce the chances of unintended users viewing search results that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See [Limit access by backend role](#advanced-limit-access-by-backend-role) for details.\\n\\n## (Advanced) Limit access by backend role\\n\\nUse backend roles to configure fine-grained access to asynchronous searches based on roles. For example, users of different departments in an organization can view asynchronous searches owned by their own department.\\n\\nFirst, make sure your users have the appropriate [backend roles]({{site.url}}{{site.baseurl}}\/security\/access-control\/index\/). Backend roles usually come from an [LDAP server]({{site.url}}{{site.baseurl}}\/security\/configuration\/ldap\/) or [SAML provider]({{site.url}}{{site.baseurl}}\/security\/configuration\/saml\/). However, if you use the internal user database, you can use the REST API to [add them manually]({{site.url}}{{site.baseurl}}\/security\/access-control\/api#create-user).\\n\\nNow when users view asynchronous search resources in OpenSearch Dashboards (or make REST API calls), they only see asynchronous searches submitted by users who have a subset of the backend role.\\nFor example, consider two users: `judy` and `elon`.\\n\\n`judy` has an IT backend role:\\n\\n```json\\nPUT _plugins\/_security\/api\/internalusers\/judy\\n{\\n  \\\"password\\\": \\\"judy\\\",\\n  \\\"backend_roles\\\": [\\n    \\\"IT\\\"\\n  ],\\n  \\\"attributes\\\": {}\\n}\\n```\\n\\n`elon` has an admin backend role:\\n\\n```json\\nPUT _plugins\/_security\/api\/internalusers\/elon\\n{\\n  \\\"password\\\": \\\"elon\\\",\\n  \\\"backend_roles\\\": [\\n    \\\"admin\\\"\\n  ],\\n  \\\"attributes\\\": {}\\n}\\n```\\n\\nBoth `judy` and `elon` have full access to asynchronous search:\\n\\n```json\\nPUT _plugins\/_security\/api\/rolesmapping\/async_full_access\\n{\\n  \\\"backend_roles\\\": [],\\n  \\\"hosts\\\": [],\\n  \\\"users\\\": [\\n    \\\"judy\\\",\\n    \\\"elon\\\"\\n  ]\\n}\\n```\\n\\nBecause they have different backend roles, an asynchronous search submitted by `judy` will not be visible to `elon` and vice versa.\\n\\n`judy` needs to have at least the superset of all roles that `elon` has to see `elon`'s asynchronous searches.\\n\\nFor example, if `judy` has five backend roles and `elon` has one of these roles, then `judy` can see asynchronous searches submitted by `elon`, but `elon` can\\u2019t see the asynchronous searches submitted by `judy`. This means that `judy` can perform GET and DELETE operations on asynchronous searches submitted by `elon`, but not the reverse.\\n\\nIf none of the users have any backend roles, all three will be able to see the others' searches.\\n\\nFor example, consider three users: `judy`, `elon`, and `jack`.\\n\\n`judy`, `elon`, and `jack` have no backend roles set up:\\n\\n```json\\nPUT _plugins\/_security\/api\/internalusers\/judy\\n{\\n  \\\"password\\\": \\\"judy\\\",\\n  \\\"backend_roles\\\": [],\\n  \\\"attributes\\\": {}\\n}\\n```\\n\\n```json\\nPUT _plugins\/_security\/api\/internalusers\/elon\\n{\\n  \\\"password\\\": \\\"elon\\\",\\n  \\\"backend_roles\\\": [],\\n  \\\"attributes\\\": {}\\n}\\n```\\n\\n```json\\nPUT _plugins\/_security\/api\/internalusers\/jack\\n{\\n  \\\"password\\\": \\\"jack\\\",\\n  \\\"backend_roles\\\": [],\\n  \\\"attributes\\\": {}\\n}\\n```\\n\\nBoth `judy` and `elon` have full access to asynchronous search:\\n\\n```json\\nPUT _plugins\/_security\/api\/rolesmapping\/async_full_access\\n{\\n  \\\"backend_roles\\\": [],\\n  \\\"hosts\\\": [],\\n  \\\"users\\\": [\\\"judy\\\",\\\"elon\\\"]\\n}\\n```\\n\\n`jack` has read access to asynchronous search results:\\n\\n```json\\nPUT _plugins\/_security\/api\/rolesmapping\/async_read_access\\n{\\n  \\\"backend_roles\\\": [],\\n  \\\"hosts\\\": [],\\n  \\\"users\\\": [\\\"jack\\\"]\\n}\\n```\\n\\nBecause none of the users have backend roles, they will be able to see each other's asynchronous searches. So, if `judy` submits an asynchronous search, `elon`, who has full access, will be able to see that search. `jack`, who has read access, will also be able to see `judy`'s asynchronous search.\\n\\n\"","generated_answer":"There is not enough information available to answer this question. The context provided does not mention anything about your coworker or why you can see and delete their asynchronous searches but they cannot see yours. The information provided focuses on how to configure fine-grained access control for asynchronous searches using backend roles, but does not address the specific issue you are facing.","usage":"{\"inputTokens\": 1970, \"outputTokens\": 90, \"totalTokens\": 2060}","latency":1890,"reasoning":"None","score":null,"faithfulness":0.0,"answer_relevancy":0.148,"cost":0.000605}
