query_text,context
I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working? ,"---
layout: default
title: Index codecs
nav_order: 3
parent: Index settings
---

# Index codecs

Index codecs determine how the index’s stored fields are compressed and stored on disk. The index codec is controlled by the static `index.codec` setting that specifies the compression algorithm. The setting impacts the index shard size and index operation performance.  

## Supported codecs

OpenSearch provides support for four codecs that can be used for compressing the stored fields. Each codec offers different tradeoffs between compression ratio (storage size) and indexing performance (speed): 

* `default` -- This codec employs the [LZ4 algorithm](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) with a preset dictionary, which prioritizes performance over compression ratio. It offers faster indexing and search operations when compared with `best_compression` but may result in larger index/shard sizes. If no codec is provided in the index settings, then LZ4 is used as the default algorithm for compression.
* `best_compression` -- This codec uses [zlib](https://en.wikipedia.org/wiki/Zlib) as an underlying algorithm for compression. It achieves high compression ratios that result in smaller index sizes. However, this may incur additional CPU usage during index operations and may subsequently result in high indexing and search latencies. 

As of OpenSearch 2.9, two new codecs based on the [Zstandard compression algorithm](https://github.com/facebook/zstd) are available. This algorithm provides a good balance between compression ratio and speed.

It may be challenging to change the codec setting of an existing index (see [Changing an index codec](#changing-an-index-codec)), so it is important to test a representative workload in a non-production environment before using a new codec setting.
{: .important}

* `zstd` (OpenSearch 2.9 and later) -- This codec provides significant compression comparable to the `best_compression` codec with reasonable CPU usage and improved indexing and search performance compared to the `default` codec.
* `zstd_no_dict` (OpenSearch 2.9 and later) -- This codec is similar to `zstd` but excludes the dictionary compression feature. It provides faster indexing and search operations compared to `zstd` at the expense of a slightly larger index size.

As of OpenSearch 2.10, the `zstd` and `zstd_no_dict` compression codecs cannot be used for [k-NN]({{site.url}}{{site.baseurl}}/search-plugins/knn/index/) or [Security Analytics]({{site.url}}{{site.baseurl}}/security-analytics/index/) indexes.
{: .warning}

For the `zstd` and `zstd_no_dict` codecs, you can optionally specify a compression level in the `index.codec.compression_level` setting. This setting takes integers in the [1, 6] range. A higher compression level results in a higher compression ratio (smaller storage size) with a tradeoff in speed (slower compression and decompression speeds lead to greater indexing and search latencies). 

When an index segment is created, it uses the current index codec for compression. If you update the index codec, any segment created after the update will use the new compression algorithm. For specific operation considerations, see [Index codec considerations for index operations](#index-codec-considerations-for-index-operations).
{: .note}

As of OpenSearch 2.15, hardware-accelerated compression codecs for the `DEFLATE` and `LZ4` compression algorithms are available. These hardware-accelerated codecs are available on the latest 4th and 5th Gen Intel®️ Xeon®️ processors running Linux kernel 3.10 and later. For all other systems and platforms, the codecs use that platform's corresponding software implementations. 

The new hardware-accelerated codecs can be used by setting one of the following `index.codec` values:
* `qat_lz4` (OpenSearch 2.15 and later): Hardware-accelerated `LZ4`
* `qat_deflate` (OpenSearch 2.15 and later): Hardware-accelerated `DEFLATE`

`qat_deflate` offers a much better compression ratio than `qat_lz4`, with a modest drop in compression and decompression speed.
{: .note}

The `index.codec.compression_level` setting can be used to specify the compression level for both `qat_lz4` and `qat_deflate`. 

The `index.codec.qatmode` setting controls the behavior of the hardware accelerator and uses one of the following values:

* `auto`: If hardware acceleration fails, then the algorithm switches to software acceleration.
* `hardware`: Guarantees hardware-only compression. If hardware is not available, then an exception occurs until hardware exists.

For information about the `index.codec.qatmode` setting's effects on snapshots, see the [Snapshots](#snapshots) section.

For more information about hardware acceleration on Intel, see the [Intel (R) QAT accelerator overview](https://www.intel.com/content/www/us/en/developer/topic-technology/open/quick-assist-technology/overview.html).

## Choosing a codec 

The choice of index codec impacts the amount of disk space required to store the index data. Codecs like `best_compression`, `zstd`, and `zstd_no_dict` can achieve higher compression ratios, resulting in smaller index sizes. Conversely, the `default` codec doesn’t prioritize compression ratio, resulting in larger index sizes but faster search operations than `best_compression`.

## Index codec considerations for index operations

The following index codec considerations apply to various index operations.

### Writes

Every index consists of shards, each of which is further divided into Lucene segments. During index writes, the new segments are created based on the codec specified in the index settings. If you update the codec for an index, the new segments will use the new codec algorithm. 

### Merges

During segment merges, OpenSearch combines smaller index segments into larger segments in order to provide optimal resource utilization and improve performance. The index codec setting influences the speed and efficiency of the merge operations. The number of merges that happen on an index is a factor of the segment size, and a smaller segment size directly translates into smaller merge sizes. If you update the `index.codec` setting, the new merge operations will use the new codec when creating merged segments. The merged segments will have the compression characteristics of the new codec.

### Splits and shrinks

The [Split API]({{site.url}}{{site.baseurl}}/api-reference/index-apis/split/) splits an original index into a new index where each original primary shard is divided into two or more primary shards. The [Shrink API]({{site.url}}{{site.baseurl}}/api-reference/index-apis/shrink-index/) shrinks an existing index to a new index with a smaller number of primary shards. As part of split or shrink operations, any newly created segments will use the latest codec settings.

### Snapshots

When creating a [snapshot]({{site.url}}{{site.baseurl}}/tuning-your-cluster/availability-and-recovery/snapshots/index/), the index codec setting influences the size of the snapshot and the time required for its creation. If the codec of an index is updated, newly created snapshots will use the latest codec setting. The resulting snapshot size will reflect the compression characteristics of the latest codec setting. Existing segments included in the snapshot will retain their original compression characteristics. 

When you restore the indexes from a snapshot of a cluster to another cluster, it is important to verify that the target cluster supports the codecs of the segments in the source snapshot. For example, if the source snapshot contains segments of the `zstd` or `zstd_no_dict` codecs (introduced in OpenSearch 2.9), you won't be able to restore the snapshot to a cluster that runs on an older OpenSearch version because it doesn't support these codecs. 

For hardware-accelerated compression codecs, available in OpenSearch 2.15 and later, the value of `index.codec.qatmode` affects how snapshots and restores are performed. If the value is `auto` (the default), then snapshots and restores work without issue. However, if the value is `hardware`, then it must be reset to `auto` in order for the restore process to succeed on systems lacking the hardware accelerator.

You can modify the value of `index.codec.qatmode` during the restore process by setting its value as follows: `""index_settings"": {""index.codec.qatmode"": ""auto""}`.
{: .note}

### Reindexing

When you are performing a [reindex]({{site.url}}{{site.baseurl}}/im-plugin/reindex-data/) operation from a source index, the new segments created in the target index will have the properties of the codec settings of the target index. 

### Index rollups and transforms

When an index [rollup]({{site.url}}{{site.baseurl}}/im-plugin/index-rollups/) or [transform]({{site.url}}{{site.baseurl}}/im-plugin/index-transforms/) job is completed, the segments created in the target index will have the properties of the index codec specified during target index creation, irrespective of the source index codec. If the target index is created dynamically through a rollup job, the default codec is used for segments of the target index.

## Changing an index codec

It is not possible to change the codec setting of an open index. You can close the index, apply the new index codec setting, and reopen the index, at which point only new segments will be written with the new codec. This requires stopping all reads and writes to the index for a brief period to make the codec change and may result in inconsistent segment sizes and compression ratios. Alternatively, you can reindex all data from a source index into a new index with a different codec setting, though this is a very resource-intensive operation.

## Performance tuning and benchmarking

Depending on your specific use case, you might need to experiment with different index codec settings to fine-tune the performance of your OpenSearch cluster. Conducting benchmark tests with different codecs and measuring the impact on indexing speed, search performance, and resource utilization can help you identify the optimal index codec setting for your workload. With the `zstd` and `zstd_no_dict` codecs, you can also fine-tune the compression level in order to identify the optimal configuration for your cluster.

### Benchmarking

The following table provides a performance comparison of the `best_compression`, `zstd`, and `zstd_no_dict` codecs against the `default` codec. The tests were performed with the [`nyc_taxi`](https://github.com/topics/nyc-taxi-dataset) dataset. The results are listed in terms of percent change, and bold results indicate performance improvement.

| | `best_compression` | `zstd` | `zstd_no_dict` |
|:---	|:---	|:---	|:--- |
|**Write** | | | 
|Median Latency	|0%	|0%	|&minus;1%	|
|p90 Latency	|3%	|2%	|**&minus;5%**	|
|Throughput	|&minus;2%	|**7%**	|**14%**	|
|**Read**	| | | 
|Median Latency	|0%	|1%	|0%	|
|p90 Latency	|1%	|1%	|**&minus;2%**	|
|**Disk**	| | | 
| Compression ratio	|**&minus;34%**	|**&minus;35%**	|**&minus;30%**	|

"
"I'm trying to set up this new aggregate view thing for saved objects in OpenSearch Dashboards, but I'm worried about messing up our existing multi-tenancy setup. The docs mention something about tenant indexes and a kibana_server role. How do I make sure I don't break anything when I turn this feature on? And what's the deal with not being able to turn it off once it's enabled","---
layout: default
title: Dynamic configuration in OpenSearch Dashboards
parent: OpenSearch Dashboards multi-tenancy
nav_order: 147
---


# Dynamic configuration in OpenSearch Dashboards

Multi-tenancy includes dynamic configuration options in OpenSearch Dashboards so you can manage common settings for tenancy without having to make changes to the configuration YAML files on each node and then restart the cluster. You can take advantage of this functionality by using the Dashboards interface or the REST API. The following list includes descriptions of the options currently covered by dynamic configuration:

- **Disable or enable multi-tenancy**: Administrators can disable and enable multi-tenancy dynamically. Disabling multi-tenancy does not pose a risk of data loss. If and when an administrator chooses to reenable tenancy, all previously saved objects are preserved and made available. The default is `multitenancy_enabled: true`.
  
  This setting does not have an impact on the global tenant, which always remains enabled.
  {: .note }

- **Disable or enable private tenant**: This option allows administrators to enable and disable private tenants. As with the enable multi-tenancy setting, when private tenants are reenabled all previously saved objects are preserved and made available.
- **Default tenant**: This option allows an administrator to choose either a global, private, or custom tenant as the default when users log in. In cases where a user doesn't have access to the default tenant (for example, if a custom tenant unavailable to the user was specified as the default), the default transitions to the preferred tenant, which is specified by the `opensearch_security.multitenancy.tenants.preferred` setting in the `opensearch-dashboards.yml` file. See [Multi-tenancy configuration]({{site.url}}{{site.baseurl}}/security/multi-tenancy/multi-tenancy-config/) for more information about this setting.

Depending on the specific changes made to multi-tenancy using dynamic configuration, some users may be logged out of their Dashboards session once the changes are saved. For example, if an admin user disables multi-tenancy, users with either a private or custom tenant as their selected tenant will be logged out and will need to log back in. Similarly, if an admin user disables private tenants, users with the private tenant selected will be logged out and will need to log back in. 

The global tenant, however, is a special case. Because this tenant is never disabled, users with the global tenant selected as their active tenant will experience no interruption to their session. Furthermore, changing the default tenant has no impact on a user's session.


## Configuring multi-tenancy in OpenSearch Dashboards

To configure multi-tenancy in Dashboards, follow these steps:

1. Begin by selecting **Security** in the Dashboards home page menu. Then select **Tenancy** from the Security menu on the left side of the screen. The **Multi-tenancy** page is displayed. 
1. By default, the **Manage** tab is displayed. Select the **Configure** tab to display the dynamic settings for multi-tenancy.
   * In the **Multi-tenancy** field, select the **Enable tenancy** check box to enable multi-tenancy. Clear the check box to disable the feature. The default is `true`.
   * In the **Tenants** field, you can enable or disable private tenants for users. By default the check box is selected and the feature is enabled.
   * In the **Default tenant** field, use the dropdown menu to select a default tenant. The menu includes Global, Private, and any other custom tenants that are available to users.
1. After making your preferred changes, select **Save changes** in the lower right corner of the window. A pop-up window appears listing the configuration items you've changed and asks you to review your changes.
1. Select the check boxes beside the items you want to confirm and then select **Apply changes**. The changes are implemented dynamically.


## Configuring multi-tenancy with the REST API

In addition to using the Dashboards interface, you can manage dynamic configurations using the REST API. 

### Get tenancy configuration

The GET call retrieves settings for the dynamic configuration:

```json
GET /_plugins/_security/api/tenancy/config
```
{% include copy-curl.html %}

#### Example response

```json
{
    ""mulitenancy_enabled"": true,
    ""private_tenant_enabled"": true,
    ""default_tenant"": ""global tenant""
}
```

### Update tenant configuration

The PUT call updates settings for dynamic configuration:

```json
PUT /_plugins/_security/api/tenancy/config
{
    ""default_tenant"": ""custom tenant 1"",
    ""private_tenant_enabled"": false,
    ""mulitenancy_enabled"": true
}
```
{% include copy-curl.html %}

### Example response

```json
{
    ""mulitenancy_enabled"": true,
    ""private_tenant_enabled"": false,
    ""default_tenant"": ""custom tenant 1""
}
```

### Dashboardsinfo API

You can also use the Dashboardsinfo API to retrieve the status of multi-tenancy settings for the user logged in to Dashboards:

```json
GET /_plugins/_security/dashboardsinfo
```
{% include copy-curl.html %}

### Example response

```json
{
  ""user_name"" : ""admin"",
  ""not_fail_on_forbidden_enabled"" : false,
  ""opensearch_dashboards_mt_enabled"" : true,
  ""opensearch_dashboards_index"" : "".kibana"",
  ""opensearch_dashboards_server_user"" : ""kibanaserver"",
  ""multitenancy_enabled"" : true,
  ""private_tenant_enabled"" : true,
  ""default_tenant"" : ""Private""
}
```

"
What software license does Opensearch use? ,"<img src=""https://opensearch.org/assets/img/opensearch-logo-themed.svg"" height=""64px"">

# About the OpenSearch documentation repo

The `documentation-website` repository contains the user documentation for OpenSearch. You can find the rendered documentation at [opensearch.org/docs](https://opensearch.org/docs).


## Contributing

Community contributions remain essential to keeping the documentation comprehensive, useful, well organized, and up to date. If you are interested in submitting an issue or contributing content, see [CONTRIBUTING](CONTRIBUTING.md). 

The following resources provide important guidance regarding contributions to the documentation:  

- [OpenSearch Project Style Guidelines](STYLE_GUIDE.md) -- The style guide covers the style standards to be observed when creating OpenSearch Project content.
- [OpenSearch terms](TERMS.md) -- The terms list contains key OpenSearch terms and tips on how and when to use them.  
- [API Style Guide](API_STYLE_GUIDE.md) -- The API Style Guide provides the basic structure for creating OpenSearch API documentation.
- [Formatting Guide](FORMATTING_GUIDE.md) -- The OpenSearch documentation uses a modified version of the [just-the-docs](https://github.com/pmarsceill/just-the-docs) Jekyll theme. The Formatting Guide provides an overview of the commonly used formatting elements and how to add a page to the website.


## Points of contact

If you encounter problems or have questions when contributing to the documentation, these people can help:

- [kolchfa-aws](https://github.com/kolchfa-aws)
- [Naarcha-AWS](https://github.com/Naarcha-AWS)
- [vagimeli](https://github.com/vagimeli)


## Code of conduct

This project has adopted an [Open Source Code of Conduct](https://opensearch.org/codeofconduct.html).


## Security

If you discover a potential security issue in this project, notify OpenSearch Security directly by emailing security@opensearch.org. To prevent any additional risk caused by the potential issue, do **not** create a public GitHub issue.

## License

This project is licensed under the [Apache 2.0 License](LICENSE).


## Copyright

Copyright OpenSearch contributors.
"
Does GPU accelerated nodes support Pytorch?,"---
layout: default
title: GPU acceleration
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 150
---


# GPU acceleration 

When running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training. 

## Supported GPUs

Currently, ML nodes support the following GPU instances:

- [NVIDIA instances with CUDA 11.6](https://aws.amazon.com/nvidia/)
- [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)

If you need GPU power, you can provision GPU instances through [Amazon Elastic Compute Cloud (Amazon EC2)](https://aws.amazon.com/ec2/). For more information on how to provision a GPU instance, see [Recommended GPU Instances](https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html).

## Supported images

You can use GPU acceleration with both [Docker images](https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md) with CUDA 11.6 and [Amazon Machine Images (AMIs)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html).

## PyTorch

GPU-accelerated ML nodes require [PyTorch](https://pytorch.org/docs/stable/index.html) 1.12.1 work with ML models.

## Setting up a GPU-accelerated ML node

Depending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts. 

### Preparing an NVIDIA ML node

NVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the `nvidia-uvm` kernel inside the `/dev` directory. To check for the kernel, enter `ls -al /dev | grep nvidia-uvm`.

If the `nvidia-uvm` kernel does not exist, run `nvidia-uvm-init.sh`:

```
#!/bin/bash
## Script to initialize nvidia device nodes.
## https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications
/sbin/modprobe nvidia
if [ ""$?"" -eq 0 ]; then
  # Count the number of NVIDIA controllers found.
  NVDEVS=`lspci | grep -i NVIDIA`
  N3D=`echo ""$NVDEVS"" | grep ""3D controller"" | wc -l`
  NVGA=`echo ""$NVDEVS"" | grep ""VGA compatible controller"" | wc -l`
  N=`expr $N3D + $NVGA - 1`
  for i in `seq 0 $N`; do
    mknod -m 666 /dev/nvidia$i c 195 $i
  done
  mknod -m 666 /dev/nvidiactl c 195 255
else
  exit 1
fi
/sbin/modprobe nvidia-uvm
if [ ""$?"" -eq 0 ]; then
  # Find out the major device number used by the nvidia-uvm driver
  D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`
  mknod -m 666 /dev/nvidia-uvm c $D 0
  mknod -m 666 /dev/nvidia-uvm-tools c $D 0
else
  exit 1
fi
```

After verifying that `nvidia-uvm` exists under `/dev`, you can start OpenSearch inside your cluster. 

### Preparing AWS Inferentia ML node

Depending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster. 

To start, [download and install OpenSearch]({{site.url}}{{site.baseurl}}/install-and-configure/index/) on your cluster.

Then export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory `opensearch-2.5.0`, so `OPENSEARCH_HOME` = `opensearch-2.5.0`:

```
echo ""export OPENSEARCH_HOME=~/opensearch-2.5.0"" | tee -a ~/.bash_profile
echo ""export PYTORCH_VERSION=1.12.1"" | tee -a ~/.bash_profile
source ~/.bash_profile
```

Next, create a shell script file called `prepare_torch_neuron.sh`. You can copy and customize one of the following examples based on your Linux operating system:

- [Ubuntu 20.04](#ubuntu-2004)
- [Amazon Linux 2](#amazon-linux-2)

After you've run the scripts, exit your current terminal and open a new terminal to start OpenSearch.

GPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems.
{: .note}

#### Ubuntu 20.04

```
. /etc/os-release
sudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF
deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main
EOF
wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -

# Update OS packages
sudo apt-get update -y

################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################

# Install OS headers
sudo apt-get install linux-headers-$(uname -r) -y

# Install Neuron Driver
sudo apt-get install aws-neuronx-dkms -y

####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################

# Install Neuron Tools
sudo apt-get install aws-neuronx-tools -y

######################################################
#   Only for Ubuntu 20 - Install Python3.7
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt-get install python3.7
######################################################
# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo apt-get install -y python3.7-venv g++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set pip repository to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

#Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf==3.20.1"" torchvision

# If you need to trace neuron model, install the transformers for tracing the Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile

# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

#### Amazon Linux 2

```
# Configure Linux for Neuron repository updates
sudo tee /etc/yum.repos.d/neuron.repo > /dev/null <<EOF
[neuron]
name=Neuron YUM Repository
baseurl=https://yum.repos.neuron.amazonaws.com
enabled=1
metadata_expire=0
EOF
sudo rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB
# Update OS packages
sudo yum update -y
################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################
# Install OS headers
sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y
# Install Neuron Driver
####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################
sudo yum install aws-neuronx-dkms -y
# Install Neuron Tools
sudo yum install aws-neuronx-tools -y

# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo yum install -y python3.7-venv gcc-c++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set Pip repository  to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

# Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf<4"" torchvision

# If you need to run the trace neuron model, install transformers for tracing Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile
# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

When the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch.

OpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually.

#### Prepare ML node manually

If the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually:

1. Deploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see [Deploy on AWS accelerator instance](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/setup/pytorch-install.html#deploy-on-aws-ml-accelerator-instance).

2. Copy the Neuron library into OpenSearch. The following command uses a directory named `opensearch-2.5.0`:

   ```
   OPENSEARCH_HOME=~/opensearch-2.5.0
   ```

3. Set the `PYTORCH_EXTRA_LIBRARY_PATH` path. In this example, we create a `pytorch` virtual environment in the OPENSEARCH_HOME folder:

   ```
   PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/


   mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r  $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
   export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
  ```

4. (Optional) To monitor the GPU usage of your accelerator instance, install [Neuron tools](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/index.html), which allows models to be used inside your instance:

   ```
   # Install Neuron Tools
   sudo apt-get install aws-neuronx-tools -y
   ```

   ```
   # Add Neuron tools your PATH
   export PATH=/opt/aws/neuron/bin:$PATH
   ```
  
   ```
   # Test Neuron tools
   neuron-top
   ```


5. To make sure you have enough memory to upload a model, increase the JVM stack size to `>+2MB`:

   ```
   echo ""-Xss2m"" | sudo tee -a $OPENSEARCH_HOME/config/jvm.options
   ```

6. Start OpenSearch. 

## Troubleshooting

Due to the amount of data required to work with ML models, you might encounter the following `max file descriptors` or `vm.max_map_count` errors when trying to run OpenSearch in a your cluster: 

```
[1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535]
[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
```

To troubleshoot the max file descriptors error, run the following command:

```
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
```

To fix the `vm.max_map_count` error, run this command to increase the count to `262114`:

```
sudo sysctl -w vm.max_map_count=262144
```

## Next steps

If you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see [Compiling and Deploying HuggingFace Pretrained BERT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html).

"
Does opensearch support hugging face models? If so which ones? ,"---
layout: default
title: Pretrained models
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 120
---

# OpenSearch-provided pretrained models
**Introduced 2.9**
{: .label .label-purple }

OpenSearch provides a variety of open-source pretrained models that can assist with a range of machine learning (ML) search and analytics use cases. You can upload any supported model to the OpenSearch cluster and use it locally.

## Supported pretrained models

OpenSearch supports the following models, categorized by type. Text embedding models are sourced from [Hugging Face](https://huggingface.co/). Sparse encoding models are trained by OpenSearch. Although models with the same type will have similar use cases, each model has a different model size and will perform differently depending on your cluster setup. For a performance comparison of some pretrained models, see the [SBERT documentation](https://www.sbert.net/docs/pretrained_models.html#model-overview).

Running local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.
{: .important}

### Sentence transformers

Sentence transformer models map sentences and paragraphs across a dimensional dense vector space. The number of vectors depends on the type of model. You can use these models for use cases such as clustering or semantic search.

The following table provides a list of sentence transformer models and artifact links you can use to download them. Note that you must prefix the model name with `huggingface/`, as shown in the **Model name** column. 

| Model name | Version | Vector dimensions | Auto-truncation | TorchScript artifact | ONNX artifact |
|:---|:---|:---|:---|:---|:---|
| `huggingface/sentence-transformers/all-distilroberta-v1` | 1.0.1 | 768-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/torch_script/sentence-transformers_all-distilroberta-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/onnx/sentence-transformers_all-distilroberta-v1-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/all-MiniLM-L6-v2` | 1.0.1 | 384-dimensional dense vector space.  | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/torch_script/sentence-transformers_all-MiniLM-L6-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/onnx/sentence-transformers_all-MiniLM-L6-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/all-MiniLM-L12-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/torch_script/sentence-transformers_all-MiniLM-L12-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/onnx/sentence-transformers_all-MiniLM-L12-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/all-mpnet-base-v2` | 1.0.1 | 768-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/torch_script/sentence-transformers_all-mpnet-base-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/onnx/sentence-transformers_all-mpnet-base-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/msmarco-distilbert-base-tas-b` | 1.0.2 | 768-dimensional dense vector space. Optimized for semantic search. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/torch_script/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.2-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/onnx/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.2-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/onnx/config.json) |
| `huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1` | 1.0.1 | 384-dimensional dense vector space. Designed for semantic search and trained on 215 million question/answer pairs. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/torch_script/sentence-transformers_multi-qa-MiniLM-L6-cos-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/onnx/sentence-transformers_multi-qa-MiniLM-L6-cos-v1-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/torch_script/sentence-transformers_multi-qa-mpnet-base-dot-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/onnx/sentence-transformers_multi-qa-mpnet-base-dot-v1-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/torch_script/sentence-transformers_paraphrase-MiniLM-L3-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/onnx/sentence-transformers_paraphrase-MiniLM-L3-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/torch_script/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/onnx/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/paraphrase-mpnet-base-v2` | 1.0.0 | 768-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/torch_script/sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/onnx/sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/onnx/config.json) |
| `huggingface/sentence-transformers/distiluse-base-multilingual-cased-v1` | 1.0.1 | 512-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/distiluse-base-multilingual-cased-v1/1.0.1/torch_script/sentence-transformers_distiluse-base-multilingual-cased-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/distiluse-base-multilingual-cased-v1/1.0.1/torch_script/config.json) | Not available |


### Sparse encoding models
**Introduced 2.11**
{: .label .label-purple }

Sparse encoding models transfer text into a sparse vector and convert the vector to a list of `<token: weight>` pairs representing the text entry and its corresponding weight in the sparse vector. You can use these models for use cases such as clustering or sparse neural search.

We recommend the following combinations for optimal performance:

- Use the `amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill` model during both ingestion and search.
- Use the `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill` model during ingestion and the
`amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1` tokenizer during search.

For more information about the preceding options for running neural sparse search, see [Generating sparse vector embeddings within OpenSearch]({{site.url}}{{site.baseurl}}/search-plugins/neural-sparse-with-pipelines/).

The following table provides a list of sparse encoding models and artifact links you can use to download them.

| Model name | Version | Auto-truncation | TorchScript artifact | Description |
|:---|:---|:---|:---|:---|
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-v1` | 1.0.1 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v1/1.0.1/torch_script/neural-sparse_opensearch-neural-sparse-encoding-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v1/1.0.1/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v1). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill` | 1.0.0 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill/1.0.0/torch_script/neural-sparse_opensearch-neural-sparse-encoding-v2-distill-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill/1.0.0/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v2-distill). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v1` | 1.0.1 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v1/1.0.1/torch_script/neural-sparse_opensearch-neural-sparse-encoding-doc-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v1/1.0.1/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v1). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill` | 1.0.0 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill/1.0.0/torch_script/neural-sparse_opensearch-neural-sparse-encoding-doc-v2-distill-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill/1.0.0/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-mini` | 1.0.0 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-mini/1.0.0/torch_script/neural-sparse_opensearch-neural-sparse-encoding-doc-v2-mini-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-mini/1.0.0/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini). |
| `amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1` | 1.0.1 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1/1.0.1/torch_script/neural-sparse_opensearch-neural-sparse-tokenizer-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1/1.0.1/torch_script/config.json) | A neural sparse tokenizer. The tokenizer splits text into tokens and assigns each token a predefined weight, which is the token's inverse document frequency (IDF). If the IDF file is not provided, the weight defaults to 1. For more information, see [Preparing a model]({{site.url}}{{site.baseurl}}/ml-commons-plugin/custom-local-models/#preparing-a-model). |

### Cross-encoder models
**Introduced 2.12**
{: .label .label-purple }

Cross-encoder models support query reranking. 

The following table provides a list of cross-encoder models and artifact links you can use to download them. Note that you must prefix the model name with `huggingface/cross-encoders`, as shown in the **Model name** column. 

| Model name | Version | TorchScript artifact | ONNX artifact |
|:---|:---|:---|:---|
| `huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2` | 1.0.2 | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/torch_script/cross-encoders_ms-marco-MiniLM-L-6-v2-1.0.2-torch_script.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/onnx/cross-encoders_ms-marco-MiniLM-L-6-v2-1.0.2-onnx.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/onnx/config.json) |
| `huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2` | 1.0.2 | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/torch_script/cross-encoders_ms-marco-MiniLM-L-12-v2-1.0.2-torch_script.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/onnx/cross-encoders_ms-marco-MiniLM-L-12-v2-1.0.2-onnx.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/onnx/config.json) 

## Prerequisites

On clusters with dedicated ML nodes, specify `""only_run_on_ml_node"": ""true""` for improved performance. For more information, see [ML Commons cluster settings]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/). 

This example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. To ensure that this basic local setup works, specify the following cluster settings:

```json
PUT _cluster/settings
{
  ""persistent"": {
    ""plugins"": {
      ""ml_commons"": {
        ""only_run_on_ml_node"": ""false"",
        ""model_access_control_enabled"": ""true"",
        ""native_memory_threshold"": ""99""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Step 1: Register a model group

To register a model, you have the following options:

- You can use `model_group_id` to register a model version to an existing model group.
- If you do not use `model_group_id`, ML Commons creates a model with a new model group.

To register a model group, send the following request:

```json
POST /_plugins/_ml/model_groups/_register
{
  ""name"": ""local_model_group"",
  ""description"": ""A model group for local models""
}
```
{% include copy-curl.html %}

The response contains the model group ID that you'll use to register a model to this model group:

```json
{
 ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
 ""status"": ""CREATED""
}
```

To learn more about model groups, see [Model access control]({{site.url}}{{site.baseurl}}/ml-commons-plugin/model-access-control/).

## Step 2: Register a local OpenSearch-provided model

To register an OpenSearch-provided model to the model group created in step 1, provide the model group ID from step 1 in the following request.

Because pretrained models originate from the ML Commons model repository, you only need to provide the `name`, `version`, `model_group_id`, and `model_format` in the register API request:  

```json
POST /_plugins/_ml/models/_register
{
  ""name"": ""huggingface/sentence-transformers/msmarco-distilbert-base-tas-b"",
  ""version"": ""1.0.2"",
  ""model_group_id"": ""Z1eQf4oB5Vm0Tdw8EIP2"",
  ""model_format"": ""TORCH_SCRIPT""
}
```
{% include copy-curl.html %}

OpenSearch returns the task ID of the register operation:

```json
{
  ""task_id"": ""cVeMb4kBJ1eYAeTMFFgj"",
  ""status"": ""CREATED""
}
```

To check the status of the operation, provide the task ID to the [Tasks API]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/tasks-apis/get-task/):

```bash
GET /_plugins/_ml/tasks/cVeMb4kBJ1eYAeTMFFgj
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""REGISTER_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""XPcXLV7RQoi5m8NI_jEOVQ""
  ],
  ""create_time"": 1689793598499,
  ""last_update_time"": 1689793598530,
  ""is_async"": false
}
```

Take note of the returned `model_id` because you’ll need it to deploy the model.

## Step 3: Deploy the model

The deploy operation reads the model's chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into and longer it takes for the model to load into memory.

To deploy the registered model, provide its model ID from step 3 in the following request:

```bash
POST /_plugins/_ml/models/cleMb4kBJ1eYAeTMFFg4/_deploy
```
{% include copy-curl.html %}

The response contains the task ID that you can use to check the status of the deploy operation:

```json
{
  ""task_id"": ""vVePb4kBJ1eYAeTM7ljG"",
  ""status"": ""CREATED""
}
```

As in the previous step, check the status of the operation by calling the Tasks API:

```bash
GET /_plugins/_ml/tasks/vVePb4kBJ1eYAeTM7ljG
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""DEPLOY_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""n-72khvBTBi3bnIIR8FTTw""
  ],
  ""create_time"": 1689793851077,
  ""last_update_time"": 1689793851101,
  ""is_async"": true
}
```

If a cluster or node is restarted, then you need to redeploy the model. To learn how to set up automatic redeployment, see [Enable auto redeploy]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/#enable-auto-redeploy).
{: .tip} 

## Step 4 (Optional): Test the model

Use the [Predict API]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/train-predict/predict/) to test the model.

### Text embedding model

For a text embedding model, send the following request:

```json
POST /_plugins/_ml/_predict/text_embedding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""],
  ""return_number"": true,
  ""target_response"": [""sentence_embedding""]
}
```
{% include copy-curl.html %}

The response contains text embeddings for the provided sentence:

```json
{
  ""inference_results"" : [
    {
      ""output"" : [
        {
          ""name"" : ""sentence_embedding"",
          ""data_type"" : ""FLOAT32"",
          ""shape"" : [
            768
          ],
          ""data"" : [
            0.25517133,
            -0.28009856,
            0.48519906,
            ...
          ]
        }
      ]
    }
  ]
}
```

### Sparse encoding model

For a sparse encoding model, send the following request:

```json
POST /_plugins/_ml/_predict/sparse_encoding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""]
}
```
{% include copy-curl.html %}

The response contains the tokens and weights:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""name"": ""output"",
          ""dataAsMap"": {
            ""response"": [
              {
                ""saturday"": 0.48336542,
                ""week"": 0.1034762,
                ""mood"": 0.09698499,
                ""sunshine"": 0.5738209,
                ""bright"": 0.1756877,
                ...
              }
          }
        }
    }
}
```

### Cross-encoder model

For a cross-encoder model, send the following request:

```json
POST _plugins/_ml/models/<model_id>/_predict
{
    ""query_text"": ""today is sunny"",
    ""text_docs"": [
        ""how are you"",
        ""today is sunny"",
        ""today is july fifth"",
        ""it is winter""
    ]
}
```
{% include copy-curl.html %}

The model calculates the similarity score of `query_text` and each document in `text_docs` and returns a list of scores for each document in the order they were provided in `text_docs`:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            -6.077798
          ],
          ""byte_buffer"": {
            ""array"": ""Un3CwA=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    },
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            10.223609
          ],
          ""byte_buffer"": {
            ""array"": ""55MjQQ=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    },
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            -1.3987057
          ],
          ""byte_buffer"": {
            ""array"": ""ygizvw=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    },
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            -4.5923924
          ],
          ""byte_buffer"": {
            ""array"": ""4fSSwA=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    }
  ]
}
```

A higher document score means higher similarity. In the preceding response, documents are scored as follows against the query text `today is sunny`:

Document text | Score
:--- | :---
`how are you` | -6.077798
`today is sunny` | 10.223609
`today is july fifth` | -1.3987057
`it is winter` | -4.5923924

The document that contains the same text as the query is scored the highest, and the remaining documents are scored based on the text similarity.

## Step 5: Use the model for search

To learn how to set up a vector index and use text embedding models for search, see [Semantic search]({{site.url}}{{site.baseurl}}/search-plugins/semantic-search/).

To learn how to set up a vector index and use sparse encoding models for search, see [Neural sparse search]({{site.url}}{{site.baseurl}}/search-plugins/neural-sparse-search/).

To learn how to use cross-encoder models for reranking, see [Reranking search results]({{site.url}}{{site.baseurl}}/search-plugins/search-relevance/reranking-search-results/).

"
"I have a custom model, can I run it in Opensearch?","---
layout: default
title: Custom models
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 120
---

# Custom local models
**Introduced 2.9**
{: .label .label-purple }

To use a custom model locally, you can upload it to the OpenSearch cluster.

## Model support

As of OpenSearch 2.6, OpenSearch supports local text embedding models.

As of OpenSearch 2.11, OpenSearch supports local sparse encoding models.

As of OpenSearch 2.12, OpenSearch supports local cross-encoder models.

As of OpenSearch 2.13, OpenSearch supports local question answering models.

Running local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.
{: .important}

## Preparing a model

For all the models, you must provide a tokenizer JSON file within the model zip file.

For sparse encoding models, make sure your output format is `{""output"":<sparse_vector>}` so that ML Commons can post-process the sparse vector.

If you fine-tune a sparse model on your own dataset, you may also want to use your own sparse tokenizer model. It is preferable to provide your own [IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) JSON file in the tokenizer model zip file because this increases query performance when you use the tokenizer model in the query. Alternatively, you can use an OpenSearch-provided generic [IDF from MSMARCO](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1/1.0.0/torch_script/opensearch-neural-sparse-tokenizer-v1-1.0.0.zip). If the IDF file is not provided, the default weight of each token is set to 1, which may influence sparse neural search performance.  

### Model format

To use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the [TorchScript](https://pytorch.org/docs/stable/jit.html) and [ONNX](https://onnx.ai/) formats.

You must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript [model file](https://github.com/opensearch-project/ml-commons/blob/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip).

Additionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:

```bash
shasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip
```

### Model size

Most deep learning models are more than 100 MB, making it difficult to fit them into a single document. OpenSearch splits the model file into smaller chunks to be stored in a model index. When allocating ML or data nodes for your OpenSearch cluster, make sure you correctly size your ML nodes so that you have enough memory when making ML inferences.

## Prerequisites 

To upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from [Hugging Face](https://huggingface.co/), or train a new model in accordance with your needs.

### Cluster settings

This example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. 

On clusters with dedicated ML nodes, specify `""only_run_on_ml_node"": ""true""` for improved performance. For more information, see [ML Commons cluster settings]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/).

To ensure that this basic local setup works, specify the following cluster settings:

```json
PUT _cluster/settings
{
  ""persistent"": {
    ""plugins"": {
      ""ml_commons"": {
        ""allow_registering_model_via_url"": ""true"",
        ""only_run_on_ml_node"": ""false"",
        ""model_access_control_enabled"": ""true"",
        ""native_memory_threshold"": ""99""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Step 1: Register a model group

To register a model, you have the following options:

- You can use `model_group_id` to register a model version to an existing model group.
- If you do not use `model_group_id`, ML Commons creates a model with a new model group.

To register a model group, send the following request:

```json
POST /_plugins/_ml/model_groups/_register
{
  ""name"": ""local_model_group"",
  ""description"": ""A model group for local models""
}
```
{% include copy-curl.html %}

The response contains the model group ID that you'll use to register a model to this model group:

```json
{
 ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
 ""status"": ""CREATED""
}
```

To learn more about model groups, see [Model access control]({{site.url}}{{site.baseurl}}/ml-commons-plugin/model-access-control/).

## Step 2: Register a local model

To register a local model to the model group created in step 1, send a Register Model API request. For descriptions of Register Model API parameters, see [Register a model]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/model-apis/register-model/). 

The `function_name` corresponds to the model type. For text embedding models, set this parameter to `TEXT_EMBEDDING`. For sparse encoding models, set this parameter to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`. For cross-encoder models, set this parameter to `TEXT_SIMILARITY`. For question answering models, set this parameter to `QUESTION_ANSWERING`. In this example, set `function_name` to `TEXT_EMBEDDING` because you're registering a text embedding model. 

Provide the model group ID from step 1 and send the following request:

```json
POST /_plugins/_ml/models/_register
{
  ""name"": ""huggingface/sentence-transformers/msmarco-distilbert-base-tas-b"",
  ""version"": ""1.0.1"",
  ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
  ""description"": ""This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search."",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""model_format"": ""TORCH_SCRIPT"",
  ""model_content_size_in_bytes"": 266352827,
  ""model_content_hash_value"": ""acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413"",
  ""model_config"": {
    ""model_type"": ""distilbert"",
    ""embedding_dimension"": 768,
    ""framework_type"": ""sentence_transformers"",
    ""all_config"": ""{\\""_name_or_path\\"":\\""old_models/msmarco-distilbert-base-tas-b/0_Transformer\\"",\\""activation\\"":\\""gelu\\"",\\""architectures\\"":[\\""DistilBertModel\\""],\\""attention_dropout\\"":0.1,\\""dim\\"":768,\\""dropout\\"":0.1,\\""hidden_dim\\"":3072,\\""initializer_range\\"":0.02,\\""max_position_embeddings\\"":512,\\""model_type\\"":\\""distilbert\\"",\\""n_heads\\"":12,\\""n_layers\\"":6,\\""pad_token_id\\"":0,\\""qa_dropout\\"":0.1,\\""seq_classif_dropout\\"":0.2,\\""sinusoidal_pos_embds\\"":false,\\""tie_weights_\\"":true,\\""transformers_version\\"":\\""4.7.0\\"",\\""vocab_size\\"":30522}""
  },
  ""created_time"": 1676073973126,
  ""url"": ""https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.1/torch_script/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip""
}
```
{% include copy-curl.html %}

Note that in OpenSearch Dashboards, wrapping the `all_config` field contents in triple quotes (`""""""`) automatically escapes quotation marks within the field and provides better readability:

```json
POST /_plugins/_ml/models/_register
{
  ""name"": ""huggingface/sentence-transformers/msmarco-distilbert-base-tas-b"",
  ""version"": ""1.0.1"",
  ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
  ""description"": ""This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search."",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""model_format"": ""TORCH_SCRIPT"",
  ""model_content_size_in_bytes"": 266352827,
  ""model_content_hash_value"": ""acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413"",
  ""model_config"": {
    ""model_type"": ""distilbert"",
    ""embedding_dimension"": 768,
    ""framework_type"": ""sentence_transformers"",
    ""all_config"": """"""{""_name_or_path"":""old_models/msmarco-distilbert-base-tas-b/0_Transformer"",""activation"":""gelu"",""architectures"":[""DistilBertModel""],""attention_dropout"":0.1,""dim"":768,""dropout"":0.1,""hidden_dim"":3072,""initializer_range"":0.02,""max_position_embeddings"":512,""model_type"":""distilbert"",""n_heads"":12,""n_layers"":6,""pad_token_id"":0,""qa_dropout"":0.1,""seq_classif_dropout"":0.2,""sinusoidal_pos_embds"":false,""tie_weights_"":true,""transformers_version"":""4.7.0"",""vocab_size"":30522}""""""
  },
  ""created_time"": 1676073973126,
  ""url"": ""https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.1/torch_script/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip""
}
```
{% include copy.html %}

OpenSearch returns the task ID of the register operation:

```json
{
  ""task_id"": ""cVeMb4kBJ1eYAeTMFFgj"",
  ""status"": ""CREATED""
}
```

To check the status of the operation, provide the task ID to the [Get task]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/tasks-apis/get-task/):

```bash
GET /_plugins/_ml/tasks/cVeMb4kBJ1eYAeTMFFgj
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""REGISTER_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""XPcXLV7RQoi5m8NI_jEOVQ""
  ],
  ""create_time"": 1689793598499,
  ""last_update_time"": 1689793598530,
  ""is_async"": false
}
```

Take note of the returned `model_id` because you’ll need it to deploy the model.

## Step 3: Deploy the model

The deploy operation reads the model's chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into and longer it takes for the model to load into memory.

To deploy the registered model, provide its model ID from step 3 in the following request:

```bash
POST /_plugins/_ml/models/cleMb4kBJ1eYAeTMFFg4/_deploy
```
{% include copy-curl.html %}

The response contains the task ID that you can use to check the status of the deploy operation:

```json
{
  ""task_id"": ""vVePb4kBJ1eYAeTM7ljG"",
  ""status"": ""CREATED""
}
```

As in the previous step, check the status of the operation by calling the Tasks API:

```bash
GET /_plugins/_ml/tasks/vVePb4kBJ1eYAeTM7ljG
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""DEPLOY_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""n-72khvBTBi3bnIIR8FTTw""
  ],
  ""create_time"": 1689793851077,
  ""last_update_time"": 1689793851101,
  ""is_async"": true
}
```

If a cluster or node is restarted, then you need to redeploy the model. To learn how to set up automatic redeployment, see [Enable auto redeploy]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/#enable-auto-redeploy).
{: .tip} 

## Step 4 (Optional): Test the model

Use the [Predict API]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/train-predict/predict/) to test the model.

For a text embedding model, send the following request:

```json
POST /_plugins/_ml/_predict/text_embedding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""],
  ""return_number"": true,
  ""target_response"": [""sentence_embedding""]
}
```
{% include copy-curl.html %}

The response contains text embeddings for the provided sentence:

```json
{
  ""inference_results"" : [
    {
      ""output"" : [
        {
          ""name"" : ""sentence_embedding"",
          ""data_type"" : ""FLOAT32"",
          ""shape"" : [
            768
          ],
          ""data"" : [
            0.25517133,
            -0.28009856,
            0.48519906,
            ...
          ]
        }
      ]
    }
  ]
}
```

For a sparse encoding model, send the following request:

```json
POST /_plugins/_ml/_predict/sparse_encoding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""]
}
```
{% include copy-curl.html %}

The response contains the tokens and weights:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""name"": ""output"",
          ""dataAsMap"": {
            ""response"": [
              {
                ""saturday"": 0.48336542,
                ""week"": 0.1034762,
                ""mood"": 0.09698499,
                ""sunshine"": 0.5738209,
                ""bright"": 0.1756877,
                ...
              }
          }
        }
    }
}
```

## Step 5: Use the model for search

To learn how to use the model for vector search, see [Using an ML model for neural search]({{site.url}}{{site.baseurl}}/search-plugins/neural-search/#using-an-ml-model-for-neural-search).

## Question answering models

A question answering model extracts the answer to a question from a given context. ML Commons supports context in `text` format.

To register a question answering model, send a request in the following format. Specify the `function_name` as `QUESTION_ANSWERING`:

```json
POST /_plugins/_ml/models/_register
{
    ""name"": ""question_answering"",
    ""version"": ""1.0.0"",
    ""function_name"": ""QUESTION_ANSWERING"",
    ""description"": ""test model"",
    ""model_format"": ""TORCH_SCRIPT"",
    ""model_group_id"": ""lN4AP40BKolAMNtR4KJ5"",
    ""model_content_hash_value"": ""e837c8fc05fd58a6e2e8383b319257f9c3859dfb3edc89b26badfaf8a4405ff6"",
    ""model_config"": { 
        ""model_type"": ""bert"",
        ""framework_type"": ""huggingface_transformers""
    },
    ""url"": ""https://github.com/opensearch-project/ml-commons/blob/main/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/question_answering/question_answering_pt.zip?raw=true""
}
```
{% include copy-curl.html %}

Then send a request to deploy the model:

```json
POST _plugins/_ml/models/<model_id>/_deploy
```
{% include copy-curl.html %}

To test a question answering model, send the following request. It requires a `question` and the relevant `context` from which the answer will be generated:

```json
POST /_plugins/_ml/_predict/question_answering/<model_id>
{
  ""question"": ""Where do I live?""
  ""context"": ""My name is John. I live in New York""
}
```
{% include copy-curl.html %}

The response provides the answer based on the context:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""result"": ""New York""
        }
    }
}
```
"
"I have a model and some ML nodes, how do I boost it's performance?","---
layout: default
title: GPU acceleration
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 150
---


# GPU acceleration 

When running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training. 

## Supported GPUs

Currently, ML nodes support the following GPU instances:

- [NVIDIA instances with CUDA 11.6](https://aws.amazon.com/nvidia/)
- [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)

If you need GPU power, you can provision GPU instances through [Amazon Elastic Compute Cloud (Amazon EC2)](https://aws.amazon.com/ec2/). For more information on how to provision a GPU instance, see [Recommended GPU Instances](https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html).

## Supported images

You can use GPU acceleration with both [Docker images](https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md) with CUDA 11.6 and [Amazon Machine Images (AMIs)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html).

## PyTorch

GPU-accelerated ML nodes require [PyTorch](https://pytorch.org/docs/stable/index.html) 1.12.1 work with ML models.

## Setting up a GPU-accelerated ML node

Depending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts. 

### Preparing an NVIDIA ML node

NVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the `nvidia-uvm` kernel inside the `/dev` directory. To check for the kernel, enter `ls -al /dev | grep nvidia-uvm`.

If the `nvidia-uvm` kernel does not exist, run `nvidia-uvm-init.sh`:

```
#!/bin/bash
## Script to initialize nvidia device nodes.
## https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications
/sbin/modprobe nvidia
if [ ""$?"" -eq 0 ]; then
  # Count the number of NVIDIA controllers found.
  NVDEVS=`lspci | grep -i NVIDIA`
  N3D=`echo ""$NVDEVS"" | grep ""3D controller"" | wc -l`
  NVGA=`echo ""$NVDEVS"" | grep ""VGA compatible controller"" | wc -l`
  N=`expr $N3D + $NVGA - 1`
  for i in `seq 0 $N`; do
    mknod -m 666 /dev/nvidia$i c 195 $i
  done
  mknod -m 666 /dev/nvidiactl c 195 255
else
  exit 1
fi
/sbin/modprobe nvidia-uvm
if [ ""$?"" -eq 0 ]; then
  # Find out the major device number used by the nvidia-uvm driver
  D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`
  mknod -m 666 /dev/nvidia-uvm c $D 0
  mknod -m 666 /dev/nvidia-uvm-tools c $D 0
else
  exit 1
fi
```

After verifying that `nvidia-uvm` exists under `/dev`, you can start OpenSearch inside your cluster. 

### Preparing AWS Inferentia ML node

Depending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster. 

To start, [download and install OpenSearch]({{site.url}}{{site.baseurl}}/install-and-configure/index/) on your cluster.

Then export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory `opensearch-2.5.0`, so `OPENSEARCH_HOME` = `opensearch-2.5.0`:

```
echo ""export OPENSEARCH_HOME=~/opensearch-2.5.0"" | tee -a ~/.bash_profile
echo ""export PYTORCH_VERSION=1.12.1"" | tee -a ~/.bash_profile
source ~/.bash_profile
```

Next, create a shell script file called `prepare_torch_neuron.sh`. You can copy and customize one of the following examples based on your Linux operating system:

- [Ubuntu 20.04](#ubuntu-2004)
- [Amazon Linux 2](#amazon-linux-2)

After you've run the scripts, exit your current terminal and open a new terminal to start OpenSearch.

GPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems.
{: .note}

#### Ubuntu 20.04

```
. /etc/os-release
sudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF
deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main
EOF
wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -

# Update OS packages
sudo apt-get update -y

################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################

# Install OS headers
sudo apt-get install linux-headers-$(uname -r) -y

# Install Neuron Driver
sudo apt-get install aws-neuronx-dkms -y

####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################

# Install Neuron Tools
sudo apt-get install aws-neuronx-tools -y

######################################################
#   Only for Ubuntu 20 - Install Python3.7
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt-get install python3.7
######################################################
# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo apt-get install -y python3.7-venv g++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set pip repository to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

#Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf==3.20.1"" torchvision

# If you need to trace neuron model, install the transformers for tracing the Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile

# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

#### Amazon Linux 2

```
# Configure Linux for Neuron repository updates
sudo tee /etc/yum.repos.d/neuron.repo > /dev/null <<EOF
[neuron]
name=Neuron YUM Repository
baseurl=https://yum.repos.neuron.amazonaws.com
enabled=1
metadata_expire=0
EOF
sudo rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB
# Update OS packages
sudo yum update -y
################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################
# Install OS headers
sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y
# Install Neuron Driver
####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################
sudo yum install aws-neuronx-dkms -y
# Install Neuron Tools
sudo yum install aws-neuronx-tools -y

# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo yum install -y python3.7-venv gcc-c++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set Pip repository  to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

# Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf<4"" torchvision

# If you need to run the trace neuron model, install transformers for tracing Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile
# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

When the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch.

OpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually.

#### Prepare ML node manually

If the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually:

1. Deploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see [Deploy on AWS accelerator instance](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/setup/pytorch-install.html#deploy-on-aws-ml-accelerator-instance).

2. Copy the Neuron library into OpenSearch. The following command uses a directory named `opensearch-2.5.0`:

   ```
   OPENSEARCH_HOME=~/opensearch-2.5.0
   ```

3. Set the `PYTORCH_EXTRA_LIBRARY_PATH` path. In this example, we create a `pytorch` virtual environment in the OPENSEARCH_HOME folder:

   ```
   PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/


   mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r  $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
   export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
  ```

4. (Optional) To monitor the GPU usage of your accelerator instance, install [Neuron tools](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/index.html), which allows models to be used inside your instance:

   ```
   # Install Neuron Tools
   sudo apt-get install aws-neuronx-tools -y
   ```

   ```
   # Add Neuron tools your PATH
   export PATH=/opt/aws/neuron/bin:$PATH
   ```
  
   ```
   # Test Neuron tools
   neuron-top
   ```


5. To make sure you have enough memory to upload a model, increase the JVM stack size to `>+2MB`:

   ```
   echo ""-Xss2m"" | sudo tee -a $OPENSEARCH_HOME/config/jvm.options
   ```

6. Start OpenSearch. 

## Troubleshooting

Due to the amount of data required to work with ML models, you might encounter the following `max file descriptors` or `vm.max_map_count` errors when trying to run OpenSearch in a your cluster: 

```
[1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535]
[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
```

To troubleshoot the max file descriptors error, run the following command:

```
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
```

To fix the `vm.max_map_count` error, run this command to increase the count to `262114`:

```
sudo sysctl -w vm.max_map_count=262144
```

## Next steps

If you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see [Compiling and Deploying HuggingFace Pretrained BERT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html).

"
Can you show me an example of how to use lat/long coordinates?,"---
layout: default
title: Geopoint
nav_order: 56
has_children: false
parent: Geographic field types
grand_parent: Supported field types
redirect_from:
  - /opensearch/supported-field-types/geo-point/
  - /field-types/geo-point/
---

# Geopoint field type

A geopoint field type contains a geographic point specified by latitude and longitude. 

## Example

Create a mapping with a geopoint field type:

```json
PUT testindex1
{
  ""mappings"": {
    ""properties"": {
      ""point"": {
        ""type"": ""geo_point""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Formats

Geopoints can be indexed in the following formats:

- An object with a latitude and longitude

```json
PUT testindex1/_doc/1
{
  ""point"": { 
    ""lat"": 40.71,
    ""lon"": 74.00
  }
}
```
{% include copy-curl.html %}

- A string in the ""`latitude`,`longitude`"" format

```json
PUT testindex1/_doc/2
{
  ""point"": ""40.71,74.00"" 
}
```
{% include copy-curl.html %}

- A geohash

```json
PUT testindex1/_doc/3
{
  ""point"": ""txhxegj0uyp3""
}
```
{% include copy-curl.html %}

- An array in the [`longitude`, `latitude`] format

```json
PUT testindex1/_doc/4
{
  ""point"": [74.00, 40.71] 
}
```
{% include copy-curl.html %}

- A [Well-Known Text](https://docs.opengeospatial.org/is/12-063r5/12-063r5.html) POINT in the ""POINT(`longitude` `latitude`)"" format

```json
PUT testindex1/_doc/5
{
  ""point"": ""POINT (74.00 40.71)""
}
```
{% include copy-curl.html %}

- GeoJSON format, where the `coordinates` are in the [`longitude`, `latitude`] format

```json
PUT testindex1/_doc/6
{
  ""point"": {
    ""type"": ""Point"",
    ""coordinates"": [74.00, 40.71]
  }
}
```
{% include copy-curl.html %}

## Parameters

The following table lists the parameters accepted by geopoint field types. All parameters are optional.

Parameter | Description 
:--- | :--- 
`ignore_malformed` | A Boolean value that specifies to ignore malformed values and not to throw an exception. Valid values for latitude are [-90, 90]. Valid values for longitude are [-180, 180]. Default is `false`.
`ignore_z_value` | Specific to points with three coordinates. If `ignore_z_value` is `true`, the third coordinate is not indexed but is still stored in the _source field. If `ignore_z_value` is `false`, an exception is thrown.
[`null_value`]({{site.url}}{{site.baseurl}}/opensearch/supported-field-types/index#null-value) | A  value to be used in place of `null`. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is `null`. Default is `null`."
How do I use vector search?,"---
layout: default
title: Vector search
nav_order: 22
has_children: false
has_toc: false
---

# Vector search

OpenSearch is a comprehensive search platform that supports a variety of data types, including vectors. OpenSearch vector database functionality is seamlessly integrated with its generic database function.

In OpenSearch, you can generate vector embeddings, store those embeddings in an index, and use them for vector search. Choose one of the following options:

- Generate embeddings using a library of your choice before ingesting them into OpenSearch. Once you ingest vectors into an index, you can perform a vector similarity search on the vector space. For more information, see [Working with embeddings generated outside of OpenSearch](#working-with-embeddings-generated-outside-of-opensearch). 
- Automatically generate embeddings within OpenSearch. To use embeddings for semantic search, the ingested text (the corpus) and the query need to be embedded using the same model. [Neural search]({{site.url}}{{site.baseurl}}/search-plugins/neural-search/) packages this functionality, eliminating the need to manage the internal details. For more information, see [Generating vector embeddings within OpenSearch](#generating-vector-embeddings-in-opensearch).

## Working with embeddings generated outside of OpenSearch

After you generate vector embeddings, upload them to an OpenSearch index and search the index using vector search. For a complete example, see [Example](#example).

### k-NN index

To build a vector database and use vector search, you must specify your index as a [k-NN index]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/) when creating it by setting `index.knn` to `true`:

```json
PUT test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 1024,
        ""method"": {
          ""name"": ""hnsw"",
          ""space_type"": ""l2"",
          ""engine"": ""nmslib"",
          ""parameters"": {
            ""ef_construction"": 128,
            ""m"": 24
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

### k-NN vector

You must designate the field that will store vectors as a [`knn_vector`]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector/) field type. OpenSearch supports vectors of up to 16,000 dimensions, each of which is represented as a 32-bit or 16-bit float. 

To save storage space, you can use `byte` or `binary` vectors. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector) and [Binary k-NN vectors]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#binary-k-nn-vectors).

### k-NN vector search

Vector search finds the vectors in your database that are most similar to the query vector. OpenSearch supports the following search methods:

- [Approximate search](#approximate-search) (approximate k-NN, or ANN): Returns approximate nearest neighbors to the query vector. Usually, approximate search algorithms sacrifice indexing speed and search accuracy in exchange for performance benefits such as lower latency, smaller memory footprints, and more scalable search. For most use cases, approximate search is the best option.

- Exact search (exact k-NN): A brute-force, exact k-NN search of vector fields. OpenSearch supports the following types of exact search: 
  - [Exact k-NN with scoring script]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-score-script/): Using the k-NN scoring script, you can apply a filter to an index before executing the nearest neighbor search. 
  - [Painless extensions]({{site.url}}{{site.baseurl}}/search-plugins/knn/painless-functions/): Adds the distance functions as Painless extensions that you can use in more complex combinations. You can use this method to perform a brute-force, exact k-NN search of an index, which also supports pre-filtering. 

### Approximate search

OpenSearch supports several algorithms for approximate vector search, each with its own advantages. For complete documentation, see [Approximate search]({{site.url}}{{site.baseurl}}/search-plugins/knn/approximate-knn/). For more information about the search methods and engines, see [Method definitions]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#method-definitions). For method recommendations, see [Choosing the right method]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#choosing-the-right-method).

To use approximate vector search, specify one of the following search methods (algorithms) in the `method` parameter:

- Hierarchical Navigable Small World (HNSW)
- Inverted File System (IVF)

Additionally, specify the engine (library) that implements this method in the `engine` parameter:

- [Non-Metric Space Library (NMSLIB)](https://github.com/nmslib/nmslib)
- [Facebook AI Similarity Search (Faiss)](https://github.com/facebookresearch/faiss)
- Lucene

The following table lists the combinations of search methods and libraries supported by the k-NN engine for approximate vector search.

Method | Engine
:--- | :---
HNSW | NMSLIB, Faiss, Lucene
IVF | Faiss 

### Engine recommendations

In general, select NMSLIB or Faiss for large-scale use cases. Lucene is a good option for smaller deployments and offers benefits like smart filtering, where the optimal filtering strategy—pre-filtering, post-filtering, or exact k-NN—is automatically applied depending on the situation. The following table summarizes the differences between each option.

| |  NMSLIB/HNSW |  Faiss/HNSW |  Faiss/IVF |  Lucene/HNSW |
|:---|:---|:---|:---|:---|
|  Max dimensions |  16,000  |  16,000 |  16,000 |  16,000 |
|  Filter |  Post-filter |  Post-filter |  Post-filter |  Filter during search |
|  Training required |  No |  No |  Yes |  No |
|  Similarity metrics |  `l2`, `innerproduct`, `cosinesimil`, `l1`, `linf`  |  `l2`, `innerproduct` |  `l2`, `innerproduct` |  `l2`, `cosinesimil` |
|  Number of vectors   |  Tens of billions |  Tens of billions |  Tens of billions |  Less than 10 million |
|  Indexing latency |  Low |  Low  |  Lowest  |  Low  |
|  Query latency and quality  |  Low latency and high quality |  Low latency and high quality  |  Low latency and low quality  |  High latency and high quality  |
|  Vector compression  |  Flat |  Flat <br>Product quantization |  Flat <br>Product quantization |  Flat  |
|  Memory consumption |  High  |  High <br> Low with PQ |  Medium <br> Low with PQ |  High  |

### Example

In this example, you'll create a k-NN index, add data to the index, and search the data.

#### Step 1: Create a k-NN index

First, create an index that will store sample hotel data. Set `index.knn` to `true` and specify the `location` field as a `knn_vector`:

```json
PUT /hotels-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100,
      ""number_of_shards"": 1,
      ""number_of_replicas"": 0
    }
  },
  ""mappings"": {
    ""properties"": {
      ""location"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""space_type"": ""l2"",
          ""engine"": ""lucene"",
          ""parameters"": {
            ""ef_construction"": 100,
            ""m"": 16
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

#### Step 2: Add data to your index

Next, add data to your index. Each document represents a hotel. The `location` field in each document contains a vector specifying the hotel's location:

```json
POST /_bulk
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""1"" } }
{ ""location"": [5.2, 4.4] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""2"" } }
{ ""location"": [5.2, 3.9] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""3"" } }
{ ""location"": [4.9, 3.4] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""4"" } }
{ ""location"": [4.2, 4.6] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""5"" } }
{ ""location"": [3.3, 4.5] }
```
{% include copy-curl.html %}

#### Step 3: Search your data

Now search for hotels closest to the pin location `[5, 4]`. This location is labeled `Pin` in the following image. Each hotel is labeled with its document number.

![Hotels on a coordinate plane]({{site.url}}{{site.baseurl}}/images/k-nn-search-hotels.png/)

To search for the top three closest hotels, set `k` to `3`:

```json
POST /hotels-index/_search
{
  ""size"": 3,
  ""query"": {
    ""knn"": {
      ""location"": {
        ""vector"": [
          5,
          4
        ],
        ""k"": 3
      }
    }
  }
}
```
{% include copy-curl.html %}

The response contains the hotels closest to the specified pin location:

```json
{
  ""took"": 1093,
  ""timed_out"": false,
  ""_shards"": {
    ""total"": 1,
    ""successful"": 1,
    ""skipped"": 0,
    ""failed"": 0
  },
  ""hits"": {
    ""total"": {
      ""value"": 3,
      ""relation"": ""eq""
    },
    ""max_score"": 0.952381,
    ""hits"": [
      {
        ""_index"": ""hotels-index"",
        ""_id"": ""2"",
        ""_score"": 0.952381,
        ""_source"": {
          ""location"": [
            5.2,
            3.9
          ]
        }
      },
      {
        ""_index"": ""hotels-index"",
        ""_id"": ""1"",
        ""_score"": 0.8333333,
        ""_source"": {
          ""location"": [
            5.2,
            4.4
          ]
        }
      },
      {
        ""_index"": ""hotels-index"",
        ""_id"": ""3"",
        ""_score"": 0.72992706,
        ""_source"": {
          ""location"": [
            4.9,
            3.4
          ]
        }
      }
    ]
  }
}
```

### Vector search with filtering

For information about vector search with filtering, see [k-NN search with filters]({{site.url}}{{site.baseurl}}/search-plugins/knn/filter-search-knn/).

## Generating vector embeddings in OpenSearch

[Neural search]({{site.url}}{{site.baseurl}}/search-plugins/neural-search/) encapsulates the infrastructure needed to perform semantic vector searches. After you integrate an inference (embedding) service, neural search functions like lexical search, accepting a textual query and returning relevant documents.

When you index your data, neural search transforms text into vector embeddings and indexes both the text and its vector embeddings in a vector index. When you use a neural query during search, neural search converts the query text into vector embeddings and uses vector search to return the results.

### Choosing a model

The first step in setting up neural search is choosing a model. You can upload a model to your OpenSearch cluster, use one of the pretrained models provided by OpenSearch, or connect to an externally hosted model. For more information, see [Integrating ML models]({{site.url}}{{site.baseurl}}/ml-commons-plugin/integrating-ml-models/).

### Neural search tutorial

For a step-by-step tutorial, see [Neural search tutorial]({{site.url}}{{site.baseurl}}/search-plugins/neural-search-tutorial/).

### Search methods

Choose one of the following search methods to use your model for neural search:

- [Semantic search]({{site.url}}{{site.baseurl}}/search-plugins/semantic-search/): Uses dense retrieval based on text embedding models to search text data. 

- [Hybrid search]({{site.url}}{{site.baseurl}}/search-plugins/hybrid-search/): Combines lexical and neural search to improve search relevance. 

- [Multimodal search]({{site.url}}{{site.baseurl}}/search-plugins/multimodal-search/): Uses neural search with multimodal embedding models to search text and image data.

- [Neural sparse search]({{site.url}}{{site.baseurl}}/search-plugins/neural-sparse-search/): Uses neural search with sparse retrieval based on sparse embedding models to search text data.

- [Conversational search]({{site.url}}{{site.baseurl}}/search-plugins/conversational-search/): With conversational search, you can ask questions in natural language, receive a text response, and ask additional clarifying questions.
"
How do I understand the memory requirements for using hnsw?,"---
layout: default
title: k-NN vector quantization
nav_order: 27
parent: k-NN search
has_children: false
has_math: true
---

# k-NN vector quantization

By default, the k-NN plugin supports the indexing and querying of vectors of type `float`, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping `float` vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native `nmslib` and `faiss` engines). To reduce the memory footprint, you can use vector quantization.

OpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, 16-bit scalar quantization, and product quantization (PQ).

## Lucene byte vector

Starting with k-NN plugin version 2.9, you can use `byte` vectors with the Lucene engine in order to reduce the amount of required memory. This requires quantizing the vectors outside of OpenSearch before ingesting them into an OpenSearch index. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector).

## Lucene scalar quantization

Starting with version 2.16, the k-NN plugin supports built-in scalar quantization for the Lucene engine. Unlike the [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector), which requires you to quantize vectors before ingesting the documents, the Lucene scalar quantizer quantizes input vectors in OpenSearch during ingestion. The Lucene scalar quantizer converts 32-bit floating-point input vectors into 7-bit integer vectors in each segment using the minimum and maximum quantiles computed based on the [`confidence_interval`](#confidence-interval) parameter. During search, the query vector is quantized in each segment using the segment's minimum and maximum quantiles in order to compute the distance between the query vector and the segment's quantized input vectors. 

Quantization can decrease the memory footprint by a factor of 4 in exchange for some loss in recall. Additionally, quantization slightly increases disk usage because it requires storing both the raw input vectors and the quantized vectors.

### Using Lucene scalar quantization

To use the Lucene scalar quantizer, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

### Confidence interval

Optionally, you can specify the `confidence_interval` parameter in the `method.parameters.encoder` object.
The `confidence_interval` is used to compute the minimum and maximum quantiles in order to quantize the vectors:
- If you set the `confidence_interval` to a value in the `0.9` to `1.0` range, inclusive, then the quantiles are calculated statically. For example, setting the `confidence_interval` to `0.9` specifies to compute the minimum and maximum quantiles based on the middle 90% of the vector values, excluding the minimum 5% and maximum 5% of the values. 
- Setting `confidence_interval` to `0` specifies to compute the quantiles dynamically, which involves oversampling and additional computations performed on the input data.
- When `confidence_interval` is not set, it is computed based on the vector dimension $$d$$ using the formula $$max(0.9, 1 - \\frac{1}{1 + d})$$.

Lucene scalar quantization is applied only to `float` vectors. If you change the default value of the `data_type` parameter from `float` to `byte` or any other type when mapping a [k-NN vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector/), then the request is rejected.
{: .warning}

The following example method definition specifies the Lucene `sq` encoder with the `confidence_interval` set to `1.0`. This `confidence_interval` specifies to consider all the input vectors when computing the minimum and maximum quantiles. Vectors are quantized to 7 bits by default:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""confidence_interval"": 1.0
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

There are no changes to ingestion or query mapping and no range limitations for the input vectors. 

### Memory estimation

In the ideal scenario, 7-bit vectors created by the Lucene scalar quantizer use only 25% of the memory required by 32-bit vectors.

#### HNSW memory estimation

The memory required for the Hierarchical Navigable Small World (HNSW) graph can be estimated as `1.1 * (dimension + 8 * M)` bytes/vector, where `M` is the maximum number of bidirectional links created for each element during the construction of the graph.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (256 + 8 * 16) * 1,000,000 ~= 0.4 GB
```

## Faiss 16-bit scalar quantization 
 
Starting with version 2.13, the k-NN plugin supports performing scalar quantization for the Faiss engine within OpenSearch. Within the Faiss engine, a scalar quantizer (SQfp16) performs the conversion between 32-bit and 16-bit vectors. At ingestion time, when you upload 32-bit floating-point vectors to OpenSearch, SQfp16 quantizes them into 16-bit floating-point vectors and stores the quantized vectors in a k-NN index. 

At search time, SQfp16 decodes the vector values back into 32-bit floating-point values for distance computation. The SQfp16 quantization can decrease the memory footprint by a factor of 2. Additionally, it leads to a minimal loss in recall when differences between vector values are large compared to the error introduced by eliminating their two least significant bits. When used with [SIMD optimization]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index#simd-optimization-for-the-faiss-engine), SQfp16 quantization can also significantly reduce search latencies and improve indexing throughput. 

SIMD optimization is not supported on Windows. Using Faiss scalar quantization on Windows can lead to a significant drop in performance, including decreased indexing throughput and increased search latencies.
{: .warning} 

### Using Faiss scalar quantization

To use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

Optionally, you can specify the parameters in `method.parameters.encoder`. For more information about `encoder` object parameters, see [SQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#sq-parameters).

The `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the [-65504.0, 65504.0] range. To define how to handle out-of-range values, the preceding request specifies the `clip` parameter. By default, this parameter is `false`, and any vectors containing out-of-range values are rejected. 

When `clip` is set to `true` (as in the preceding request), out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is `[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.

We recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.
{: .note}

The following example method definition specifies the Faiss SQfp16 encoder, which rejects any indexing request that contains out-of-range vector values (because the `clip` parameter is `false` by default):

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""type"": ""fp16""
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

During ingestion, make sure each vector dimension is in the supported range ([-65504.0, 65504.0]).

```json
PUT test-index/_doc/1
{
  ""my_vector1"": [-65504.0, 65503.845, 55.82]
}
```
{% include copy-curl.html %}

During querying, the query vector has no range limitation:

```json
GET test-index/_search
{
  ""size"": 2,
  ""query"": {
    ""knn"": {
      ""my_vector1"": {
        ""vector"": [265436.876, -120906.256, 99.84],
        ""k"": 2
      }
    }
  }
}
```
{% include copy-curl.html %}

### Memory estimation

In the best-case scenario, 16-bit vectors produced by the Faiss SQfp16 quantizer require 50% of the memory that 32-bit vectors require. 

#### HNSW memory estimation

The memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB
```

#### IVF memory estimation

The memory required for IVF is estimated to be `1.1 * (((2 * dimension) * num_vectors) + (4 * nlist * d))` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and `nlist` of 128. The memory requirement can be estimated as follows:

```r
1.1 * (((2 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 0.525 GB
```

## Faiss product quantization

PQ is a technique used to represent a vector in a configurable amount of bits. In general, it can be used to achieve a higher level of compression as compared to byte or scalar quantization. PQ works by separating vectors into _m_ subvectors and encoding each subvector with _code_size_ bits. Thus, the total amount of memory for the vector is `m*code_size` bits, plus overhead. For details about the parameters, see [PQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#pq-parameters). PQ is only supported for the _Faiss_ engine and can be used with either the _HNSW_ or _IVF_ approximate nearest neighbor (ANN) algorithms.

### Using Faiss product quantization

To minimize loss in accuracy, PQ requires a _training_ step that builds a model based on the distribution of the data that will be searched.

The product quantizer is trained by running k-means clustering on a set of training vectors for each subvector space and extracts the centroids to be used for encoding. The training vectors can be either a subset of the vectors to be ingested or vectors that have the same distribution and dimension as the vectors to be ingested.

In OpenSearch, the training vectors need to be present in an index. In general, the amount of training data will depend on which ANN algorithm is used and how much data will be stored in the index. For IVF-based indexes, a recommended number of training vectors is `max(1000*nlist, 2^code_size * 1000)`. For HNSW-based indexes, a recommended number is `2^code_size*1000`. See the [Faiss documentation](https://github.com/facebookresearch/faiss/wiki/FAQ#how-many-training-points-do-i-need-for-k-means) for more information about the methodology used to calculate these figures.

For PQ, both _m_ and _code_size_ need to be selected. _m_ determines the number of subvectors into which vectors should be split for separate encoding. Consequently, the _dimension_ needs to be divisible by _m_. _code_size_ determines the number of bits used to encode each subvector. In general, we recommend a setting of `code_size = 8` and then tuning _m_ to get the desired trade-off between memory footprint and recall.

For an example of setting up an index with PQ, see the [Building a k-NN index from a model]({{site.url}}{{site.baseurl}}/search-plugins/knn/approximate-knn/#building-a-k-nn-index-from-a-model) tutorial.

### Memory estimation

While PQ is meant to represent individual vectors with `m*code_size` bits, in reality, the indexes consume more space. This is mainly due to the overhead of storing certain code tables and auxiliary data structures.

Some of the memory formulas depend on the number of segments present. This is not typically known beforehand, but a recommended default value is 300.
{: .note}

#### HNSW memory estimation

The memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.

As an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1 * ((8 / 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB
```

#### IVF memory estimation

The memory required for IVF with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24) * num_vectors  + num_segments * (2^code_size * 4 * d + 4 * ivf_nlist * d))` bytes.

For example, assume that you have 1 million vectors with a dimension of 256, `ivf_nlist` of 512, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1*((8 / 8 * 64 + 24) * 1000000  + 100 * (2^8 * 4 * 256 + 4 * 512 * 256))  ~= 0.171 GB
```
"
Can you show me some different examples of using different quantization methods for vectors?,"---
layout: default
title: k-NN vector quantization
nav_order: 27
parent: k-NN search
has_children: false
has_math: true
---

# k-NN vector quantization

By default, the k-NN plugin supports the indexing and querying of vectors of type `float`, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping `float` vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native `nmslib` and `faiss` engines). To reduce the memory footprint, you can use vector quantization.

OpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, 16-bit scalar quantization, and product quantization (PQ).

## Lucene byte vector

Starting with k-NN plugin version 2.9, you can use `byte` vectors with the Lucene engine in order to reduce the amount of required memory. This requires quantizing the vectors outside of OpenSearch before ingesting them into an OpenSearch index. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector).

## Lucene scalar quantization

Starting with version 2.16, the k-NN plugin supports built-in scalar quantization for the Lucene engine. Unlike the [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector), which requires you to quantize vectors before ingesting the documents, the Lucene scalar quantizer quantizes input vectors in OpenSearch during ingestion. The Lucene scalar quantizer converts 32-bit floating-point input vectors into 7-bit integer vectors in each segment using the minimum and maximum quantiles computed based on the [`confidence_interval`](#confidence-interval) parameter. During search, the query vector is quantized in each segment using the segment's minimum and maximum quantiles in order to compute the distance between the query vector and the segment's quantized input vectors. 

Quantization can decrease the memory footprint by a factor of 4 in exchange for some loss in recall. Additionally, quantization slightly increases disk usage because it requires storing both the raw input vectors and the quantized vectors.

### Using Lucene scalar quantization

To use the Lucene scalar quantizer, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

### Confidence interval

Optionally, you can specify the `confidence_interval` parameter in the `method.parameters.encoder` object.
The `confidence_interval` is used to compute the minimum and maximum quantiles in order to quantize the vectors:
- If you set the `confidence_interval` to a value in the `0.9` to `1.0` range, inclusive, then the quantiles are calculated statically. For example, setting the `confidence_interval` to `0.9` specifies to compute the minimum and maximum quantiles based on the middle 90% of the vector values, excluding the minimum 5% and maximum 5% of the values. 
- Setting `confidence_interval` to `0` specifies to compute the quantiles dynamically, which involves oversampling and additional computations performed on the input data.
- When `confidence_interval` is not set, it is computed based on the vector dimension $$d$$ using the formula $$max(0.9, 1 - \\frac{1}{1 + d})$$.

Lucene scalar quantization is applied only to `float` vectors. If you change the default value of the `data_type` parameter from `float` to `byte` or any other type when mapping a [k-NN vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector/), then the request is rejected.
{: .warning}

The following example method definition specifies the Lucene `sq` encoder with the `confidence_interval` set to `1.0`. This `confidence_interval` specifies to consider all the input vectors when computing the minimum and maximum quantiles. Vectors are quantized to 7 bits by default:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""confidence_interval"": 1.0
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

There are no changes to ingestion or query mapping and no range limitations for the input vectors. 

### Memory estimation

In the ideal scenario, 7-bit vectors created by the Lucene scalar quantizer use only 25% of the memory required by 32-bit vectors.

#### HNSW memory estimation

The memory required for the Hierarchical Navigable Small World (HNSW) graph can be estimated as `1.1 * (dimension + 8 * M)` bytes/vector, where `M` is the maximum number of bidirectional links created for each element during the construction of the graph.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (256 + 8 * 16) * 1,000,000 ~= 0.4 GB
```

## Faiss 16-bit scalar quantization 
 
Starting with version 2.13, the k-NN plugin supports performing scalar quantization for the Faiss engine within OpenSearch. Within the Faiss engine, a scalar quantizer (SQfp16) performs the conversion between 32-bit and 16-bit vectors. At ingestion time, when you upload 32-bit floating-point vectors to OpenSearch, SQfp16 quantizes them into 16-bit floating-point vectors and stores the quantized vectors in a k-NN index. 

At search time, SQfp16 decodes the vector values back into 32-bit floating-point values for distance computation. The SQfp16 quantization can decrease the memory footprint by a factor of 2. Additionally, it leads to a minimal loss in recall when differences between vector values are large compared to the error introduced by eliminating their two least significant bits. When used with [SIMD optimization]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index#simd-optimization-for-the-faiss-engine), SQfp16 quantization can also significantly reduce search latencies and improve indexing throughput. 

SIMD optimization is not supported on Windows. Using Faiss scalar quantization on Windows can lead to a significant drop in performance, including decreased indexing throughput and increased search latencies.
{: .warning} 

### Using Faiss scalar quantization

To use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

Optionally, you can specify the parameters in `method.parameters.encoder`. For more information about `encoder` object parameters, see [SQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#sq-parameters).

The `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the [-65504.0, 65504.0] range. To define how to handle out-of-range values, the preceding request specifies the `clip` parameter. By default, this parameter is `false`, and any vectors containing out-of-range values are rejected. 

When `clip` is set to `true` (as in the preceding request), out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is `[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.

We recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.
{: .note}

The following example method definition specifies the Faiss SQfp16 encoder, which rejects any indexing request that contains out-of-range vector values (because the `clip` parameter is `false` by default):

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""type"": ""fp16""
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

During ingestion, make sure each vector dimension is in the supported range ([-65504.0, 65504.0]).

```json
PUT test-index/_doc/1
{
  ""my_vector1"": [-65504.0, 65503.845, 55.82]
}
```
{% include copy-curl.html %}

During querying, the query vector has no range limitation:

```json
GET test-index/_search
{
  ""size"": 2,
  ""query"": {
    ""knn"": {
      ""my_vector1"": {
        ""vector"": [265436.876, -120906.256, 99.84],
        ""k"": 2
      }
    }
  }
}
```
{% include copy-curl.html %}

### Memory estimation

In the best-case scenario, 16-bit vectors produced by the Faiss SQfp16 quantizer require 50% of the memory that 32-bit vectors require. 

#### HNSW memory estimation

The memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB
```

#### IVF memory estimation

The memory required for IVF is estimated to be `1.1 * (((2 * dimension) * num_vectors) + (4 * nlist * d))` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and `nlist` of 128. The memory requirement can be estimated as follows:

```r
1.1 * (((2 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 0.525 GB
```

## Faiss product quantization

PQ is a technique used to represent a vector in a configurable amount of bits. In general, it can be used to achieve a higher level of compression as compared to byte or scalar quantization. PQ works by separating vectors into _m_ subvectors and encoding each subvector with _code_size_ bits. Thus, the total amount of memory for the vector is `m*code_size` bits, plus overhead. For details about the parameters, see [PQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#pq-parameters). PQ is only supported for the _Faiss_ engine and can be used with either the _HNSW_ or _IVF_ approximate nearest neighbor (ANN) algorithms.

### Using Faiss product quantization

To minimize loss in accuracy, PQ requires a _training_ step that builds a model based on the distribution of the data that will be searched.

The product quantizer is trained by running k-means clustering on a set of training vectors for each subvector space and extracts the centroids to be used for encoding. The training vectors can be either a subset of the vectors to be ingested or vectors that have the same distribution and dimension as the vectors to be ingested.

In OpenSearch, the training vectors need to be present in an index. In general, the amount of training data will depend on which ANN algorithm is used and how much data will be stored in the index. For IVF-based indexes, a recommended number of training vectors is `max(1000*nlist, 2^code_size * 1000)`. For HNSW-based indexes, a recommended number is `2^code_size*1000`. See the [Faiss documentation](https://github.com/facebookresearch/faiss/wiki/FAQ#how-many-training-points-do-i-need-for-k-means) for more information about the methodology used to calculate these figures.

For PQ, both _m_ and _code_size_ need to be selected. _m_ determines the number of subvectors into which vectors should be split for separate encoding. Consequently, the _dimension_ needs to be divisible by _m_. _code_size_ determines the number of bits used to encode each subvector. In general, we recommend a setting of `code_size = 8` and then tuning _m_ to get the desired trade-off between memory footprint and recall.

For an example of setting up an index with PQ, see the [Building a k-NN index from a model]({{site.url}}{{site.baseurl}}/search-plugins/knn/approximate-knn/#building-a-k-nn-index-from-a-model) tutorial.

### Memory estimation

While PQ is meant to represent individual vectors with `m*code_size` bits, in reality, the indexes consume more space. This is mainly due to the overhead of storing certain code tables and auxiliary data structures.

Some of the memory formulas depend on the number of segments present. This is not typically known beforehand, but a recommended default value is 300.
{: .note}

#### HNSW memory estimation

The memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.

As an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1 * ((8 / 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB
```

#### IVF memory estimation

The memory required for IVF with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24) * num_vectors  + num_segments * (2^code_size * 4 * d + 4 * ivf_nlist * d))` bytes.

For example, assume that you have 1 million vectors with a dimension of 256, `ivf_nlist` of 512, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1*((8 / 8 * 64 + 24) * 1000000  + 100 * (2^8 * 4 * 256 + 4 * 512 * 256))  ~= 0.171 GB
```
"
"Would you recommend I use ReRank? If so, what type of model would you recommend for ReRank? ","---
layout: default
title: Reranking search results
parent: Search relevance
has_children: false
nav_order: 60
---

# Reranking search results
Introduced 2.12
{: .label .label-purple }

You can rerank search results using a cross-encoder reranker in order to improve search relevance. To implement reranking, you need to configure a [search pipeline]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/index/) that runs at search time. The search pipeline intercepts search results and applies the [`rerank` processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/) to them. The `rerank` processor evaluates the search results and sorts them based on the new scores provided by the cross-encoder model. 

**PREREQUISITE**<br>
Before configuring a reranking pipeline, you must set up a cross-encoder model. For information about using an OpenSearch-provided model, see [Cross-encoder models]({{site.url}}{{site.baseurl}}/ml-commons-plugin/pretrained-models/#cross-encoder-models). For information about using a custom model, see [Custom local models]({{site.url}}{{site.baseurl}}/ml-commons-plugin/custom-local-models/).
{: .note}

## Running a search with reranking

To run a search with reranking, follow these steps:

1. [Configure a search pipeline](#step-1-configure-a-search-pipeline).
1. [Create an index for ingestion](#step-2-create-an-index-for-ingestion).
1. [Ingest documents into the index](#step-3-ingest-documents-into-the-index).
1. [Search using reranking](#step-4-search-using-reranking).

## Step 1: Configure a search pipeline

Next, configure a search pipeline with a [`rerank` processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/).

The following example request creates a search pipeline with an `ml_opensearch` rerank processor. In the request, provide a model ID for the cross-encoder model and the document fields to use as context:

```json
PUT /_search/pipeline/my_pipeline
{
  ""description"": ""Pipeline for reranking with a cross-encoder"",
  ""response_processors"": [
    {
      ""rerank"": {
        ""ml_opensearch"": {
          ""model_id"": ""gnDIbI0BfUsSoeNT_jAw""
        },
        ""context"": {
          ""document_fields"": [
            ""passage_text""
          ]
        }
      }
    }
  ]
}
```
{% include copy-curl.html %}

For more information about the request fields, see [Request fields]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/#request-fields).

## Step 2: Create an index for ingestion

In order to use the rerank processor defined in your pipeline, create an OpenSearch index and add the pipeline created in the previous step as the default pipeline:

```json
PUT /my-index
{
  ""settings"": {
    ""index.search.default_pipeline"" : ""my_pipeline""
  },
  ""mappings"": {
    ""properties"": {
      ""passage_text"": {
        ""type"": ""text""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Step 3: Ingest documents into the index

To ingest documents into the index created in the previous step, send the following bulk request:

```json
POST /_bulk
{ ""index"": { ""_index"": ""my-index"" } }
{ ""passage_text"" : ""I said welcome to them and we entered the house"" }
{ ""index"": { ""_index"": ""my-index"" } }
{ ""passage_text"" : ""I feel welcomed in their family"" }
{ ""index"": { ""_index"": ""my-index"" } }
{ ""passage_text"" : ""Welcoming gifts are great"" }

```
{% include copy-curl.html %}

## Step 4: Search using reranking

To perform reranking search on your index, use any OpenSearch query and provide an additional `ext.rerank` field:

```json
POST /my-index/_search
{
  ""query"": {
    ""match"": {
      ""passage_text"": ""how to welcome in family""
    }
  },
  ""ext"": {
    ""rerank"": {
      ""query_context"": {
         ""query_text"": ""how to welcome in family""
      }
    }
  }
}
```
{% include copy-curl.html %}

Alternatively, you can provide the full path to the field containing the context. For more information, see [Rerank processor example]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/#example).

## Using rerank and normalization processors together

When you use a rerank processor in conjunction with a [normalization processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/normalization-processor/) and a hybrid query, the rerank processor alters the final document scores. This is because the rerank processor operates after the normalization processor in the search pipeline.
{: .note}

The processing order is as follows: 

- Normalization processor: This processor normalizes the document scores based on the configured normalization method. For more information, see [Normalization processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/normalization-processor/).
- Rerank processor: Following normalization, the rerank processor further adjusts the document scores. This adjustment can significantly impact the final ordering of search results.

This processing order has the following implications:

- Score modification: The rerank processor modifies the scores that were initially adjusted by the normalization processor, potentially leading to different ranking results than initially expected.
- Hybrid queries: In the context of hybrid queries, where multiple types of queries and scoring mechanisms are combined, this behavior is particularly noteworthy. The combined scores from the initial query are normalized first and then reranked, resulting in a two-stage scoring modification."
How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?,"---
layout: default
title: Tiered cache
parent: Caching
grand_parent: Improving search performance
nav_order: 10
---

# Tiered cache

This is an experimental feature and is not recommended for use in a production environment. For updates on the progress of the feature or if you want to leave feedback, see the associated [GitHub issue](https://github.com/opensearch-project/OpenSearch/issues/10024).    
{: .warning}

A tiered cache is a multi-level cache in which each tier has its own characteristics and performance levels. By combining different tiers, you can achieve a balance between cache performance and size.

## Types of tiered caches

OpenSearch provides an implementation of a `_tiered` spillover `cache_`. This implementation spills any items removed from the upper tiers to the lower tiers of cache. The upper tier, such as the on-heap tier, is smaller in size but offers better latency. The lower tier, such as the disk cache, is larger in size but slower in terms of latency. OpenSearch offers both on-heap and disk tiers. 

## Enabling a tiered cache

To enable a tiered cache, configure the following setting in `opensearch.yml`:

```yaml
opensearch.experimental.feature.pluggable.caching.enabled: true
```
{% include copy.html %}

For more information about ways to enable experimental features, see [Experimental feature flags]({{site.url}}{{site.baseurl}}/install-and-configure/configuring-opensearch/experimental/).

## Installing required plugins

To use tiered caching, install a tiered cache plugin. As of OpenSearch 2.13, the only available cache plugin is the `cache-ehcache` plugin. This plugin provides a disk cache implementation that can be used as a disk tier within a tiered cache. For more information about installing non-bundled plugins, see [Additional plugins]({{site.url}}{{site.baseurl}}/install-and-configure/plugins/#additional-plugins).

A tiered cache will fail to initialize if the `cache-ehcache` plugin is not installed or if disk cache properties are not set. 
{: .warning}

## Tiered cache settings

In OpenSearch 2.14, a request cache can be used in a tiered cache. To begin, configure the following settings in the `opensearch.yml` file.

### Cache store name

To use the OpenSearch-provided tiered spillover cache implementation, set the cache store name to `tiered_spillover`, as shown in the following example:

```yaml
indices.request.cache.store.name: tiered_spillover
```
{% include copy.html %}

### Setting on-heap and disk store tiers

Set the on-heap and disk store tiers to `opensearch_onheap` and `ehcache_disk`, as shown in the following example:

```yaml
indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap
indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk
```
The `opensearch_onheap` setting uses the built-in on-heap cache available in OpenSearch. 

The `ehcache_disk` setting is the disk cache implementation from [Ehcache](https://www.ehcache.org/) and requires installing the `cache-ehcache` plugin.

{% include copy.html %}

### Configuring on-heap and disk stores

The following table lists the cache store settings for the `opensearch_onheap` store.

Setting | Data type | Default | Description
:--- | :--- | :--- | :---
`indices.request.cache.opensearch_onheap.size` | Percentage | 1% of the heap size | The size of the on-heap cache. Optional.
`indices.request.cache.opensearch_onheap.expire` | Time unit | `MAX_VALUE` (disabled) | Specifies a time-to-live (TTL) for the cached results. Optional.

The following table lists the disk cache store settings for the `ehcache_disk` store.

Setting | Data type | Default | Description
:--- | :--- | :--- | :---
`indices.request.cache.ehcache_disk.max_size_in_bytes` | Long | `1073741824` (1 GB)  | Defines the size of the disk cache. Optional.
`indices.request.cache.ehcache_disk.storage.path` | String | `""""` | Defines the storage path for the disk cache. Required.
`indices.request.cache.ehcache_disk.expire_after_access` | Time unit | `MAX_VALUE` (disabled) | Specifies a TTL for the cached results. Optional.
`indices.request.cache.ehcache_disk.alias` | String | `ehcacheDiskCache#INDICES_REQUEST_CACHE` | Specifies an alias for the disk cache. Optional.
`indices.request.cache.ehcache_disk.segments` | Integer | `16` | Defines the number of segments into which the disk cache is separated. Used for concurrency. Optional.
`indices.request.cache.ehcache_disk.concurrency` | Integer | `1` | Defines the number of distinct write queues created for the disk store, where a group of segments shares a write queue. Optional.

### Additional settings for the `tiered_spillover` store

The following table lists additional settings for the `tiered_spillover` store setting.

Setting | Data type | Default | Description
:--- | :--- | :--- | :---
`indices.request.cache.tiered_spillover.disk.store.policies.took_time.threshold` | Time unit | `10ms` | A policy used to determine whether to cache a query into a disk cache based on its took time. This is a dynamic setting. Optional.
`indices.request.cache.tiered_spillover.disk.store.enabled` | Boolean | `True` | Enables or disables the disk cache dynamically within a tiered spillover cache. Note: After disabling a disk cache, entries are not removed automatically and requires the cache to be manually cleared. Optional.

### Delete stale entries settings

The following table lists the settings related to the deletion of stale entries from the cache.

Setting | Data type | Default | Description
:--- | :--- |:--------| :---
`indices.requests.cache.cleanup.staleness_threshold` | String | `0%`    | Defines the percentage of stale keys in the cache post. After identification, all stale cache entries are deleted. Optional.
`indices.requests.cache.cleanup.interval` | Time unit | `1m`  | Defines the frequency at which the request cache's stale entries are deleted. Optional.

## Getting statistics for the `tiered_spillover` store 

To assess the impact of using the tiered spillover cache, use the [Node Stats API]({{site.url}}{{site.baseurl}}/api-reference/nodes-apis/nodes-stats/#caches), as shown in the following example: 

```json
GET /_nodes/stats/caches/request_cache?level=tier
```

"
I need to combine relevance scores from different types of searches.,"---
layout: default
title: Hybrid
parent: Compound queries
nav_order: 70
---

# Hybrid query

You can use a hybrid query to combine relevance scores from multiple queries into one score for a given document. A hybrid query contains a list of one or more queries and independently calculates document scores at the shard level for each subquery. The subquery rewriting is performed at the coordinating node level in order to avoid duplicate computations.

## Example

Learn how to use the `hybrid` query by following the steps in [Using hybrid search]({{site.url}}{{site.baseurl}}/search-plugins/hybrid-search/#using-hybrid-search).

For a comprehensive example, follow the [Neural search tutorial]({{site.url}}{{site.baseurl}}/ml-commons-plugin/semantic-search#tutorial).

## Parameters

The following table lists all top-level parameters supported by `hybrid` queries.

Parameter | Description
:--- | :---
`queries` | An array of one or more query clauses that are used to match documents. A document must match at least one query clause in order to be returned in the results. The documents' relevance scores from all query clauses are combined into one score by applying a [search pipeline]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/index/). The maximum number of query clauses is 5. Required.

## Disabling hybrid queries

By default, hybrid queries are enabled. To disable hybrid queries in your cluster, set the `plugins.neural_search.hybrid_search_disabled` setting to `true` in `opensearch.yml`. "
What metrics does query metrics give me out of the box? ,"---
layout: default
title: Query metrics
parent: Query insights
nav_order: 20
---

# Query metrics

Key query [metrics](#metrics), such as aggregation types, query types, latency, and resource usage per query type, are captured along the search path by using the OpenTelemetry (OTel) instrumentation framework. The telemetry data can be consumed using OTel metrics [exporters]({{site.url}}{{site.baseurl}}/observing-your-data/trace/distributed-tracing/#exporters).

## Configuring query metric generation

To configure query metric generation, use the following steps.

### Step 1: Install the Query Insights plugin

For information about installing the Query Insights plugin, see [Installing the Query Insights plugin]({{site.url}}{{site.baseurl}}/observing-your-data/query-insights/index/#installing-the-query-insights-plugin).

### Step 2: Install the OpenTelemetry plugin

For information about installing the OpenTelemetry plugin, see [Distributed tracing]({{site.url}}{{site.baseurl}}/observing-your-data/trace/distributed-tracing/).

### Step 3: Enable query metrics

Enable query metrics by configuring the following `opensearch.yml` settings:

```yaml
telemetry.feature.metrics.enabled: true
search.query.metrics.enabled: true
```
{% include copy.html %}

The following is a complete sample configuration that includes a telemetry configuration:

```yaml
# Enable query metrics feature
search.query.metrics.enabled: true
telemetry.feature.metrics.enabled: true

# OTel-related configuration
opensearch.experimental.feature.telemetry.enabled: true
telemetry.tracer.sampler.probability: 1.0
telemetry.feature.tracer.enabled: true
```
{% include copy.html %}

Alternatively, you can configure query metric generation using the API:

```json
PUT _cluster/settings
{
  ""persistent"" : {
    ""search.query.metrics.enabled"" : true
  }
}
```
{% include copy-curl.html %}

Configure the export of metrics and traces using a gRPC exporter. For more information, see [Exporters]({{site.url}}{{site.baseurl}}/observing-your-data/trace/distributed-tracing/#exporters). You can skip this step if you use the [default logging exporter](#default-logging-exporter):

```yaml
telemetry.otel.tracer.span.exporter.class: io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter
telemetry.otel.metrics.exporter.class: io.opentelemetry.exporter.otlp.metrics.OtlpGrpcMetricExporter
```
{% include copy.html %}

## Metrics

Query metrics provide the following measurements:

- The number of queries per query type (for example, the number of `match` or `regex` queries)
- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)
- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)
- Histograms of `latency` for each query type, aggregation type, and sort order
- Histograms of `cpu` for each query type, aggregation type, and sort order
- Histograms of `memory` for each query type, aggregation type, and sort order

## Default logging exporter

By default, if no gRPC exporters are configured, then the metrics and traces are exported to log files. The data is saved in the `opensearch/logs` directory in the following files:

- `opensearch_otel_metrics.log`
- `opensearch_otel_traces.log`
"
Does opensearch work with open telemetry?,"---
layout: default
title: Metric analytics
nav_order: 40
---

# Metric analytics
Introduced 2.4
{: .label .label-purple }

With the release of OpenSearch 2.4, you can now ingest and visualize metric data stored directly in OpenSearch using the **Metrics** tool. This equips you with tools to analyze and correlate data across logs, traces, and metrics.

Before the introduction of this feature, you could only ingest and visualize logs and traces from your monitored environments. With the **Metrics** tool, you can now observe your digital assets with more granularity, gain deeper insight into the health of your infrastructure, and better inform your root cause analysis.

The **Metrics** tool offers federated visualization capabilities in addition to the following:

 - An OpenSearch cluster containing an [OpenTelemetry (OTel)-compatible metrics index](https://github.com/opensearch-project/opensearch-catalog/tree/main/docs/schema/observability/metrics) with OTel-based signals. See [What is OpenTelemetry?](https://opentelemetry.io/docs/what-is-opentelemetry/) for an overview of OTel.
 - An OpenSearch cluster containing a [Prometheus data source](https://github.com/opensearch-project/sql/blob/main/docs/dev/datasource-prometheus.md) connected to a Prometheus server. 

The following image displays the flow for retrieving metrics from Prometheus and displaying them on a visualization dashboard.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/prom-metrics.png"" alt=""Prometheus data source"" width=""700""/>

The following image displays an observability dashboard that visualizes metric data from the OpenSearch index using OTel queries.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/otel-metrics.png"" alt=""OTel data source"" width=""700""/>

---

## Configuring Prometheus to send metric data to OpenSearch

You must first create a connection from [Prometheus](https://prometheus.io/) to OpenSearch using the [SQL plugin](https://github.com/opensearch-project/sql). You can then configure a connection to Prometheus by using the `_datasources` API endpoint. 

The following example shows a request that configures a Prometheus data source without any authentication:

```json
POST _plugins/_query/_datasources 
{
    ""name"" : ""my_prometheus"",
    ""connector"": ""prometheus"",
    ""properties"" : {
        ""prometheus.uri"" : ""http://localhost:9090""
    }
}
```
{% include copy-curl.html %}

The following example shows how to configure a Prometheus data source using AWS Signature Version 4 authentication:

```json
POST _plugins/_query/_datasources
{
    ""name"" : ""my_prometheus"",
    ""connector"": ""prometheus"",
    ""properties"" : {
        ""prometheus.uri"" : ""http://localhost:8080"",
        ""prometheus.auth.type"" : ""awssigv4"",
        ""prometheus.auth.region"" : ""us-east-1"",
        ""prometheus.auth.access_key"" : ""{{accessKey}}""
        ""prometheus.auth.secret_key"" : ""{{secretKey}}""
    }
}
```
{% include copy-curl.html %}

After configuring the connection, you can view Prometheus metrics in OpenSearch Dashboards by going to the **Observability** > **Metrics** page, as shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/metrics1.png"" alt=""Prometheus metrics displayed on a dashboard"" width=""700""/>

### Developer resources

See the following developer resources for sample code, articles, tutorials, and API references:

* [Datasource Settings](https://github.com/opensearch-project/sql/blob/main/docs/user/ppl/admin/datasources.rst), which contains information about authentication and authorization of data source APIs.
* [Prometheus Connector](https://github.com/opensearch-project/sql/blob/main/docs/user/ppl/admin/connectors/prometheus_connector.rst), which contains configuration information.
* [Simple Schema for Observability](https://github.com/opensearch-project/opensearch-catalog/tree/main/docs/schema/observability), which contains information about the OTel schema and ingest pipeline.
* [OTel Metrics Source](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/otel-metrics-source), which contains information about the Data Prepper metrics pipeline and ingestion.

---

## Experimenting with OpenTelemetry Metrics in the OpenSearch demo environment 

The OpenSearch [`opentelemetry-demo` repository](https://github.com/opensearch-project/opentelemetry-demo) provides a practical demonstration of collecting, processing, and visualizing metric data through **OpenTelemetry Metrics** from OpenTelemetry and using the **Metrics** tool in OpenSearch Dashboards.

### Visualizing OTel metrics in OpenSearch

To visualize OTel metric data in OpenSearch, follow these steps: 

1. Install the [`opentelemetry-demo` repository](https://github.com/opensearch-project/opentelemetry-demo). See the [Getting Started](https://github.com/opensearch-project/opentelemetry-demo/blob/main/tutorial/GettingStarted.md) guide for instructions.
2. Collect the OTel signals, including metric signals. See the [OTel Collector](https://opentelemetry.io/docs/collector/) guide for instructions.  
3. Configure the OTel pipeline to emit metric signals. See the [OTel Collector Pipeline](https://github.com/opensearch-project/opentelemetry-demo/tree/main/src/otelcollector) guide for instructions.

#### Example YAML config file

```yaml
    service:
      extensions: [basicauth/client]
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp, debug, spanmetrics, otlp/traces, opensearch/traces]
        metrics:
          receivers: [otlp, spanmetrics]
          processors: [filter/ottl, transform, batch]
          exporters: [otlphttp/prometheus, otlp/metrics, debug]
        logs:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp/logs,  opensearch/logs, debug]
```
{% include copy-curl.html %}
    
4. Configure the [Data Prepper pipeline](https://github.com/opensearch-project/opentelemetry-demo/blob/main/src/dataprepper/pipelines.yaml) to emit the collected metric signals into the OpenSearch metrics index.

#### Example YAML config file

```yaml
    otel-metrics-pipeline:
      workers: 8
      delay: 3000
      source:
        otel_metrics_source:
          health_check_service: true
          ssl: false
      buffer:
        bounded_blocking:
          buffer_size: 1024 # max number of records the buffer accepts
          batch_size: 1024 # max number of records the buffer drains after each read
      processor:
        - otel_metrics:
            calculate_histogram_buckets: true
            calculate_exponential_histogram_buckets: true
            exponential_histogram_max_allowed_scale: 10
            flatten_attributes: false
      sink:
        - opensearch:
            hosts: [""https://opensearch-node1:9200""]
            username: ""admin""
            password: ""my_%New%_passW0rd!@#""
            insecure: true
            index_type: custom
            template_file: ""templates/ss4o_metrics.json""
            index: ss4o_metrics-otel-%{yyyy.MM.dd}
            bulk_size: 4
```
{% include copy-curl.html %}

5. Ingest metric data into OpenSearch. As the demo starts generating data, the metric signals will be added to the OpenSearch index that supports the OpenTelemetry Metrics schema format.
6. On the **Metrics** page, choose `Otel-Index` from the **Data sources** dropdown menu and `Simple Schema for Observability Index` from the **OTel index** dropdown menu. A visualization is displayed, as shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/otel-metrics.png"" alt=""OTel metrics dashboard"" width=""700""/>

---

## Visualizing metrics in remote clusters
Introduced 2.14
{: .label .label-purple }

You can view metrics from remote OpenSearch clusters by using the **Metrics** tool. Select the database icon on the upper-right toolbar and choose a cluster from the **DATA SOURCES** dropdown menu, as shown in the following image. You can switch from a local cluster to a remote cluster.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/remote-cluster-selection.png"" alt=""Switching clusters using the Metrics analytics tool"" width=""700""/>

You can also view metric visualizations from other sources alongside local metric visualizations. From the **DATA SOURCES** dropdown menu, choose the remote metric visualization to add it to the group of visualizations already shown on the dashboard. An example dashboard is shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/otel-metrics-remote-cluster-selection.png"" alt=""Metrics dashboard"" width=""700""/>

To learn about multi-cluster support for data sources, see [Enable OpenSearch Dashboards to support multiple OpenSearch clusters](https://github.com/opensearch-project/OpenSearch-Dashboards/issues/1388).

## Creating visualizations based on custom metrics

You can create visualizations using the metric data collected by your OpenSearch cluster, including Prometheus metrics and custom metrics.

To create these visualizations, follow these steps:

1. From the OpenSearch Dashboards main menu, navigate to **Observability** > **Metrics** > **Available Metrics**.
2. Choose the metrics to add to your visualization and then select **Save**.
3. When prompted for a **Custom operational dashboards/application**, choose one of the listed options. You can edit the predefined name values in the **Metric Name** field.
4. Select **Save** to save your visualization. An example visualization is shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/metrics2.png"" alt=""Metrics analytics dashboard with visualizations"" width=""700""/>

## Defining PPL queries for Prometheus metrics

You can define [Piped Processing Language (PPL)]({{site.url}}{{site.baseurl}}/search-plugins/sql/ppl/index/) queries to interact with metrics collected by Prometheus. The following is an example PPL query for a Prometheus metric:

```
source = my_prometheus.prometheus_http_requests_total | stats avg(@value) by span(@timestamp,15s), handler, code
```
{% include copy-curl.html %}

### Creating a custom visualization based on the PPL query

To create a custom visualization based on the PPL query, follow these steps:

1. From the **Logs** page, select > **Event Explorer**.
2. On the **Explorer** page,  enter your PPL query and select **Run**. Then select **Save**.
3. When prompted to choose a **Custom Operational Dashboards/Application**, select one of the listed options. Optionally, you can edit the predefined name values in the **Metric Name** fields and can choose to save the visualization as a metric.
5. Select **Save** to save your custom visualization. 

Only queries that include a time-series visualization and statistics or span information can be saved as a metric, as shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/metrics3.png"" alt=""Saving queries as metrics"" width=""700""/>
"
How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?,"---
layout: default
title: API
nav_order: 50
parent: Notifications
redirect_from:
  - /notifications-plugin/api/
---

# Notifications API

If you want to programmatically define your notification channels and sources for versioning and reuse, you can use the Notifications REST API to define, configure, and delete notification channels and send test messages.

---

#### Table of contents
1. TOC
{:toc}

---

## List supported channel configurations

To retrieve a list of all supported notification configuration types, send a GET request to the `features` resource.

#### Example request

```json
GET /_plugins/_notifications/features
```

#### Example response

```json
{
  ""allowed_config_type_list"" : [
    ""slack"",
    ""chime"",
    ""webhook"",
    ""email"",
    ""sns"",
    ""ses_account"",
    ""smtp_account"",
    ""email_group""
  ],
  ""plugin_features"" : {
    ""tooltip_support"" : ""true""
  }
}
```

## List all notification channels

To retrieve a list of all notification channels, send a GET request to the `channels` resource.

#### Example request

```json
GET /_plugins/_notifications/channels
```

#### Example response

```json
{
  ""start_index"" : 0,
  ""total_hits"" : 2,
  ""total_hit_relation"" : ""eq"",
  ""channel_list"" : [
    {
      ""config_id"" : ""sample-id"",
      ""name"" : ""Sample Slack Channel"",
      ""description"" : ""This is a Slack channel"",
      ""config_type"" : ""slack"",
      ""is_enabled"" : true
    },
    {
      ""config_id"" : ""sample-id2"",
      ""name"" : ""Test chime channel"",
      ""description"" : ""A test chime channel"",
      ""config_type"" : ""chime"",
      ""is_enabled"" : true
    }
  ]
}
```

## List all notification configurations

To retrieve a list of all notification configurations, send a GET request to the `configs` resource.

#### Example request

```json
GET _plugins/_notifications/configs
```

#### Example response

```json
{
  ""start_index"" : 0,
  ""total_hits"" : 2,
  ""total_hit_relation"" : ""eq"",
  ""config_list"" : [
    {
      ""config_id"" : ""sample-id"",
      ""last_updated_time_ms"" : 1652760532774,
      ""created_time_ms"" : 1652760532774,
      ""config"" : {
        ""name"" : ""Sample Slack Channel"",
        ""description"" : ""This is a Slack channel"",
        ""config_type"" : ""slack"",
        ""is_enabled"" : true,
        ""slack"" : {
          ""url"" : ""https://sample-slack-webhook""
        }
      }
    },
    {
      ""config_id"" : ""sample-id2"",
      ""last_updated_time_ms"" : 1652760735380,
      ""created_time_ms"" : 1652760735380,
      ""config"" : {
        ""name"" : ""Test chime channel"",
        ""description"" : ""A test chime channel"",
        ""config_type"" : ""chime"",
        ""is_enabled"" : true,
        ""chime"" : {
          ""url"" : ""https://sample-chime-webhook""
        }
      }
    }
  ]
}
```

To filter the notification configuration types this request returns, you can refine your query with the following optional path parameters.

Parameter	| Description
:--- | :---
config_id | Specifies the channel identifier.
config_id_list | Specifies a comma-separated list of channel IDs.
from_index | The starting index to search from.
max_items | The maximum amount of items to return in your request.
sort_order | Specifies the direction to sort results in. Valid options are `asc` and `desc`.
sort_field | Field to sort results with.
last_updated_time_ms | The Unix time in milliseconds of when the channel was last updated.
created_time_ms | The Unix time in milliseconds of when the channel was created.
is_enabled | Indicates whether the channel is enabled.
config_type | The channel type. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`.
name | The channel name.
description	| The channel description.
email.email_account_id | The sender email addresses the channel uses.
email.email_group_id_list | The email groups the channel uses.
email.recipient_list | The channel recipient list.
email_group.recipient_list | The channel list of email recipient groups.
smtp_account.method | The email encryption method.
slack.url	| The Slack channel URL.
chime.url	| The Amazon Chime connection URL.
webhook.url	| The webhook URL.
smtp_account.host	| The domain of the SMTP account.
smtp_account.from_address	| The email account's sender address.
smtp_account.method | The SMTP account's encryption method.
sns.topic_arn	| The Amazon Simple Notification Service (SNS) topic's ARN.
sns.role_arn | The Amazon SNS topic's role ARN.
ses_account.region | The Amazon Simple Email Service (SES) account's AWS Region.
ses_account.role_arn | The Amazon SES account's role ARN.
ses_account.from_address | The Amazon SES account's sender email address.

## Create channel configuration

To create a notification channel configuration, send a POST request to the `configs` resource.

#### Example request

```json
POST /_plugins/_notifications/configs/
{
  ""config_id"": ""sample-id"",
  ""name"": ""sample-name"",
  ""config"": {
    ""name"": ""Sample Slack Channel"",
    ""description"": ""This is a Slack channel"",
    ""config_type"": ""slack"",
    ""is_enabled"": true,
    ""slack"": {
      ""url"": ""https://sample-slack-webhook""
    }
  }
}
```

The create channel API operation accepts the following fields in its request body:

Field |	Data type |	Description |	Required
:--- | :--- | :--- | :---
config_id | String | The configuration's custom ID. | No
config | Object |	Contains all relevant information, such as channel name, configuration type, and plugin source. |	Yes
name | String |	Name of the channel. | Yes
description |	String | The channel's description. | No
config_type |	String | The destination of your notification. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`. | Yes
is_enabled | Boolean | Indicates whether the channel is enabled for sending and receiving notifications. Default is `true`.	| No

The create channel operation accepts multiple `config_types` as possible notification destinations, so follow the format for your preferred `config_type`.

```json
""sns"": {
  ""topic_arn"": ""<arn>"",
  ""role_arn"": ""<arn>"" //optional
}
""slack"": {
  ""url"": ""https://sample-chime-webhoook""
}
""chime"": {
  ""url"": ""https://sample-amazon-chime-webhoook""
}
""webhook"": {
      ""url"": ""https://custom-webhook-test-url.com:8888/test-path?params1=value1&params2=value2""
}
""smtp_account"": {
  ""host"": ""test-host.com"",
  ""port"": 123,
  ""method"": ""start_tls"",
  ""from_address"": ""test@email.com""
}
""ses_account"": {
  ""region"": ""us-east-1"",
  ""role_arn"": ""arn:aws:iam::012345678912:role/NotificationsSESRole"",
  ""from_address"": ""test@email.com""
}
""email_group"": { //Email recipient group
  ""recipient_list"": [
    {
      ""recipient"": ""test-email1@test.com""
    },
    {
      ""recipient"": ""test-email2@test.com""
    }
  ]
}
""email"": { //The channel that sends emails
  ""email_account_id"": ""<smtp or ses account config id>"",
  ""recipient_list"": [
    {
      ""recipient"": ""custom.email@test.com""
    }
  ],
  ""email_group_id_list"": []
}
```

The following example demonstrates how to create a channel using email as a `config_type`:

```json
POST /_plugins/_notifications/configs/
{
  ""id"": ""sample-email-id"",
  ""name"": ""sample-name"",
  ""config"": {
    ""name"": ""Sample Email Channel"",
    ""description"": ""Sample email description"",
    ""config_type"": ""email"",
    ""is_enabled"": true,
    ""email"": {
      ""email_account_id"": ""<email_account_id>"",
      ""recipient_list"": [
        ""sample@email.com""
      ]
    }
  }
}
```

#### Example response

```json
{
  ""config_id"" : ""<config_id>""
}
```


## Get channel configuration

To get a channel configuration by `config_id`, send a GET request and specify the `config_id` as a path parameter.

#### Example request

```json
GET _plugins/_notifications/configs/<config_id>
```

#### Example response

```json
{
  ""start_index"" : 0,
  ""total_hits"" : 1,
  ""total_hit_relation"" : ""eq"",
  ""config_list"" : [
    {
      ""config_id"" : ""sample-id"",
      ""last_updated_time_ms"" : 1652760532774,
      ""created_time_ms"" : 1652760532774,
      ""config"" : {
        ""name"" : ""Sample Slack Channel"",
        ""description"" : ""This is a Slack channel"",
        ""config_type"" : ""slack"",
        ""is_enabled"" : true,
        ""slack"" : {
          ""url"" : ""https://sample-slack-webhook""
        }
      }
    }
  ]
}
```


## Update channel configuration

To update a channel configuration, send a POST request to the `configs` resource and specify the channel's `config_id` as a path parameter. Specify the new configuration details in the request body.

#### Example request

```json
PUT _plugins/_notifications/configs/<config_id>
{
  ""config"": {
    ""name"": ""Slack Channel"",
    ""description"": ""This is an updated channel configuration"",
    ""config_type"": ""slack"",
    ""is_enabled"": true,
    ""slack"": {
      ""url"": ""https://hooks.slack.com/sample-url""
    }
  }
}
```

#### Example response

```json
{
  ""config_id"" : ""<config_id>""
}
```


## Delete channel configuration

To delete a channel configuration, send a DELETE request to the `configs` resource and specify the `config_id` as a path parameter.

#### Example request

```json
DELETE /_plugins/_notifications/configs/<config_id>
```

#### Example response

```json
{
  ""delete_response_list"" : {
  ""<config_id>"" : ""OK""
  }
}
```

You can also submit a comma-separated list of channel IDs you want to delete, and OpenSearch deletes all of the specified notification channels.

#### Example request

```json
DELETE /_plugins/_notifications/configs/?config_id_list=<config_id1>,<config_id2>,<config_id3>...
```

#### Example response

```json
{
  ""delete_response_list"" : {
  ""<config_id1>"" : ""OK"",
  ""<config_id2>"" : ""OK"",
  ""<config_id3>"" : ""OK""
  }
}
```


## Send test notification

To send a test notification, send a GET request to `/feature/test/` and specify the channel configuration's `config_id` as a path parameter.

#### Example request

```json
GET _plugins/_notifications/feature/test/<config_id>
```

#### Example response

```json
{
  ""event_source"" : {
    ""title"" : ""Test Message Title-0Jnlh4ABa4TCWn5C5H2G"",
    ""reference_id"" : ""0Jnlh4ABa4TCWn5C5H2G"",
    ""severity"" : ""info"",
    ""tags"" : [ ]
  },
  ""status_list"" : [
    {
      ""config_id"" : ""0Jnlh4ABa4TCWn5C5H2G"",
      ""config_type"" : ""slack"",
      ""config_name"" : ""sample-id"",
      ""email_recipient_status"" : [ ],
      ""delivery_status"" : {
        ""status_code"" : ""200"",
        ""status_text"" : """"""<!doctype html>
<html>
<head>
</head>
<body>
<div>
    <h1>Example Domain</h1>
    <p>Sample paragraph.</p>
    <p><a href=""sample.example.com"">TO BE OR NOT TO BE, THAT IS THE QUESTION</a></p>
</div>
</body>
</html>
""""""
      }
    }
  ]
}

```
"
"What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?","---
layout: default
title: Installing plugins
nav_order: 90
has_children: true
redirect_from:
   - /opensearch/install/plugins/
   - /install-and-configure/install-opensearch/plugins/
---

# Installing plugins

OpenSearch comprises of a number of plugins that add features and capabilities to the core platform. The plugins available to you are dependent on how OpenSearch was installed and which plugins were subsequently added or removed. For example, the minimal distribution of OpenSearch enables only core functionality, such as indexing and search. Using the minimal distribution of OpenSearch is beneficial when you are working in a testing environment, have custom plugins, or are intending to integrate OpenSearch with other services.

The standard distribution of OpenSearch has much more functionality included. You can choose to add additional plugins or remove any of the plugins you don't need. 

For a list of the available plugins, see [Available plugins](#available-plugins).

For a plugin to work properly with OpenSearch, it may request certain permissions as part of the installation process. Review the requested permissions and proceed accordingly. It is important that you understand a plugin's functionality before installation. When opting for a community-provided plugin, ensure that the source is trustworthy and reliable.
{: .warning}

## Managing plugins

To manage plugins in OpenSearch, you can use a command line tool called `opensearch-plugin`. This tool allows you to perform the following actions:

- [List](#list) installed plugins.
- [Install](#install) plugins.
- [Remove](#remove) an installed plugin.

You can print help text by passing `-h` or `--help`. Depending on your host configuration, you might also need to run the command with `sudo` privileges.

If you're running OpenSearch in a Docker container, plugins must be installed, removed, and configured by modifying the Docker image. For more information, see [Working with plugins]({{site.url}}{{site.baseurl}}/install-and-configure/install-opensearch/docker#working-with-plugins).
{: .note}

## List

Use `list` to see a list of plugins that have already been installed.

#### Usage
```bash
bin/opensearch-plugin list
```

#### Example
```bash
$ ./opensearch-plugin list
opensearch-alerting
opensearch-anomaly-detection
opensearch-asynchronous-search
opensearch-cross-cluster-replication
opensearch-geospatial
opensearch-index-management
opensearch-job-scheduler
opensearch-knn
opensearch-ml
opensearch-notifications
opensearch-notifications-core
opensearch-observability
opensearch-performance-analyzer
opensearch-reports-scheduler
opensearch-security
opensearch-sql
```

## List (with CAT API)
You can also list installed plugins by using the [CAT API]({{site.url}}{{site.baseurl}}/api-reference/cat/cat-plugins/).

#### Usage

```bash
GET _cat/plugins
```

#### Example response

```bash
opensearch-node1 opensearch-alerting                  2.0.1.0
opensearch-node1 opensearch-anomaly-detection         2.0.1.0
opensearch-node1 opensearch-asynchronous-search       2.0.1.0
opensearch-node1 opensearch-cross-cluster-replication 2.0.1.0
opensearch-node1 opensearch-index-management          2.0.1.0
opensearch-node1 opensearch-job-scheduler             2.0.1.0
opensearch-node1 opensearch-knn                       2.0.1.0
opensearch-node1 opensearch-ml                        2.0.1.0
opensearch-node1 opensearch-notifications             2.0.1.0
opensearch-node1 opensearch-notifications-core        2.0.1.0
```

## Install

There are three ways to install plugins using the `opensearch-plugin` tool:

- [Install a plugin by name](#install-a-plugin-by-name).
- [Install a plugin from a zip file](#install-a-plugin-from-a-zip-file).
- [Install a plugin using Maven coordinates](#install-a-plugin-using-maven-coordinates).

### Install a plugin by name

You can install plugins that aren't already preinstalled in your installation by using the plugin name. For a list of plugins that may not be preinstalled, see [Additional plugins](#additional-plugins).

#### Usage
```bash
bin/opensearch-plugin install <plugin-name>
```

#### Example
```bash
$ sudo ./opensearch-plugin install analysis-icu
-> Installing analysis-icu
-> Downloading analysis-icu from opensearch
[=================================================] 100%   
-> Installed analysis-icu with folder name analysis-icu
```

### Install a plugin from a zip file

You can install remote zip files by replacing `<zip-file>` with the URL of the hosted file. The tool supports downloading over HTTP/HTTPS protocols only. For local zip files, replace `<zip-file>` with `file:` followed by the absolute or relative path to the plugin zip file, as shown in the second example that follows.

#### Usage
```bash
bin/opensearch-plugin install <zip-file>
```

#### Example
<details markdown=""block"">
  <summary>
    Select to expand the example 
  </summary>
  {: .text-delta}
  
```bash
# Zip file is hosted on a remote server - in this case, Maven central repository.
$ sudo ./opensearch-plugin install https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip
-> Installing https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip
-> Downloading https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip
[=================================================] 100%   
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.misc
* java.lang.RuntimePermission accessDeclaredMembers
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.reflect.ReflectPermission suppressAccessChecks
* java.net.SocketPermission * connect,resolve
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean
* javax.management.MBeanServerPermission createMBeanServer
* javax.management.MBeanTrustPermission register
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection

# Zip file in a local directory.
$ sudo ./opensearch-plugin install file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip
-> Installing file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip
-> Downloading file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip
[=================================================] 100%   
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.misc
* java.lang.RuntimePermission accessDeclaredMembers
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.reflect.ReflectPermission suppressAccessChecks
* java.net.SocketPermission * connect,resolve
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean
* javax.management.MBeanServerPermission createMBeanServer
* javax.management.MBeanTrustPermission register
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection
```
</details>

### Install a plugin using Maven coordinates

The `opensearch-plugin install` tool also allows you to specify Maven coordinates for available artifacts and versions hosted on [Maven Central](https://search.maven.org/search?q=org.opensearch.plugin). The tool parses the Maven coordinates you provide and constructs a URL. As a result, the host must be able to connect directly to the Maven Central site. The plugin installation fails if you pass coordinates to a proxy or local repository.

#### Usage
```bash
bin/opensearch-plugin install <groupId>:<artifactId>:<version>
```

#### Example

<details markdown=""block"">
  <summary>
    Select to expand the example 
  </summary>
  {: .text-delta}

```console
$ sudo ./opensearch-plugin install org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0
-> Installing org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0
-> Downloading org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 from maven central
[=================================================] 100%   
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.misc
* java.lang.RuntimePermission accessDeclaredMembers
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.reflect.ReflectPermission suppressAccessChecks
* java.net.SocketPermission * connect,resolve
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean
* javax.management.MBeanServerPermission createMBeanServer
* javax.management.MBeanTrustPermission register
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection
```
</details>

Restart your OpenSearch node after installing a plugin.
{: .note}

## Installing multiple plugins

Multiple plugins can be installed in a single invocation.

#### Usage
```bash
bin/opensearch-plugin install <plugin-name> <plugin-name> ... <plugin-name>
```

#### Example
```console
$ sudo ./opensearch-plugin install analysis-nori repository-s3
```

## Remove

You can remove a plugin that has already been installed with the `remove` option. 

#### Usage
```bash
bin/opensearch-plugin remove <plugin-name>
```

#### Example
```console
$ sudo ./opensearch-plugin remove opensearch-anomaly-detection
-> removing [opensearch-anomaly-detection]...
```

Restart your OpenSearch node after removing a plugin.
{: .note}

## Batch mode

When installing a plugin that requires additional privileges that are not included by default, the plugin will prompt you for confirmation of the required privileges. To grant all requested privileges, use batch mode to skip the confirmation prompt.

To force batch mode when installing plugins, add the `-b` or `--batch` option:
```bash
bin/opensearch-plugin install --batch <plugin-name>
```

## Available plugins

OpenSearch provides several bundled plugins that are available for immediate use with all OpenSearch distributions except for the minimal distribution. Additional plugins are available but must be installed separately using one of the installation options.  

### Bundled plugins

The following plugins are bundled with all OpenSearch distributions except for the minimal distribution. If you are using the minimal distribution, you can add these plugins by using one of the installation methods.

| Plugin name | Repository | Earliest available version |
| :--- | :--- | :--- |
| Alerting | [opensearch-alerting](https://github.com/opensearch-project/alerting) | 1.0.0 |
| Anomaly Detection | [opensearch-anomaly-detection](https://github.com/opensearch-project/anomaly-detection) | 1.0.0 |
| Asynchronous Search | [opensearch-asynchronous-search](https://github.com/opensearch-project/asynchronous-search) | 1.0.0 |
| Cross Cluster Replication | [opensearch-cross-cluster-replication](https://github.com/opensearch-project/cross-cluster-replication) | 1.1.0 |
| Custom Codecs | [opensearch-custom-codecs](https://github.com/opensearch-project/custom-codecs) | 2.10.0 |
| Flow Framework | [flow-framework](https://github.com/opensearch-project/flow-framework) | 2.12.0 |
| Notebooks<sup>1</sup> | [opensearch-notebooks](https://github.com/opensearch-project/dashboards-notebooks) | 1.0.0 to 1.1.0 |
| Notifications | [notifications](https://github.com/opensearch-project/notifications) | 2.0.0
| Reports Scheduler | [opensearch-reports-scheduler](https://github.com/opensearch-project/dashboards-reports) | 1.0.0 |
| Geospatial | [opensearch-geospatial](https://github.com/opensearch-project/geospatial) | 2.2.0 |
| Index Management | [opensearch-index-management](https://github.com/opensearch-project/index-management) | 1.0.0 |
| Job Scheduler | [opensearch-job-scheduler](https://github.com/opensearch-project/job-scheduler) | 1.0.0 |
| k-NN | [opensearch-knn](https://github.com/opensearch-project/k-NN) | 1.0.0 |
| ML Commons | [opensearch-ml](https://github.com/opensearch-project/ml-commons) | 1.3.0 |
| Skills | [opensearch-skills](https://github.com/opensearch-project/skills) | 2.12.0 |
| Neural Search | [neural-search](https://github.com/opensearch-project/neural-search) | 2.4.0 |
| Observability | [opensearch-observability](https://github.com/opensearch-project/observability) | 1.2.0 |
| Performance Analyzer<sup>2</sup> | [opensearch-performance-analyzer](https://github.com/opensearch-project/performance-analyzer) | 1.0.0 |
| Security | [opensearch-security](https://github.com/opensearch-project/security) | 1.0.0 |
| Security Analytics | [opensearch-security-analytics](https://github.com/opensearch-project/security-analytics) | 2.4.0 |
| SQL | [opensearch-sql](https://github.com/opensearch-project/sql) | 1.0.0 |

_<sup>1</sup>Dashboard Notebooks was merged in to the Observability plugin with the release of OpenSearch 1.2.0._<br>
_<sup>2</sup>Performance Analyzer is not available on Windows._


### Additional plugins

There are many more plugins available in addition to those provided by the default distribution. These additional plugins have been built by OpenSearch developers or members of the OpenSearch community. For a list of additional plugins you can install, see [Additional plugins]({{site.url}}{{site.baseurl}}/install-and-configure/additional-plugins/index/).

## Plugin compatibility

You can specify plugin compatibility with a particular OpenSearch version in the `plugin-descriptor.properties` file. For example, a plugin with the following property is compatible only with OpenSearch 2.3.0:

```properties
opensearch.version=2.3.0
```
Alternatively, you can specify a range of compatible OpenSearch versions by setting the `dependencies` property in the `plugin-descriptor.properties` file to one of the following notations:
- `dependencies={ opensearch: ""2.3.0"" }`: The plugin is compatible only with OpenSearch version 2.3.0.
- `dependencies={ opensearch: ""=2.3.0"" }`: The plugin is compatible only with OpenSearch version 2.3.0.
- `dependencies={ opensearch: ""~2.3.0"" }`: The plugin is compatible with all versions from 2.3.0 up to the next minor version, in this example, 2.4.0 (exclusive).
- `dependencies={ opensearch: ""^2.3.0"" }`: The plugin is compatible with all versions from 2.3.0 up to the next major version, in this example, 3.0.0 (exclusive).

You can specify only one of the `opensearch.version` or `dependencies` properties.
{: .note}

## Related links

- [Observability]({{site.url}}{{site.baseurl}}/observability-plugin/index/)
- [Security Analytics]({{site.url}}{{site.baseurl}}/security-analytics/index/)
- [Security]({{site.url}}{{site.baseurl}}/security/index/)
- [Alerting]({{site.url}}{{site.baseurl}}/monitoring-plugins/alerting/index/)
- [Anomaly detection]({{site.url}}{{site.baseurl}}/monitoring-plugins/ad/index/)
- [Asynchronous search]({{site.url}}{{site.baseurl}}/search-plugins/async/index/)
- [Cross-cluster replication]({{site.url}}{{site.baseurl}}/replication-plugin/index/)
- [Index State Management]({{site.url}}{{site.baseurl}}/im-plugin/ism/index/)
- [k-NN search]({{site.url}}{{site.baseurl}}/search-plugins/knn/index/)
- [ML Commons]({{site.url}}{{site.baseurl}}/ml-commons-plugin/index/)
- [Neural search]({{site.url}}{{site.baseurl}}/neural-search-plugin/index/)
- [Notifications]({{site.url}}{{site.baseurl}}/notifications-plugin/index/)
- [OpenSearch Dashboards]({{site.url}}{{site.baseurl}}/dashboards/index/)
- [Performance Analyzer]({{site.url}}{{site.baseurl}}/monitoring-plugins/pa/index/)
- [SQL]({{site.url}}{{site.baseurl}}/search-plugins/sql/index/)
"
What environment variable and YAML structure must be used to customize the admin password in a helm chart? ,"---
layout: default
title: Helm
parent: Installing OpenSearch
nav_order: 6
redirect_from:
  - /opensearch/install/helm/
---

# Helm

Helm is a package manager that allows you to easily install and manage OpenSearch in a Kubernetes cluster. You can define your OpenSearch configurations in a YAML file and use Helm to deploy your applications in a version-controlled and reproducible way.

The Helm chart contains the resources described in the following table.

Resource | Description
:--- | :---
`Chart.yaml` |  Information about the chart.
`values.yaml` |  Default configuration values for the chart.
`templates` |  Templates that combine with values to generate the Kubernetes manifest files.

The specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC).

For information about the default configuration, steps to configure security, and configurable parameters, see the
[README](https://github.com/opensearch-project/helm-charts/blob/main/README.md).

The instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the [Kubernetes documentation](https://kubernetes.io/docs/setup/) for steps to configure a Kubernetes cluster and the [Helm documentation](https://helm.sh/docs/intro/install/) to install Helm.
{: .note }

## Prerequisites

The default Helm chart deploys a three-node cluster. We recommend that you have at least 8 GiB of memory available for this deployment. You can expect the deployment to fail if, say, you have less than 4 GiB of memory available.

## Install OpenSearch using Helm

1. Add `opensearch` [helm-charts](https://github.com/opensearch-project/helm-charts) repository to Helm:

   ```bash
   helm repo add opensearch https://opensearch-project.github.io/helm-charts/
   ```
   {% include copy.html %}

1. Update the available charts locally from charts repositories:

   ```bash
   helm repo update
   ```
   {% include copy.html %}

1. To search for the OpenSearch-related Helm charts:

   ```bash
   helm search repo opensearch
   ```
   {% include copy.html %}

   ```bash
   NAME                            	CHART VERSION	APP VERSION	DESCRIPTION                           
   opensearch/opensearch           	1.0.7        	1.0.0      	A Helm chart for OpenSearch           
   opensearch/opensearch-dashboards	1.0.4        	1.0.0      	A Helm chart for OpenSearch Dashboards
   ```

1. Deploy OpenSearch:

   ```bash
   helm install my-deployment opensearch/opensearch
   ```
   {% include copy.html %}

You can also build the `opensearch-1.0.0.tgz` file manually:

1. Change to the `opensearch` directory:

   ```bash
   cd charts/opensearch
   ```
   {% include copy.html %}

1. Package the Helm chart:

   ```bash
   helm package .
   ```
   {% include copy.html %}

1. Deploy OpenSearch:

   ```bash
   helm install --generate-name opensearch-1.0.0.tgz
   ```
   {% include copy.html %}

   The output shows you the specifications instantiated from the install.
   To customize the deployment, pass in the values that you want to override with a custom YAML file:

   ```bash
   helm install --values=customvalues.yaml opensearch-1.0.0.tgz
   ```
   {% include copy.html %}

For OpenSearch 2.12 or greater, customize the admin password in `values.yaml` under `extraEnvs`, as shown in the following example:

```yaml
extraEnvs:
  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
    value: <custom-admin-password>
```

#### Sample output

  ```yaml
  NAME: opensearch-1-1629223146
  LAST DEPLOYED: Tue Aug 17 17:59:07 2021
  NAMESPACE: default
  STATUS: deployed
  REVISION: 1
  TEST SUITE: None
  NOTES:
  Watch all cluster members come up.
    $ kubectl get pods --namespace=default -l app=opensearch-cluster-master -w
  ```

To make sure your OpenSearch pod is up and running, run the following command:

```bash
$ kubectl get pods
NAME                                                  READY   STATUS    RESTARTS   AGE
opensearch-cluster-master-0                           1/1     Running   0          3m56s
opensearch-cluster-master-1                           1/1     Running   0          3m56s
opensearch-cluster-master-2                           1/1     Running   0          3m56s
```

To access the OpenSearch shell:

```bash
$ kubectl exec -it opensearch-cluster-master-0 -- /bin/bash
```
{% include copy.html %}

You can send requests to the pod to verify that OpenSearch is up and running:

```json
$ curl -XGET https://localhost:9200 -u 'admin:admin' --insecure
{
  ""name"" : ""opensearch-cluster-master-1"",
  ""cluster_name"" : ""opensearch-cluster"",
  ""cluster_uuid"" : ""hP2gq5bPS3SLp8Z7wXm8YQ"",
  ""version"" : {
    ""distribution"" : ""opensearch"",
    ""number"" : <version>,
    ""build_type"" : <build-type>,
    ""build_hash"" : <build-hash>,
    ""build_date"" : <build-date>,
    ""build_snapshot"" : false,
    ""lucene_version"" : <lucene-version>,
    ""minimum_wire_compatibility_version"" : ""6.8.0"",
    ""minimum_index_compatibility_version"" : ""6.0.0-beta1""
  },
  ""tagline"" : ""The OpenSearch Project: https://opensearch.org/""
}
```

## Uninstall using Helm

To identify the OpenSearch deployment that you want to delete:

```bash
$ helm list
NAME  NAMESPACEREVISIONUPDATED  STATUS  CHART APP VERSION
opensearch-1-1629223146 default 1 2021-08-17 17:59:07.664498239 +0000 UTCdeployedopensearch-1.0.0    1.0.0       
```

To delete or uninstall a deployment, run the following command:

```bash
helm delete opensearch-1-1629223146
```
{% include copy.html %}

For steps to install OpenSearch Dashboards, see [Helm to install OpenSearch Dashboards]({{site.url}}{{site.baseurl}}/dashboards/install/helm/)."
How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?,"---
layout: default
title: Reverse nested
parent: Bucket aggregations
nav_order: 160
redirect_from:
  - /query-dsl/aggregations/bucket/reverse-nested/
---

# Reverse nested aggregations

You can aggregate values from nested documents to their parent; this aggregation is called `reverse_nested`.
You can use `reverse_nested` to aggregate a field from the parent document after grouping by the field from the nested object. The `reverse_nested` aggregation ""joins back"" the root page and gets the `load_time` for each for your variations.

The `reverse_nested` aggregation is a sub-aggregation inside a nested aggregation. It accepts a single option named `path`. This option defines how many steps backwards in the document hierarchy OpenSearch takes to calculate the aggregations.

```json
GET logs/_search
{
  ""query"": {
    ""match"": { ""response"": ""200"" }
  },
  ""aggs"": {
    ""pages"": {
      ""nested"": {
        ""path"": ""pages""
      },
      ""aggs"": {
        ""top_pages_per_load_time"": {
          ""terms"": {
            ""field"": ""pages.load_time""
          },
          ""aggs"": {
            ""comment_to_logs"": {
              ""reverse_nested"": {},
              ""aggs"": {
                ""min_load_time"": {
                  ""min"": {
                    ""field"": ""pages.load_time""
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

#### Example response

```json
...
""aggregations"" : {
  ""pages"" : {
    ""doc_count"" : 2,
    ""top_pages_per_load_time"" : {
      ""doc_count_error_upper_bound"" : 0,
      ""sum_other_doc_count"" : 0,
      ""buckets"" : [
        {
          ""key"" : 200.0,
          ""doc_count"" : 1,
          ""comment_to_logs"" : {
            ""doc_count"" : 1,
            ""min_load_time"" : {
              ""value"" : null
            }
          }
        },
        {
          ""key"" : 500.0,
          ""doc_count"" : 1,
          ""comment_to_logs"" : {
            ""doc_count"" : 1,
            ""min_load_time"" : {
              ""value"" : null
            }
          }
        }
      ]
    }
  }
 }
}
```

The response shows the logs index has one page with a `load_time` of 200 and one with a `load_time` of 500."
Make it simpler like what happens when I have missing values in an agg?,"---
layout: default
title: Missing
parent: Bucket aggregations
nav_order: 120
redirect_from:
  - /query-dsl/aggregations/bucket/missing/
---

# Missing aggregations

If you have documents in your index that don’t contain the aggregating field at all or the aggregating field has a value of NULL, use the `missing` parameter to specify the name of the bucket such documents should be placed in.

The following example adds any missing values to a bucket named ""N/A"":

```json
GET opensearch_dashboards_sample_data_logs/_search
{
  ""size"": 0,
  ""aggs"": {
    ""response_codes"": {
      ""terms"": {
        ""field"": ""response.keyword"",
        ""size"": 10,
        ""missing"": ""N/A""
      }
    }
  }
}
```
{% include copy-curl.html %}

Because the default value for the `min_doc_count` parameter is 1, the `missing` parameter doesn't return any buckets in its response. Set `min_doc_count` parameter to 0 to see the ""N/A"" bucket in the response:

```json
GET opensearch_dashboards_sample_data_logs/_search
{
  ""size"": 0,
  ""aggs"": {
    ""response_codes"": {
      ""terms"": {
        ""field"": ""response.keyword"",
        ""size"": 10,
        ""missing"": ""N/A"",
        ""min_doc_count"": 0
      }
    }
  }
}
```

#### Example response

```json
...
""aggregations"" : {
  ""response_codes"" : {
    ""doc_count_error_upper_bound"" : 0,
    ""sum_other_doc_count"" : 0,
    ""buckets"" : [
      {
        ""key"" : ""200"",
        ""doc_count"" : 12832
      },
      {
        ""key"" : ""404"",
        ""doc_count"" : 801
      },
      {
        ""key"" : ""503"",
        ""doc_count"" : 441
      },
      {
        ""key"" : ""N/A"",
        ""doc_count"" : 0
      }
    ]
  }
 }
}
```"
Can you explain compound queries to me? ,"---
layout: default
title: Compound queries
has_children: true
has_toc: false
nav_order: 40
redirect_from: 
  - /opensearch/query-dsl/compound/index/
  - /query-dsl/compound/index/
  - /query-dsl/query-dsl/compound/
  - /query-dsl/compound/
---

# Compound queries

Compound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. 

The following table lists all compound query types.

Query type | Description
:--- | :---
[`bool`]({{site.url}}{{site.baseurl}}/query-dsl/compound/bool/) (Boolean)| Combines multiple query clauses with Boolean logic. 
[`boosting`]({{site.url}}{{site.baseurl}}/query-dsl/compound/boosting/) | Changes the relevance score of documents without removing them from the search results. Returns documents that match a `positive` query, but downgrades the relevance of documents in the results that match a `negative` query.
[`constant_score`]({{site.url}}{{site.baseurl}}/query-dsl/compound/constant-score/) | Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the `boost` value.
[`dis_max`]({{site.url}}{{site.baseurl}}/query-dsl/compound/disjunction-max/) (disjunction max) | Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value.
[`function_score`]({{site.url}}{{site.baseurl}}/query-dsl/compound/function-score/) | Recalculates the relevance score of documents that are returned by a query using a function that you define.
[`hybrid`]({{site.url}}{{site.baseurl}}/query-dsl/compound/hybrid/) | Combines relevance scores from multiple queries into one score for a given document.
"
What is the difference between vector search and full text search? ,"---
layout: default
title: Term-level and full-text queries compared
nav_order: 10
redirect_from:
  - /query-dsl/query-dsl/term-vs-full-text/
  - /opensearch/query-dsl/term-vs-full-text/
---

# Term-level and full-text queries compared

You can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries [analyze]({{site.url}}{{site.baseurl}}/analyzers/) the query string. The following table summarizes the differences between term-level and full-text queries.

| | Term-level queries | Full-text queries
:--- | :--- | :---
*Description* | Term-level queries answer which documents match a query. | Full-text queries answer how well the documents match a query.
*Analyzer* | The search term isn't analyzed. This means that the term query searches for your search term as it is.  | The search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document's field.
*Relevance* | Term-level queries simply return documents that match without sorting them based on the relevance score. They still calculate the relevance score, but this score is the same for all the documents that are returned. | Full-text queries calculate a relevance score for each match and sort the results by decreasing order of relevance.
*Use Case* | Use term-level queries when you want to match exact values such as numbers, dates, or tags and don't need the matches to be sorted by relevance. | Use full-text queries to match text fields and sort by relevance after taking into account factors like casing and stemming variants.

OpenSearch uses the BM25 ranking algorithm to calculate relevance scores. To learn more, see [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25).
{: .note }

## Should I use a full-text or a term-level query?

To clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster.

### Example: Phrase search

In this example, you'll search the complete works of Shakespeare for the phrase ""To be, or not to be"" in the `text_entry` field. 

First, use a **term-level query** for this search:

```json
GET shakespeare/_search
{
  ""query"": {
    ""term"": {
      ""text_entry"": ""To be, or not to be""
    }
  }
}
```

The response contains no matches, indicated by zero `hits`:

```json
{
  ""took"" : 3,
  ""timed_out"" : false,
  ""_shards"" : {
    ""total"" : 1,
    ""successful"" : 1,
    ""skipped"" : 0,
    ""failed"" : 0
  },
  ""hits"" : {
    ""total"" : {
      ""value"" : 0,
      ""relation"" : ""eq""
    },
    ""max_score"" : null,
    ""hits"" : [ ]
  }
}
```

This is because the term “To be, or not to be” is searched literally in the inverted index, where only the analyzed values of the text fields are stored. Term-level queries aren’t suited for searching analyzed text fields because they often yield unexpected results. When working with text data, use term-level queries only for fields mapped as `keyword`.

Now search for the same phrase using a **full-text query**:

```json
GET shakespeare/_search
{
  ""query"": {
    ""match"": {
      ""text_entry"": ""To be, or not to be""
    }
  }
}
```

The search query “To be, or not to be” is analyzed and tokenized into an array of tokens just like the `text_entry` field of the documents. The full-text query takes an intersection of tokens between the search query and the `text_entry` fields for all the documents, and then sorts the results by relevance score:

```json
{
  ""took"" : 19,
  ""timed_out"" : false,
  ""_shards"" : {
    ""total"" : 1,
    ""successful"" : 1,
    ""skipped"" : 0,
    ""failed"" : 0
  },
  ""hits"" : {
    ""total"" : {
      ""value"" : 10000,
      ""relation"" : ""gte""
    },
    ""max_score"" : 17.419369,
    ""hits"" : [
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""34229"",
        ""_score"" : 17.419369,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 34230,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 19,
          ""line_number"" : ""3.1.64"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""To be, or not to be: that is the question:""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""109930"",
        ""_score"" : 14.883024,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 109931,
          ""play_name"" : ""A Winters Tale"",
          ""speech_number"" : 23,
          ""line_number"" : ""4.4.153"",
          ""speaker"" : ""PERDITA"",
          ""text_entry"" : ""Not like a corse; or if, not to be buried,""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""103117"",
        ""_score"" : 14.782743,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 103118,
          ""play_name"" : ""Twelfth Night"",
          ""speech_number"" : 53,
          ""line_number"" : ""1.3.95"",
          ""speaker"" : ""SIR ANDREW"",
          ""text_entry"" : ""will not be seen; or if she be, its four to one""
        }
      }
    ]
  }
}
...
```

For a list of all full-text queries, see [Full-text queries]({{site.url}}{{site.baseurl}}/opensearch/query-dsl/full-text/index).

### Example: Exact term search

If you want to search for an exact term like “HAMLET” in the `speaker` field and don't need the results to be sorted by relevance score, a term-level query is more efficient:

```json
GET shakespeare/_search
{
  ""query"": {
    ""term"": {
      ""speaker"": ""HAMLET""
    }
  }
}
```

The response contains document matches:

```json
{
  ""took"" : 5,
  ""timed_out"" : false,
  ""_shards"" : {
    ""total"" : 1,
    ""successful"" : 1,
    ""skipped"" : 0,
    ""failed"" : 0
  },
  ""hits"" : {
    ""total"" : {
      ""value"" : 1582,
      ""relation"" : ""eq""
    },
    ""max_score"" : 4.2540946,
    ""hits"" : [
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""32700"",
        ""_score"" : 4.2540946,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 32701,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 9,
          ""line_number"" : ""1.2.66"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""[Aside]  A little more than kin, and less than kind.""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""32702"",
        ""_score"" : 4.2540946,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 32703,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 11,
          ""line_number"" : ""1.2.68"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""Not so, my lord; I am too much i' the sun.""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""32709"",
        ""_score"" : 4.2540946,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 32710,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 13,
          ""line_number"" : ""1.2.75"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""Ay, madam, it is common.""
        }
      }
    ]
  }
}
...
```

The term-level queries provide exact matches. So if you search for “Hamlet”, you don’t receive any matches, because “HAMLET” is a keyword field and is stored in OpenSearch literally and not in an analyzed form.
The search query “HAMLET” is also searched literally. So to get a match for this field, we need to enter the exact same characters.
"
"Why can't my coworker see my async searches, but I can see and delete theirs? This role thing is messing everything up!","---
layout: default
title: Asynchronous search security
nav_order: 2
parent: Asynchronous search
grand_parent: Improving search performance
has_children: false
redirect_from:
 - /search-plugins/async/security/
---

# Asynchronous search security

You can use the Security plugin with asynchronous searches to limit non-admin users to specific actions. For example, you might want some users to only be able to submit or delete asynchronous searches, while you might want others to only view the results.

All asynchronous search indexes are protected as system indexes. Only a super admin user or an admin user with a Transport Layer Security (TLS) certificate can access system indexes. For more information, see [System indexes]({{site.url}}{{site.baseurl}}/security/configuration/system-indices/).

## Basic permissions

As an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see [Asynchronous search]({{site.url}}{{site.baseurl}}/).

The Security plugin has two built-in roles that cover most asynchronous search use cases: `asynchronous_search_full_access` and `asynchronous_search_read_access`. For descriptions of each, see [Predefined roles]({{site.url}}{{site.baseurl}}/security/access-control/users-roles#predefined-roles).

If these roles don’t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the `cluster:admin/opensearch/asynchronous_search/delete` permission lets you delete a previously submitted asynchronous search.

### A note on Asynchronous Search and fine-grained access control

By design, the Asynchronous Search plugin extracts data from a target index and stores the data in a separate index to make search results available to users with the proper permissions. Although a user with either the `asynchronous_search_read_access` or `cluster:admin/opensearch/asynchronous_search/get` permission cannot submit the asynchronous search request itself, that user can get and view the search results using the associated search ID. [Document-level security]({{site.url}}{{site.baseurl}}/security/access-control/document-level-security) (DLS) and [field-level security]({{site.url}}{{site.baseurl}}/security/access-control/field-level-security) (FLS) access controls are designed to protect the data in the target index. But once the data is stored outside this index, users with these access permissions are able to use search IDs to get and view asynchronous search results, which may include data that is otherwise concealed by DLS and FLS access control in the target index.

To reduce the chances of unintended users viewing search results that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See [Limit access by backend role](#advanced-limit-access-by-backend-role) for details.

## (Advanced) Limit access by backend role

Use backend roles to configure fine-grained access to asynchronous searches based on roles. For example, users of different departments in an organization can view asynchronous searches owned by their own department.

First, make sure your users have the appropriate [backend roles]({{site.url}}{{site.baseurl}}/security/access-control/index/). Backend roles usually come from an [LDAP server]({{site.url}}{{site.baseurl}}/security/configuration/ldap/) or [SAML provider]({{site.url}}{{site.baseurl}}/security/configuration/saml/). However, if you use the internal user database, you can use the REST API to [add them manually]({{site.url}}{{site.baseurl}}/security/access-control/api#create-user).

Now when users view asynchronous search resources in OpenSearch Dashboards (or make REST API calls), they only see asynchronous searches submitted by users who have a subset of the backend role.
For example, consider two users: `judy` and `elon`.

`judy` has an IT backend role:

```json
PUT _plugins/_security/api/internalusers/judy
{
  ""password"": ""judy"",
  ""backend_roles"": [
    ""IT""
  ],
  ""attributes"": {}
}
```

`elon` has an admin backend role:

```json
PUT _plugins/_security/api/internalusers/elon
{
  ""password"": ""elon"",
  ""backend_roles"": [
    ""admin""
  ],
  ""attributes"": {}
}
```

Both `judy` and `elon` have full access to asynchronous search:

```json
PUT _plugins/_security/api/rolesmapping/async_full_access
{
  ""backend_roles"": [],
  ""hosts"": [],
  ""users"": [
    ""judy"",
    ""elon""
  ]
}
```

Because they have different backend roles, an asynchronous search submitted by `judy` will not be visible to `elon` and vice versa.

`judy` needs to have at least the superset of all roles that `elon` has to see `elon`'s asynchronous searches.

For example, if `judy` has five backend roles and `elon` has one of these roles, then `judy` can see asynchronous searches submitted by `elon`, but `elon` can’t see the asynchronous searches submitted by `judy`. This means that `judy` can perform GET and DELETE operations on asynchronous searches submitted by `elon`, but not the reverse.

If none of the users have any backend roles, all three will be able to see the others' searches.

For example, consider three users: `judy`, `elon`, and `jack`.

`judy`, `elon`, and `jack` have no backend roles set up:

```json
PUT _plugins/_security/api/internalusers/judy
{
  ""password"": ""judy"",
  ""backend_roles"": [],
  ""attributes"": {}
}
```

```json
PUT _plugins/_security/api/internalusers/elon
{
  ""password"": ""elon"",
  ""backend_roles"": [],
  ""attributes"": {}
}
```

```json
PUT _plugins/_security/api/internalusers/jack
{
  ""password"": ""jack"",
  ""backend_roles"": [],
  ""attributes"": {}
}
```

Both `judy` and `elon` have full access to asynchronous search:

```json
PUT _plugins/_security/api/rolesmapping/async_full_access
{
  ""backend_roles"": [],
  ""hosts"": [],
  ""users"": [""judy"",""elon""]
}
```

`jack` has read access to asynchronous search results:

```json
PUT _plugins/_security/api/rolesmapping/async_read_access
{
  ""backend_roles"": [],
  ""hosts"": [],
  ""users"": [""jack""]
}
```

Because none of the users have backend roles, they will be able to see each other's asynchronous searches. So, if `judy` submits an asynchronous search, `elon`, who has full access, will be able to see that search. `jack`, who has read access, will also be able to see `judy`'s asynchronous search."
