query_text,context,llm_response,human_grade,human_reasoning
I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working? ,"---
layout: default
title: Index codecs
nav_order: 3
parent: Index settings
---

# Index codecs

Index codecs determine how the index’s stored fields are compressed and stored on disk. The index codec is controlled by the static `index.codec` setting that specifies the compression algorithm. The setting impacts the index shard size and index operation performance.  

## Supported codecs

OpenSearch provides support for four codecs that can be used for compressing the stored fields. Each codec offers different tradeoffs between compression ratio (storage size) and indexing performance (speed): 

* `default` -- This codec employs the [LZ4 algorithm](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) with a preset dictionary, which prioritizes performance over compression ratio. It offers faster indexing and search operations when compared with `best_compression` but may result in larger index/shard sizes. If no codec is provided in the index settings, then LZ4 is used as the default algorithm for compression.
* `best_compression` -- This codec uses [zlib](https://en.wikipedia.org/wiki/Zlib) as an underlying algorithm for compression. It achieves high compression ratios that result in smaller index sizes. However, this may incur additional CPU usage during index operations and may subsequently result in high indexing and search latencies. 

As of OpenSearch 2.9, two new codecs based on the [Zstandard compression algorithm](https://github.com/facebook/zstd) are available. This algorithm provides a good balance between compression ratio and speed.

It may be challenging to change the codec setting of an existing index (see [Changing an index codec](#changing-an-index-codec)), so it is important to test a representative workload in a non-production environment before using a new codec setting.
{: .important}

* `zstd` (OpenSearch 2.9 and later) -- This codec provides significant compression comparable to the `best_compression` codec with reasonable CPU usage and improved indexing and search performance compared to the `default` codec.
* `zstd_no_dict` (OpenSearch 2.9 and later) -- This codec is similar to `zstd` but excludes the dictionary compression feature. It provides faster indexing and search operations compared to `zstd` at the expense of a slightly larger index size.

As of OpenSearch 2.10, the `zstd` and `zstd_no_dict` compression codecs cannot be used for [k-NN]({{site.url}}{{site.baseurl}}/search-plugins/knn/index/) or [Security Analytics]({{site.url}}{{site.baseurl}}/security-analytics/index/) indexes.
{: .warning}

For the `zstd` and `zstd_no_dict` codecs, you can optionally specify a compression level in the `index.codec.compression_level` setting. This setting takes integers in the [1, 6] range. A higher compression level results in a higher compression ratio (smaller storage size) with a tradeoff in speed (slower compression and decompression speeds lead to greater indexing and search latencies). 

When an index segment is created, it uses the current index codec for compression. If you update the index codec, any segment created after the update will use the new compression algorithm. For specific operation considerations, see [Index codec considerations for index operations](#index-codec-considerations-for-index-operations).
{: .note}

As of OpenSearch 2.15, hardware-accelerated compression codecs for the `DEFLATE` and `LZ4` compression algorithms are available. These hardware-accelerated codecs are available on the latest 4th and 5th Gen Intel®️ Xeon®️ processors running Linux kernel 3.10 and later. For all other systems and platforms, the codecs use that platform's corresponding software implementations. 

The new hardware-accelerated codecs can be used by setting one of the following `index.codec` values:
* `qat_lz4` (OpenSearch 2.15 and later): Hardware-accelerated `LZ4`
* `qat_deflate` (OpenSearch 2.15 and later): Hardware-accelerated `DEFLATE`

`qat_deflate` offers a much better compression ratio than `qat_lz4`, with a modest drop in compression and decompression speed.
{: .note}

The `index.codec.compression_level` setting can be used to specify the compression level for both `qat_lz4` and `qat_deflate`. 

The `index.codec.qatmode` setting controls the behavior of the hardware accelerator and uses one of the following values:

* `auto`: If hardware acceleration fails, then the algorithm switches to software acceleration.
* `hardware`: Guarantees hardware-only compression. If hardware is not available, then an exception occurs until hardware exists.

For information about the `index.codec.qatmode` setting's effects on snapshots, see the [Snapshots](#snapshots) section.

For more information about hardware acceleration on Intel, see the [Intel (R) QAT accelerator overview](https://www.intel.com/content/www/us/en/developer/topic-technology/open/quick-assist-technology/overview.html).

## Choosing a codec 

The choice of index codec impacts the amount of disk space required to store the index data. Codecs like `best_compression`, `zstd`, and `zstd_no_dict` can achieve higher compression ratios, resulting in smaller index sizes. Conversely, the `default` codec doesn’t prioritize compression ratio, resulting in larger index sizes but faster search operations than `best_compression`.

## Index codec considerations for index operations

The following index codec considerations apply to various index operations.

### Writes

Every index consists of shards, each of which is further divided into Lucene segments. During index writes, the new segments are created based on the codec specified in the index settings. If you update the codec for an index, the new segments will use the new codec algorithm. 

### Merges

During segment merges, OpenSearch combines smaller index segments into larger segments in order to provide optimal resource utilization and improve performance. The index codec setting influences the speed and efficiency of the merge operations. The number of merges that happen on an index is a factor of the segment size, and a smaller segment size directly translates into smaller merge sizes. If you update the `index.codec` setting, the new merge operations will use the new codec when creating merged segments. The merged segments will have the compression characteristics of the new codec.

### Splits and shrinks

The [Split API]({{site.url}}{{site.baseurl}}/api-reference/index-apis/split/) splits an original index into a new index where each original primary shard is divided into two or more primary shards. The [Shrink API]({{site.url}}{{site.baseurl}}/api-reference/index-apis/shrink-index/) shrinks an existing index to a new index with a smaller number of primary shards. As part of split or shrink operations, any newly created segments will use the latest codec settings.

### Snapshots

When creating a [snapshot]({{site.url}}{{site.baseurl}}/tuning-your-cluster/availability-and-recovery/snapshots/index/), the index codec setting influences the size of the snapshot and the time required for its creation. If the codec of an index is updated, newly created snapshots will use the latest codec setting. The resulting snapshot size will reflect the compression characteristics of the latest codec setting. Existing segments included in the snapshot will retain their original compression characteristics. 

When you restore the indexes from a snapshot of a cluster to another cluster, it is important to verify that the target cluster supports the codecs of the segments in the source snapshot. For example, if the source snapshot contains segments of the `zstd` or `zstd_no_dict` codecs (introduced in OpenSearch 2.9), you won't be able to restore the snapshot to a cluster that runs on an older OpenSearch version because it doesn't support these codecs. 

For hardware-accelerated compression codecs, available in OpenSearch 2.15 and later, the value of `index.codec.qatmode` affects how snapshots and restores are performed. If the value is `auto` (the default), then snapshots and restores work without issue. However, if the value is `hardware`, then it must be reset to `auto` in order for the restore process to succeed on systems lacking the hardware accelerator.

You can modify the value of `index.codec.qatmode` during the restore process by setting its value as follows: `""index_settings"": {""index.codec.qatmode"": ""auto""}`.
{: .note}

### Reindexing

When you are performing a [reindex]({{site.url}}{{site.baseurl}}/im-plugin/reindex-data/) operation from a source index, the new segments created in the target index will have the properties of the codec settings of the target index. 

### Index rollups and transforms

When an index [rollup]({{site.url}}{{site.baseurl}}/im-plugin/index-rollups/) or [transform]({{site.url}}{{site.baseurl}}/im-plugin/index-transforms/) job is completed, the segments created in the target index will have the properties of the index codec specified during target index creation, irrespective of the source index codec. If the target index is created dynamically through a rollup job, the default codec is used for segments of the target index.

## Changing an index codec

It is not possible to change the codec setting of an open index. You can close the index, apply the new index codec setting, and reopen the index, at which point only new segments will be written with the new codec. This requires stopping all reads and writes to the index for a brief period to make the codec change and may result in inconsistent segment sizes and compression ratios. Alternatively, you can reindex all data from a source index into a new index with a different codec setting, though this is a very resource-intensive operation.

## Performance tuning and benchmarking

Depending on your specific use case, you might need to experiment with different index codec settings to fine-tune the performance of your OpenSearch cluster. Conducting benchmark tests with different codecs and measuring the impact on indexing speed, search performance, and resource utilization can help you identify the optimal index codec setting for your workload. With the `zstd` and `zstd_no_dict` codecs, you can also fine-tune the compression level in order to identify the optimal configuration for your cluster.

### Benchmarking

The following table provides a performance comparison of the `best_compression`, `zstd`, and `zstd_no_dict` codecs against the `default` codec. The tests were performed with the [`nyc_taxi`](https://github.com/topics/nyc-taxi-dataset) dataset. The results are listed in terms of percent change, and bold results indicate performance improvement.

| | `best_compression` | `zstd` | `zstd_no_dict` |
|:---	|:---	|:---	|:--- |
|**Write** | | | 
|Median Latency	|0%	|0%	|&minus;1%	|
|p90 Latency	|3%	|2%	|**&minus;5%**	|
|Throughput	|&minus;2%	|**7%**	|**14%**	|
|**Read**	| | | 
|Median Latency	|0%	|1%	|0%	|
|p90 Latency	|1%	|1%	|**&minus;2%**	|
|**Disk**	| | | 
| Compression ratio	|**&minus;34%**	|**&minus;35%**	|**&minus;30%**	|

","The `zstd` and `zstd_no_dict` codecs were introduced in OpenSearch 2.9. They are not available in OpenSearch versions prior to 2.9.

In OpenSearch 2.1, the supported index codecs are:

- `default` (LZ4 compression)
- `best_compression` (zlib compression)

To use the `zstd` or `zstd_no_dict` codecs, you need to upgrade your OpenSearch cluster to version 2.9 or later.",5.0,"Context Utilization: 1 - The response stays firmly grounded in the information provided in the context, demonstrating a thorough understanding of the relevant details. It does not introduce any external information or assumptions, which aligns well with the LLM evaluation.

Completeness: 1 - The answer comprehensively addresses the key issue of why the zstd compression is not working in OpenSearch 2.1, providing a clear and concise explanation based on the context. It covers all the necessary details, including the supported codecs in OpenSearch 2.1 and the introduction of the zstd codecs in later versions.

Conciseness: 1 - The response is succinct and to the point, conveying the required information without any unnecessary elaboration or tangents. The wording is efficient and focused, mirroring the LLM evaluation's assessment of the answer's conciseness.

Accuracy: 1 - The facts presented in the response align perfectly with the information provided in the context, demonstrating a precise understanding of the relevant details. There are no inaccuracies or inconsistencies, which is consistent with the LLM evaluation's assessment.

Clarity: 1 - The answer is easy to follow and understand, with a logical flow and clear explanations. The language used is straightforward and accessible, making the key points readily comprehensible, as noted in the LLM evaluation."
"I'm trying to set up this new aggregate view thing for saved objects in OpenSearch Dashboards, but I'm worried about messing up our existing multi-tenancy setup. The docs mention something about tenant indexes and a kibana_server role. How do I make sure I don't break anything when I turn this feature on? And what's the deal with not being able to turn it off once it's enabled","---
layout: default
title: Dynamic configuration in OpenSearch Dashboards
parent: OpenSearch Dashboards multi-tenancy
nav_order: 147
---


# Dynamic configuration in OpenSearch Dashboards

Multi-tenancy includes dynamic configuration options in OpenSearch Dashboards so you can manage common settings for tenancy without having to make changes to the configuration YAML files on each node and then restart the cluster. You can take advantage of this functionality by using the Dashboards interface or the REST API. The following list includes descriptions of the options currently covered by dynamic configuration:

- **Disable or enable multi-tenancy**: Administrators can disable and enable multi-tenancy dynamically. Disabling multi-tenancy does not pose a risk of data loss. If and when an administrator chooses to reenable tenancy, all previously saved objects are preserved and made available. The default is `multitenancy_enabled: true`.
  
  This setting does not have an impact on the global tenant, which always remains enabled.
  {: .note }

- **Disable or enable private tenant**: This option allows administrators to enable and disable private tenants. As with the enable multi-tenancy setting, when private tenants are reenabled all previously saved objects are preserved and made available.
- **Default tenant**: This option allows an administrator to choose either a global, private, or custom tenant as the default when users log in. In cases where a user doesn't have access to the default tenant (for example, if a custom tenant unavailable to the user was specified as the default), the default transitions to the preferred tenant, which is specified by the `opensearch_security.multitenancy.tenants.preferred` setting in the `opensearch-dashboards.yml` file. See [Multi-tenancy configuration]({{site.url}}{{site.baseurl}}/security/multi-tenancy/multi-tenancy-config/) for more information about this setting.

Depending on the specific changes made to multi-tenancy using dynamic configuration, some users may be logged out of their Dashboards session once the changes are saved. For example, if an admin user disables multi-tenancy, users with either a private or custom tenant as their selected tenant will be logged out and will need to log back in. Similarly, if an admin user disables private tenants, users with the private tenant selected will be logged out and will need to log back in. 

The global tenant, however, is a special case. Because this tenant is never disabled, users with the global tenant selected as their active tenant will experience no interruption to their session. Furthermore, changing the default tenant has no impact on a user's session.


## Configuring multi-tenancy in OpenSearch Dashboards

To configure multi-tenancy in Dashboards, follow these steps:

1. Begin by selecting **Security** in the Dashboards home page menu. Then select **Tenancy** from the Security menu on the left side of the screen. The **Multi-tenancy** page is displayed. 
1. By default, the **Manage** tab is displayed. Select the **Configure** tab to display the dynamic settings for multi-tenancy.
   * In the **Multi-tenancy** field, select the **Enable tenancy** check box to enable multi-tenancy. Clear the check box to disable the feature. The default is `true`.
   * In the **Tenants** field, you can enable or disable private tenants for users. By default the check box is selected and the feature is enabled.
   * In the **Default tenant** field, use the dropdown menu to select a default tenant. The menu includes Global, Private, and any other custom tenants that are available to users.
1. After making your preferred changes, select **Save changes** in the lower right corner of the window. A pop-up window appears listing the configuration items you've changed and asks you to review your changes.
1. Select the check boxes beside the items you want to confirm and then select **Apply changes**. The changes are implemented dynamically.


## Configuring multi-tenancy with the REST API

In addition to using the Dashboards interface, you can manage dynamic configurations using the REST API. 

### Get tenancy configuration

The GET call retrieves settings for the dynamic configuration:

```json
GET /_plugins/_security/api/tenancy/config
```
{% include copy-curl.html %}

#### Example response

```json
{
    ""mulitenancy_enabled"": true,
    ""private_tenant_enabled"": true,
    ""default_tenant"": ""global tenant""
}
```

### Update tenant configuration

The PUT call updates settings for dynamic configuration:

```json
PUT /_plugins/_security/api/tenancy/config
{
    ""default_tenant"": ""custom tenant 1"",
    ""private_tenant_enabled"": false,
    ""mulitenancy_enabled"": true
}
```
{% include copy-curl.html %}

### Example response

```json
{
    ""mulitenancy_enabled"": true,
    ""private_tenant_enabled"": false,
    ""default_tenant"": ""custom tenant 1""
}
```

### Dashboardsinfo API

You can also use the Dashboardsinfo API to retrieve the status of multi-tenancy settings for the user logged in to Dashboards:

```json
GET /_plugins/_security/dashboardsinfo
```
{% include copy-curl.html %}

### Example response

```json
{
  ""user_name"" : ""admin"",
  ""not_fail_on_forbidden_enabled"" : false,
  ""opensearch_dashboards_mt_enabled"" : true,
  ""opensearch_dashboards_index"" : "".kibana"",
  ""opensearch_dashboards_server_user"" : ""kibanaserver"",
  ""multitenancy_enabled"" : true,
  ""private_tenant_enabled"" : true,
  ""default_tenant"" : ""Private""
}
```

","Unfortunately, the provided context does not contain any information about setting up an aggregate view for saved objects or the implications of enabling that feature on an existing multi-tenancy setup in OpenSearch Dashboards. The context is focused on explaining the dynamic configuration options for multi-tenancy itself, such as enabling/disabling multi-tenancy, private tenants, and setting the default tenant.

Since I don't have enough context about the aggregate view feature you're referring to, I can't provide any specific guidance on how to enable it safely without disrupting your multi-tenancy configuration or what limitations there might be around disabling it once enabled. The documentation you're referring to is not included in the provided context.",3.0,"Context Utilization: 0 - The provided response does not utilize any of the context information about the OpenSearch Dashboards multi-tenancy setup and dynamic configuration options. The answer solely focuses on acknowledging the lack of relevant details in the given context, rather than attempting to provide any insights or guidance based on the available information.
Completeness: 0 - The response completely fails to address the original question, which was seeking advice on how to safely enable the aggregate view feature without disrupting the existing multi-tenancy setup. The answer does not offer any suggestions or recommendations, even within the limitations of the provided context.
Conciseness: 1 - The response is concise and efficiently communicates the inability to provide a meaningful answer due to the missing context, without any unnecessary elaboration.
Accuracy: 1 - The statements made about the lack of relevant information in the context are accurate and truthful.
Clarity: 1 - The response is clear in conveying that the necessary context is not available to address the specific question asked."
What software license does Opensearch use? ,"<img src=""https://opensearch.org/assets/img/opensearch-logo-themed.svg"" height=""64px"">

# About the OpenSearch documentation repo

The `documentation-website` repository contains the user documentation for OpenSearch. You can find the rendered documentation at [opensearch.org/docs](https://opensearch.org/docs).


## Contributing

Community contributions remain essential to keeping the documentation comprehensive, useful, well organized, and up to date. If you are interested in submitting an issue or contributing content, see [CONTRIBUTING](CONTRIBUTING.md). 

The following resources provide important guidance regarding contributions to the documentation:  

- [OpenSearch Project Style Guidelines](STYLE_GUIDE.md) -- The style guide covers the style standards to be observed when creating OpenSearch Project content.
- [OpenSearch terms](TERMS.md) -- The terms list contains key OpenSearch terms and tips on how and when to use them.  
- [API Style Guide](API_STYLE_GUIDE.md) -- The API Style Guide provides the basic structure for creating OpenSearch API documentation.
- [Formatting Guide](FORMATTING_GUIDE.md) -- The OpenSearch documentation uses a modified version of the [just-the-docs](https://github.com/pmarsceill/just-the-docs) Jekyll theme. The Formatting Guide provides an overview of the commonly used formatting elements and how to add a page to the website.


## Points of contact

If you encounter problems or have questions when contributing to the documentation, these people can help:

- [kolchfa-aws](https://github.com/kolchfa-aws)
- [Naarcha-AWS](https://github.com/Naarcha-AWS)
- [vagimeli](https://github.com/vagimeli)


## Code of conduct

This project has adopted an [Open Source Code of Conduct](https://opensearch.org/codeofconduct.html).


## Security

If you discover a potential security issue in this project, notify OpenSearch Security directly by emailing security@opensearch.org. To prevent any additional risk caused by the potential issue, do **not** create a public GitHub issue.

## License

This project is licensed under the [Apache 2.0 License](LICENSE).


## Copyright

Copyright OpenSearch contributors.
","According to the context provided, the OpenSearch documentation repository is licensed under the Apache 2.0 License.",4.0,"Context Utilization: 1 - The response effectively leverages the information provided in the context to directly answer the question. It does not introduce any external details beyond what is necessary to address the query.

Completeness: 1 - The answer concisely and accurately conveys the key piece of information needed to respond to the question - that OpenSearch uses the Apache 2.0 License. No important details are missing.

Conciseness: 0 - While the response is relatively short, it could be further streamlined by removing the additional context about the OpenSearch documentation repository. The core information about the license could be stated in a more succinct manner.

Accuracy: 1 - The statement that OpenSearch uses the Apache 2.0 License is fully consistent with the details provided in the context. The response accurately reflects the information available.

Clarity: 1 - The answer is straightforward and easy to understand. The language used is clear and unambiguous, effectively communicating the relevant information."
Does GPU accelerated nodes support Pytorch?,"---
layout: default
title: GPU acceleration
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 150
---


# GPU acceleration 

When running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training. 

## Supported GPUs

Currently, ML nodes support the following GPU instances:

- [NVIDIA instances with CUDA 11.6](https://aws.amazon.com/nvidia/)
- [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)

If you need GPU power, you can provision GPU instances through [Amazon Elastic Compute Cloud (Amazon EC2)](https://aws.amazon.com/ec2/). For more information on how to provision a GPU instance, see [Recommended GPU Instances](https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html).

## Supported images

You can use GPU acceleration with both [Docker images](https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md) with CUDA 11.6 and [Amazon Machine Images (AMIs)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html).

## PyTorch

GPU-accelerated ML nodes require [PyTorch](https://pytorch.org/docs/stable/index.html) 1.12.1 work with ML models.

## Setting up a GPU-accelerated ML node

Depending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts. 

### Preparing an NVIDIA ML node

NVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the `nvidia-uvm` kernel inside the `/dev` directory. To check for the kernel, enter `ls -al /dev | grep nvidia-uvm`.

If the `nvidia-uvm` kernel does not exist, run `nvidia-uvm-init.sh`:

```
#!/bin/bash
## Script to initialize nvidia device nodes.
## https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications
/sbin/modprobe nvidia
if [ ""$?"" -eq 0 ]; then
  # Count the number of NVIDIA controllers found.
  NVDEVS=`lspci | grep -i NVIDIA`
  N3D=`echo ""$NVDEVS"" | grep ""3D controller"" | wc -l`
  NVGA=`echo ""$NVDEVS"" | grep ""VGA compatible controller"" | wc -l`
  N=`expr $N3D + $NVGA - 1`
  for i in `seq 0 $N`; do
    mknod -m 666 /dev/nvidia$i c 195 $i
  done
  mknod -m 666 /dev/nvidiactl c 195 255
else
  exit 1
fi
/sbin/modprobe nvidia-uvm
if [ ""$?"" -eq 0 ]; then
  # Find out the major device number used by the nvidia-uvm driver
  D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`
  mknod -m 666 /dev/nvidia-uvm c $D 0
  mknod -m 666 /dev/nvidia-uvm-tools c $D 0
else
  exit 1
fi
```

After verifying that `nvidia-uvm` exists under `/dev`, you can start OpenSearch inside your cluster. 

### Preparing AWS Inferentia ML node

Depending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster. 

To start, [download and install OpenSearch]({{site.url}}{{site.baseurl}}/install-and-configure/index/) on your cluster.

Then export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory `opensearch-2.5.0`, so `OPENSEARCH_HOME` = `opensearch-2.5.0`:

```
echo ""export OPENSEARCH_HOME=~/opensearch-2.5.0"" | tee -a ~/.bash_profile
echo ""export PYTORCH_VERSION=1.12.1"" | tee -a ~/.bash_profile
source ~/.bash_profile
```

Next, create a shell script file called `prepare_torch_neuron.sh`. You can copy and customize one of the following examples based on your Linux operating system:

- [Ubuntu 20.04](#ubuntu-2004)
- [Amazon Linux 2](#amazon-linux-2)

After you've run the scripts, exit your current terminal and open a new terminal to start OpenSearch.

GPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems.
{: .note}

#### Ubuntu 20.04

```
. /etc/os-release
sudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF
deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main
EOF
wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -

# Update OS packages
sudo apt-get update -y

################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################

# Install OS headers
sudo apt-get install linux-headers-$(uname -r) -y

# Install Neuron Driver
sudo apt-get install aws-neuronx-dkms -y

####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################

# Install Neuron Tools
sudo apt-get install aws-neuronx-tools -y

######################################################
#   Only for Ubuntu 20 - Install Python3.7
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt-get install python3.7
######################################################
# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo apt-get install -y python3.7-venv g++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set pip repository to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

#Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf==3.20.1"" torchvision

# If you need to trace neuron model, install the transformers for tracing the Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile

# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

#### Amazon Linux 2

```
# Configure Linux for Neuron repository updates
sudo tee /etc/yum.repos.d/neuron.repo > /dev/null <<EOF
[neuron]
name=Neuron YUM Repository
baseurl=https://yum.repos.neuron.amazonaws.com
enabled=1
metadata_expire=0
EOF
sudo rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB
# Update OS packages
sudo yum update -y
################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################
# Install OS headers
sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y
# Install Neuron Driver
####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################
sudo yum install aws-neuronx-dkms -y
# Install Neuron Tools
sudo yum install aws-neuronx-tools -y

# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo yum install -y python3.7-venv gcc-c++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set Pip repository  to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

# Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf<4"" torchvision

# If you need to run the trace neuron model, install transformers for tracing Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile
# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

When the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch.

OpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually.

#### Prepare ML node manually

If the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually:

1. Deploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see [Deploy on AWS accelerator instance](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/setup/pytorch-install.html#deploy-on-aws-ml-accelerator-instance).

2. Copy the Neuron library into OpenSearch. The following command uses a directory named `opensearch-2.5.0`:

   ```
   OPENSEARCH_HOME=~/opensearch-2.5.0
   ```

3. Set the `PYTORCH_EXTRA_LIBRARY_PATH` path. In this example, we create a `pytorch` virtual environment in the OPENSEARCH_HOME folder:

   ```
   PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/


   mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r  $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
   export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
  ```

4. (Optional) To monitor the GPU usage of your accelerator instance, install [Neuron tools](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/index.html), which allows models to be used inside your instance:

   ```
   # Install Neuron Tools
   sudo apt-get install aws-neuronx-tools -y
   ```

   ```
   # Add Neuron tools your PATH
   export PATH=/opt/aws/neuron/bin:$PATH
   ```
  
   ```
   # Test Neuron tools
   neuron-top
   ```


5. To make sure you have enough memory to upload a model, increase the JVM stack size to `>+2MB`:

   ```
   echo ""-Xss2m"" | sudo tee -a $OPENSEARCH_HOME/config/jvm.options
   ```

6. Start OpenSearch. 

## Troubleshooting

Due to the amount of data required to work with ML models, you might encounter the following `max file descriptors` or `vm.max_map_count` errors when trying to run OpenSearch in a your cluster: 

```
[1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535]
[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
```

To troubleshoot the max file descriptors error, run the following command:

```
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
```

To fix the `vm.max_map_count` error, run this command to increase the count to `262114`:

```
sudo sysctl -w vm.max_map_count=262144
```

## Next steps

If you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see [Compiling and Deploying HuggingFace Pretrained BERT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html).

","Yes, according to the documentation, GPU-accelerated ML nodes in OpenSearch require PyTorch 1.12.1 to work with ML models.

Specifically, the documentation states:

""GPU-accelerated ML nodes require [PyTorch](https://pytorch.org/docs/stable/index.html) 1.12.1 work with ML models.""

So PyTorch support is a requirement for leveraging GPU acceleration on ML nodes in OpenSearch.",4.0,"Context Utilization: 1 - The response stays closely aligned with the context provided, drawing all its information directly from the given documentation. It does not introduce any external details or go beyond the scope of the provided material.

Completeness: 1 - The answer thoroughly addresses the key point of the question, clearly confirming that GPU-accelerated ML nodes in OpenSearch require PyTorch 1.12.1. It covers this central aspect comprehensively.

Conciseness: 0 - While the response is clear and well-structured, it could be slightly more concise. Some of the explanatory details, such as the specifics of the PyTorch version requirement, could potentially be shortened or consolidated to improve the overall flow and efficiency of the answer.

Accuracy: 1 - All the factual information presented in the response aligns precisely with the context. The details about the PyTorch version and GPU acceleration requirements are stated correctly.

Clarity: 1 - The answer is well-organized and easy to follow. The explanations are straightforward and the key points are communicated effectively. The writing style is natural and engaging."
Does opensearch support hugging face models? If so which ones? ,"---
layout: default
title: Pretrained models
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 120
---

# OpenSearch-provided pretrained models
**Introduced 2.9**
{: .label .label-purple }

OpenSearch provides a variety of open-source pretrained models that can assist with a range of machine learning (ML) search and analytics use cases. You can upload any supported model to the OpenSearch cluster and use it locally.

## Supported pretrained models

OpenSearch supports the following models, categorized by type. Text embedding models are sourced from [Hugging Face](https://huggingface.co/). Sparse encoding models are trained by OpenSearch. Although models with the same type will have similar use cases, each model has a different model size and will perform differently depending on your cluster setup. For a performance comparison of some pretrained models, see the [SBERT documentation](https://www.sbert.net/docs/pretrained_models.html#model-overview).

Running local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.
{: .important}

### Sentence transformers

Sentence transformer models map sentences and paragraphs across a dimensional dense vector space. The number of vectors depends on the type of model. You can use these models for use cases such as clustering or semantic search.

The following table provides a list of sentence transformer models and artifact links you can use to download them. Note that you must prefix the model name with `huggingface/`, as shown in the **Model name** column. 

| Model name | Version | Vector dimensions | Auto-truncation | TorchScript artifact | ONNX artifact |
|:---|:---|:---|:---|:---|:---|
| `huggingface/sentence-transformers/all-distilroberta-v1` | 1.0.1 | 768-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/torch_script/sentence-transformers_all-distilroberta-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/onnx/sentence-transformers_all-distilroberta-v1-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-distilroberta-v1/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/all-MiniLM-L6-v2` | 1.0.1 | 384-dimensional dense vector space.  | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/torch_script/sentence-transformers_all-MiniLM-L6-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/onnx/sentence-transformers_all-MiniLM-L6-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L6-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/all-MiniLM-L12-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/torch_script/sentence-transformers_all-MiniLM-L12-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/onnx/sentence-transformers_all-MiniLM-L12-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-MiniLM-L12-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/all-mpnet-base-v2` | 1.0.1 | 768-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/torch_script/sentence-transformers_all-mpnet-base-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/onnx/sentence-transformers_all-mpnet-base-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/all-mpnet-base-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/msmarco-distilbert-base-tas-b` | 1.0.2 | 768-dimensional dense vector space. Optimized for semantic search. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/torch_script/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.2-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/onnx/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.2-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.2/onnx/config.json) |
| `huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1` | 1.0.1 | 384-dimensional dense vector space. Designed for semantic search and trained on 215 million question/answer pairs. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/torch_script/sentence-transformers_multi-qa-MiniLM-L6-cos-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/onnx/sentence-transformers_multi-qa-MiniLM-L6-cos-v1-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/torch_script/sentence-transformers_multi-qa-mpnet-base-dot-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/onnx/sentence-transformers_multi-qa-mpnet-base-dot-v1-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/multi-qa-mpnet-base-dot-v1/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/torch_script/sentence-transformers_paraphrase-MiniLM-L3-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/onnx/sentence-transformers_paraphrase-MiniLM-L3-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-MiniLM-L3-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | 1.0.1 | 384-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/torch_script/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/onnx/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2-1.0.1-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/1.0.1/onnx/config.json) |
| `huggingface/sentence-transformers/paraphrase-mpnet-base-v2` | 1.0.0 | 768-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/torch_script/sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/onnx/sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/paraphrase-mpnet-base-v2/1.0.0/onnx/config.json) |
| `huggingface/sentence-transformers/distiluse-base-multilingual-cased-v1` | 1.0.1 | 512-dimensional dense vector space. | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/distiluse-base-multilingual-cased-v1/1.0.1/torch_script/sentence-transformers_distiluse-base-multilingual-cased-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/distiluse-base-multilingual-cased-v1/1.0.1/torch_script/config.json) | Not available |


### Sparse encoding models
**Introduced 2.11**
{: .label .label-purple }

Sparse encoding models transfer text into a sparse vector and convert the vector to a list of `<token: weight>` pairs representing the text entry and its corresponding weight in the sparse vector. You can use these models for use cases such as clustering or sparse neural search.

We recommend the following combinations for optimal performance:

- Use the `amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill` model during both ingestion and search.
- Use the `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill` model during ingestion and the
`amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1` tokenizer during search.

For more information about the preceding options for running neural sparse search, see [Generating sparse vector embeddings within OpenSearch]({{site.url}}{{site.baseurl}}/search-plugins/neural-sparse-with-pipelines/).

The following table provides a list of sparse encoding models and artifact links you can use to download them.

| Model name | Version | Auto-truncation | TorchScript artifact | Description |
|:---|:---|:---|:---|:---|
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-v1` | 1.0.1 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v1/1.0.1/torch_script/neural-sparse_opensearch-neural-sparse-encoding-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v1/1.0.1/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v1). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill` | 1.0.0 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill/1.0.0/torch_script/neural-sparse_opensearch-neural-sparse-encoding-v2-distill-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v2-distill/1.0.0/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v2-distill). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v1` | 1.0.1 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v1/1.0.1/torch_script/neural-sparse_opensearch-neural-sparse-encoding-doc-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v1/1.0.1/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v1). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill` | 1.0.0 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill/1.0.0/torch_script/neural-sparse_opensearch-neural-sparse-encoding-doc-v2-distill-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-distill/1.0.0/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill). |
| `amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-mini` | 1.0.0 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-mini/1.0.0/torch_script/neural-sparse_opensearch-neural-sparse-encoding-doc-v2-mini-1.0.0-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v2-mini/1.0.0/torch_script/config.json) | A neural sparse encoding model. The model transforms text into a sparse vector, identifies the indices of non-zero elements in the vector, and then converts the vector into `<entry, weight>` pairs, where each entry corresponds to a non-zero element index. To experiment with this model using transformers and the PyTorch API, see the [Hugging Face documentation](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini). |
| `amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1` | 1.0.1 | Yes | - [model_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1/1.0.1/torch_script/neural-sparse_opensearch-neural-sparse-tokenizer-v1-1.0.1-torch_script.zip)<br>- [config_url](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1/1.0.1/torch_script/config.json) | A neural sparse tokenizer. The tokenizer splits text into tokens and assigns each token a predefined weight, which is the token's inverse document frequency (IDF). If the IDF file is not provided, the weight defaults to 1. For more information, see [Preparing a model]({{site.url}}{{site.baseurl}}/ml-commons-plugin/custom-local-models/#preparing-a-model). |

### Cross-encoder models
**Introduced 2.12**
{: .label .label-purple }

Cross-encoder models support query reranking. 

The following table provides a list of cross-encoder models and artifact links you can use to download them. Note that you must prefix the model name with `huggingface/cross-encoders`, as shown in the **Model name** column. 

| Model name | Version | TorchScript artifact | ONNX artifact |
|:---|:---|:---|:---|
| `huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2` | 1.0.2 | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/torch_script/cross-encoders_ms-marco-MiniLM-L-6-v2-1.0.2-torch_script.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/onnx/cross-encoders_ms-marco-MiniLM-L-6-v2-1.0.2-onnx.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2/1.0.2/onnx/config.json) |
| `huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2` | 1.0.2 | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/torch_script/cross-encoders_ms-marco-MiniLM-L-12-v2-1.0.2-torch_script.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/torch_script/config.json) | - [model_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/onnx/cross-encoders_ms-marco-MiniLM-L-12-v2-1.0.2-onnx.zip) <br>- [config_url](https://artifacts.opensearch.org/models/ml-models/huggingface/cross-encoders/ms-marco-MiniLM-L-12-v2/1.0.2/onnx/config.json) 

## Prerequisites

On clusters with dedicated ML nodes, specify `""only_run_on_ml_node"": ""true""` for improved performance. For more information, see [ML Commons cluster settings]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/). 

This example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. To ensure that this basic local setup works, specify the following cluster settings:

```json
PUT _cluster/settings
{
  ""persistent"": {
    ""plugins"": {
      ""ml_commons"": {
        ""only_run_on_ml_node"": ""false"",
        ""model_access_control_enabled"": ""true"",
        ""native_memory_threshold"": ""99""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Step 1: Register a model group

To register a model, you have the following options:

- You can use `model_group_id` to register a model version to an existing model group.
- If you do not use `model_group_id`, ML Commons creates a model with a new model group.

To register a model group, send the following request:

```json
POST /_plugins/_ml/model_groups/_register
{
  ""name"": ""local_model_group"",
  ""description"": ""A model group for local models""
}
```
{% include copy-curl.html %}

The response contains the model group ID that you'll use to register a model to this model group:

```json
{
 ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
 ""status"": ""CREATED""
}
```

To learn more about model groups, see [Model access control]({{site.url}}{{site.baseurl}}/ml-commons-plugin/model-access-control/).

## Step 2: Register a local OpenSearch-provided model

To register an OpenSearch-provided model to the model group created in step 1, provide the model group ID from step 1 in the following request.

Because pretrained models originate from the ML Commons model repository, you only need to provide the `name`, `version`, `model_group_id`, and `model_format` in the register API request:  

```json
POST /_plugins/_ml/models/_register
{
  ""name"": ""huggingface/sentence-transformers/msmarco-distilbert-base-tas-b"",
  ""version"": ""1.0.2"",
  ""model_group_id"": ""Z1eQf4oB5Vm0Tdw8EIP2"",
  ""model_format"": ""TORCH_SCRIPT""
}
```
{% include copy-curl.html %}

OpenSearch returns the task ID of the register operation:

```json
{
  ""task_id"": ""cVeMb4kBJ1eYAeTMFFgj"",
  ""status"": ""CREATED""
}
```

To check the status of the operation, provide the task ID to the [Tasks API]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/tasks-apis/get-task/):

```bash
GET /_plugins/_ml/tasks/cVeMb4kBJ1eYAeTMFFgj
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""REGISTER_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""XPcXLV7RQoi5m8NI_jEOVQ""
  ],
  ""create_time"": 1689793598499,
  ""last_update_time"": 1689793598530,
  ""is_async"": false
}
```

Take note of the returned `model_id` because you’ll need it to deploy the model.

## Step 3: Deploy the model

The deploy operation reads the model's chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into and longer it takes for the model to load into memory.

To deploy the registered model, provide its model ID from step 3 in the following request:

```bash
POST /_plugins/_ml/models/cleMb4kBJ1eYAeTMFFg4/_deploy
```
{% include copy-curl.html %}

The response contains the task ID that you can use to check the status of the deploy operation:

```json
{
  ""task_id"": ""vVePb4kBJ1eYAeTM7ljG"",
  ""status"": ""CREATED""
}
```

As in the previous step, check the status of the operation by calling the Tasks API:

```bash
GET /_plugins/_ml/tasks/vVePb4kBJ1eYAeTM7ljG
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""DEPLOY_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""n-72khvBTBi3bnIIR8FTTw""
  ],
  ""create_time"": 1689793851077,
  ""last_update_time"": 1689793851101,
  ""is_async"": true
}
```

If a cluster or node is restarted, then you need to redeploy the model. To learn how to set up automatic redeployment, see [Enable auto redeploy]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/#enable-auto-redeploy).
{: .tip} 

## Step 4 (Optional): Test the model

Use the [Predict API]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/train-predict/predict/) to test the model.

### Text embedding model

For a text embedding model, send the following request:

```json
POST /_plugins/_ml/_predict/text_embedding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""],
  ""return_number"": true,
  ""target_response"": [""sentence_embedding""]
}
```
{% include copy-curl.html %}

The response contains text embeddings for the provided sentence:

```json
{
  ""inference_results"" : [
    {
      ""output"" : [
        {
          ""name"" : ""sentence_embedding"",
          ""data_type"" : ""FLOAT32"",
          ""shape"" : [
            768
          ],
          ""data"" : [
            0.25517133,
            -0.28009856,
            0.48519906,
            ...
          ]
        }
      ]
    }
  ]
}
```

### Sparse encoding model

For a sparse encoding model, send the following request:

```json
POST /_plugins/_ml/_predict/sparse_encoding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""]
}
```
{% include copy-curl.html %}

The response contains the tokens and weights:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""name"": ""output"",
          ""dataAsMap"": {
            ""response"": [
              {
                ""saturday"": 0.48336542,
                ""week"": 0.1034762,
                ""mood"": 0.09698499,
                ""sunshine"": 0.5738209,
                ""bright"": 0.1756877,
                ...
              }
          }
        }
    }
}
```

### Cross-encoder model

For a cross-encoder model, send the following request:

```json
POST _plugins/_ml/models/<model_id>/_predict
{
    ""query_text"": ""today is sunny"",
    ""text_docs"": [
        ""how are you"",
        ""today is sunny"",
        ""today is july fifth"",
        ""it is winter""
    ]
}
```
{% include copy-curl.html %}

The model calculates the similarity score of `query_text` and each document in `text_docs` and returns a list of scores for each document in the order they were provided in `text_docs`:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            -6.077798
          ],
          ""byte_buffer"": {
            ""array"": ""Un3CwA=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    },
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            10.223609
          ],
          ""byte_buffer"": {
            ""array"": ""55MjQQ=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    },
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            -1.3987057
          ],
          ""byte_buffer"": {
            ""array"": ""ygizvw=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    },
    {
      ""output"": [
        {
          ""name"": ""similarity"",
          ""data_type"": ""FLOAT32"",
          ""shape"": [
            1
          ],
          ""data"": [
            -4.5923924
          ],
          ""byte_buffer"": {
            ""array"": ""4fSSwA=="",
            ""order"": ""LITTLE_ENDIAN""
          }
        }
      ]
    }
  ]
}
```

A higher document score means higher similarity. In the preceding response, documents are scored as follows against the query text `today is sunny`:

Document text | Score
:--- | :---
`how are you` | -6.077798
`today is sunny` | 10.223609
`today is july fifth` | -1.3987057
`it is winter` | -4.5923924

The document that contains the same text as the query is scored the highest, and the remaining documents are scored based on the text similarity.

## Step 5: Use the model for search

To learn how to set up a vector index and use text embedding models for search, see [Semantic search]({{site.url}}{{site.baseurl}}/search-plugins/semantic-search/).

To learn how to set up a vector index and use sparse encoding models for search, see [Neural sparse search]({{site.url}}{{site.baseurl}}/search-plugins/neural-sparse-search/).

To learn how to use cross-encoder models for reranking, see [Reranking search results]({{site.url}}{{site.baseurl}}/search-plugins/search-relevance/reranking-search-results/).

","Yes, OpenSearch supports various Hugging Face models for machine learning tasks. According to the OpenSearch documentation, the following Hugging Face models are supported:

1. **Sentence Transformer Models**:
   - These models map sentences and paragraphs into dense vector representations. OpenSearch supports several sentence transformer models from Hugging Face, including:
     - all-distilroberta-v1
     - all-MiniLM-L6-v2
     - all-MiniLM-L12-v2
     - all-mpnet-base-v2
     - msmarco-distilbert-base-tas-b
     - multi-qa-MiniLM-L6-cos-v1
     - multi-qa-mpnet-base-dot-v1
     - paraphrase-MiniLM-L3-v2
     - paraphrase-multilingual-MiniLM-L12-v2
     - paraphrase-mpnet-base-v2
     - distiluse-base-multilingual-cased-v1

2. **Cross-Encoder Models**:
   - These models support query reranking. OpenSearch supports the following cross-encoder models from Hugging Face:
     - ms-marco-MiniLM-L-6-v2
     - ms-marco-MiniLM-L-12-v2

The documentation provides artifact links and configuration details for downloading and using these Hugging Face models within OpenSearch.

It's important to note that the supported models may change with new OpenSearch releases, so it's always a good idea to check the official documentation for the latest information.",3.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, drawing upon the relevant details without introducing any extraneous or unsubstantiated claims.

Completeness: 1 - The answer thoroughly addresses the key aspects of the question, providing a comprehensive overview of the Hugging Face models supported by OpenSearch, including the different categories of models and their specific details.

Conciseness: 0 - While the response covers the necessary information, it could be more concise in its presentation. The extensive listing of model details and artifact links, while informative, makes the answer somewhat lengthy and potentially overwhelming for the reader.

Accuracy: 1 - The information provided in the answer aligns precisely with the facts and details outlined in the given context, demonstrating a high level of accuracy.

Clarity: 1 - The response is well-structured and easy to follow, with clear explanations and a logical flow of information. The categorization of the models and the accompanying details help the reader understand the capabilities and use cases of the supported Hugging Face models."
"I have a custom model, can I run it in Opensearch?","---
layout: default
title: Custom models
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 120
---

# Custom local models
**Introduced 2.9**
{: .label .label-purple }

To use a custom model locally, you can upload it to the OpenSearch cluster.

## Model support

As of OpenSearch 2.6, OpenSearch supports local text embedding models.

As of OpenSearch 2.11, OpenSearch supports local sparse encoding models.

As of OpenSearch 2.12, OpenSearch supports local cross-encoder models.

As of OpenSearch 2.13, OpenSearch supports local question answering models.

Running local models on the CentOS 7 operating system is not supported. Moreover, not all local models can run on all hardware and operating systems.
{: .important}

## Preparing a model

For all the models, you must provide a tokenizer JSON file within the model zip file.

For sparse encoding models, make sure your output format is `{""output"":<sparse_vector>}` so that ML Commons can post-process the sparse vector.

If you fine-tune a sparse model on your own dataset, you may also want to use your own sparse tokenizer model. It is preferable to provide your own [IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) JSON file in the tokenizer model zip file because this increases query performance when you use the tokenizer model in the query. Alternatively, you can use an OpenSearch-provided generic [IDF from MSMARCO](https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1/1.0.0/torch_script/opensearch-neural-sparse-tokenizer-v1-1.0.0.zip). If the IDF file is not provided, the default weight of each token is set to 1, which may influence sparse neural search performance.  

### Model format

To use a model in OpenSearch, you'll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the [TorchScript](https://pytorch.org/docs/stable/jit.html) and [ONNX](https://onnx.ai/) formats.

You must save the model file as zip before uploading it to OpenSearch. To ensure that ML Commons can upload your model, compress your TorchScript file before uploading. For an example, download a TorchScript [model file](https://github.com/opensearch-project/ml-commons/blob/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip).

Additionally, you must calculate a SHA256 checksum for the model zip file that you'll need to provide when registering the model. For example, on UNIX, use the following command to obtain the checksum:

```bash
shasum -a 256 sentence-transformers_paraphrase-mpnet-base-v2-1.0.0-onnx.zip
```

### Model size

Most deep learning models are more than 100 MB, making it difficult to fit them into a single document. OpenSearch splits the model file into smaller chunks to be stored in a model index. When allocating ML or data nodes for your OpenSearch cluster, make sure you correctly size your ML nodes so that you have enough memory when making ML inferences.

## Prerequisites 

To upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from [Hugging Face](https://huggingface.co/), or train a new model in accordance with your needs.

### Cluster settings

This example uses a simple setup with no dedicated ML nodes and allows running a model on a non-ML node. 

On clusters with dedicated ML nodes, specify `""only_run_on_ml_node"": ""true""` for improved performance. For more information, see [ML Commons cluster settings]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/).

To ensure that this basic local setup works, specify the following cluster settings:

```json
PUT _cluster/settings
{
  ""persistent"": {
    ""plugins"": {
      ""ml_commons"": {
        ""allow_registering_model_via_url"": ""true"",
        ""only_run_on_ml_node"": ""false"",
        ""model_access_control_enabled"": ""true"",
        ""native_memory_threshold"": ""99""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Step 1: Register a model group

To register a model, you have the following options:

- You can use `model_group_id` to register a model version to an existing model group.
- If you do not use `model_group_id`, ML Commons creates a model with a new model group.

To register a model group, send the following request:

```json
POST /_plugins/_ml/model_groups/_register
{
  ""name"": ""local_model_group"",
  ""description"": ""A model group for local models""
}
```
{% include copy-curl.html %}

The response contains the model group ID that you'll use to register a model to this model group:

```json
{
 ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
 ""status"": ""CREATED""
}
```

To learn more about model groups, see [Model access control]({{site.url}}{{site.baseurl}}/ml-commons-plugin/model-access-control/).

## Step 2: Register a local model

To register a local model to the model group created in step 1, send a Register Model API request. For descriptions of Register Model API parameters, see [Register a model]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/model-apis/register-model/). 

The `function_name` corresponds to the model type. For text embedding models, set this parameter to `TEXT_EMBEDDING`. For sparse encoding models, set this parameter to `SPARSE_ENCODING` or `SPARSE_TOKENIZE`. For cross-encoder models, set this parameter to `TEXT_SIMILARITY`. For question answering models, set this parameter to `QUESTION_ANSWERING`. In this example, set `function_name` to `TEXT_EMBEDDING` because you're registering a text embedding model. 

Provide the model group ID from step 1 and send the following request:

```json
POST /_plugins/_ml/models/_register
{
  ""name"": ""huggingface/sentence-transformers/msmarco-distilbert-base-tas-b"",
  ""version"": ""1.0.1"",
  ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
  ""description"": ""This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search."",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""model_format"": ""TORCH_SCRIPT"",
  ""model_content_size_in_bytes"": 266352827,
  ""model_content_hash_value"": ""acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413"",
  ""model_config"": {
    ""model_type"": ""distilbert"",
    ""embedding_dimension"": 768,
    ""framework_type"": ""sentence_transformers"",
    ""all_config"": ""{\\""_name_or_path\\"":\\""old_models/msmarco-distilbert-base-tas-b/0_Transformer\\"",\\""activation\\"":\\""gelu\\"",\\""architectures\\"":[\\""DistilBertModel\\""],\\""attention_dropout\\"":0.1,\\""dim\\"":768,\\""dropout\\"":0.1,\\""hidden_dim\\"":3072,\\""initializer_range\\"":0.02,\\""max_position_embeddings\\"":512,\\""model_type\\"":\\""distilbert\\"",\\""n_heads\\"":12,\\""n_layers\\"":6,\\""pad_token_id\\"":0,\\""qa_dropout\\"":0.1,\\""seq_classif_dropout\\"":0.2,\\""sinusoidal_pos_embds\\"":false,\\""tie_weights_\\"":true,\\""transformers_version\\"":\\""4.7.0\\"",\\""vocab_size\\"":30522}""
  },
  ""created_time"": 1676073973126,
  ""url"": ""https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.1/torch_script/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip""
}
```
{% include copy-curl.html %}

Note that in OpenSearch Dashboards, wrapping the `all_config` field contents in triple quotes (`""""""`) automatically escapes quotation marks within the field and provides better readability:

```json
POST /_plugins/_ml/models/_register
{
  ""name"": ""huggingface/sentence-transformers/msmarco-distilbert-base-tas-b"",
  ""version"": ""1.0.1"",
  ""model_group_id"": ""wlcnb4kBJ1eYAeTMHlV6"",
  ""description"": ""This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search."",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""model_format"": ""TORCH_SCRIPT"",
  ""model_content_size_in_bytes"": 266352827,
  ""model_content_hash_value"": ""acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413"",
  ""model_config"": {
    ""model_type"": ""distilbert"",
    ""embedding_dimension"": 768,
    ""framework_type"": ""sentence_transformers"",
    ""all_config"": """"""{""_name_or_path"":""old_models/msmarco-distilbert-base-tas-b/0_Transformer"",""activation"":""gelu"",""architectures"":[""DistilBertModel""],""attention_dropout"":0.1,""dim"":768,""dropout"":0.1,""hidden_dim"":3072,""initializer_range"":0.02,""max_position_embeddings"":512,""model_type"":""distilbert"",""n_heads"":12,""n_layers"":6,""pad_token_id"":0,""qa_dropout"":0.1,""seq_classif_dropout"":0.2,""sinusoidal_pos_embds"":false,""tie_weights_"":true,""transformers_version"":""4.7.0"",""vocab_size"":30522}""""""
  },
  ""created_time"": 1676073973126,
  ""url"": ""https://artifacts.opensearch.org/models/ml-models/huggingface/sentence-transformers/msmarco-distilbert-base-tas-b/1.0.1/torch_script/sentence-transformers_msmarco-distilbert-base-tas-b-1.0.1-torch_script.zip""
}
```
{% include copy.html %}

OpenSearch returns the task ID of the register operation:

```json
{
  ""task_id"": ""cVeMb4kBJ1eYAeTMFFgj"",
  ""status"": ""CREATED""
}
```

To check the status of the operation, provide the task ID to the [Get task]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/tasks-apis/get-task/):

```bash
GET /_plugins/_ml/tasks/cVeMb4kBJ1eYAeTMFFgj
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""REGISTER_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""XPcXLV7RQoi5m8NI_jEOVQ""
  ],
  ""create_time"": 1689793598499,
  ""last_update_time"": 1689793598530,
  ""is_async"": false
}
```

Take note of the returned `model_id` because you’ll need it to deploy the model.

## Step 3: Deploy the model

The deploy operation reads the model's chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into and longer it takes for the model to load into memory.

To deploy the registered model, provide its model ID from step 3 in the following request:

```bash
POST /_plugins/_ml/models/cleMb4kBJ1eYAeTMFFg4/_deploy
```
{% include copy-curl.html %}

The response contains the task ID that you can use to check the status of the deploy operation:

```json
{
  ""task_id"": ""vVePb4kBJ1eYAeTM7ljG"",
  ""status"": ""CREATED""
}
```

As in the previous step, check the status of the operation by calling the Tasks API:

```bash
GET /_plugins/_ml/tasks/vVePb4kBJ1eYAeTM7ljG
```
{% include copy-curl.html %}

When the operation is complete, the state changes to `COMPLETED`:

```json
{
  ""model_id"": ""cleMb4kBJ1eYAeTMFFg4"",
  ""task_type"": ""DEPLOY_MODEL"",
  ""function_name"": ""TEXT_EMBEDDING"",
  ""state"": ""COMPLETED"",
  ""worker_node"": [
    ""n-72khvBTBi3bnIIR8FTTw""
  ],
  ""create_time"": 1689793851077,
  ""last_update_time"": 1689793851101,
  ""is_async"": true
}
```

If a cluster or node is restarted, then you need to redeploy the model. To learn how to set up automatic redeployment, see [Enable auto redeploy]({{site.url}}{{site.baseurl}}/ml-commons-plugin/cluster-settings/#enable-auto-redeploy).
{: .tip} 

## Step 4 (Optional): Test the model

Use the [Predict API]({{site.url}}{{site.baseurl}}/ml-commons-plugin/api/train-predict/predict/) to test the model.

For a text embedding model, send the following request:

```json
POST /_plugins/_ml/_predict/text_embedding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""],
  ""return_number"": true,
  ""target_response"": [""sentence_embedding""]
}
```
{% include copy-curl.html %}

The response contains text embeddings for the provided sentence:

```json
{
  ""inference_results"" : [
    {
      ""output"" : [
        {
          ""name"" : ""sentence_embedding"",
          ""data_type"" : ""FLOAT32"",
          ""shape"" : [
            768
          ],
          ""data"" : [
            0.25517133,
            -0.28009856,
            0.48519906,
            ...
          ]
        }
      ]
    }
  ]
}
```

For a sparse encoding model, send the following request:

```json
POST /_plugins/_ml/_predict/sparse_encoding/cleMb4kBJ1eYAeTMFFg4
{
  ""text_docs"":[ ""today is sunny""]
}
```
{% include copy-curl.html %}

The response contains the tokens and weights:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""name"": ""output"",
          ""dataAsMap"": {
            ""response"": [
              {
                ""saturday"": 0.48336542,
                ""week"": 0.1034762,
                ""mood"": 0.09698499,
                ""sunshine"": 0.5738209,
                ""bright"": 0.1756877,
                ...
              }
          }
        }
    }
}
```

## Step 5: Use the model for search

To learn how to use the model for vector search, see [Using an ML model for neural search]({{site.url}}{{site.baseurl}}/search-plugins/neural-search/#using-an-ml-model-for-neural-search).

## Question answering models

A question answering model extracts the answer to a question from a given context. ML Commons supports context in `text` format.

To register a question answering model, send a request in the following format. Specify the `function_name` as `QUESTION_ANSWERING`:

```json
POST /_plugins/_ml/models/_register
{
    ""name"": ""question_answering"",
    ""version"": ""1.0.0"",
    ""function_name"": ""QUESTION_ANSWERING"",
    ""description"": ""test model"",
    ""model_format"": ""TORCH_SCRIPT"",
    ""model_group_id"": ""lN4AP40BKolAMNtR4KJ5"",
    ""model_content_hash_value"": ""e837c8fc05fd58a6e2e8383b319257f9c3859dfb3edc89b26badfaf8a4405ff6"",
    ""model_config"": { 
        ""model_type"": ""bert"",
        ""framework_type"": ""huggingface_transformers""
    },
    ""url"": ""https://github.com/opensearch-project/ml-commons/blob/main/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/question_answering/question_answering_pt.zip?raw=true""
}
```
{% include copy-curl.html %}

Then send a request to deploy the model:

```json
POST _plugins/_ml/models/<model_id>/_deploy
```
{% include copy-curl.html %}

To test a question answering model, send the following request. It requires a `question` and the relevant `context` from which the answer will be generated:

```json
POST /_plugins/_ml/_predict/question_answering/<model_id>
{
  ""question"": ""Where do I live?""
  ""context"": ""My name is John. I live in New York""
}
```
{% include copy-curl.html %}

The response provides the answer based on the context:

```json
{
  ""inference_results"": [
    {
      ""output"": [
        {
          ""result"": ""New York""
        }
    }
}
```
","Yes, OpenSearch supports running custom local models that you can upload to the cluster. According to the documentation, as of different OpenSearch versions, it supports the following types of custom local models:

- As of OpenSearch 2.6: Local text embedding models
- As of OpenSearch 2.11: Local sparse encoding models  
- As of OpenSearch 2.12: Local cross-encoder models
- As of OpenSearch 2.13: Local question answering models

The documentation provides steps on how to prepare, register, and deploy a custom model to run it within OpenSearch. Some key requirements are:

1. The model must be in a supported format like TorchScript or ONNX.
2. You need to package the model as a zip file and calculate its SHA256 checksum.
3. Register the model by providing details like name, version, model format, config, etc. 
4. Deploy the registered model to load it into memory.

Once deployed, you can use the model for tasks like text embedding, semantic search, question answering etc. by calling the appropriate prediction APIs.

So in summary, yes OpenSearch does allow running custom trained models locally within the cluster, as long as you follow the specified steps to register and deploy the model correctly.",3.0,"Context Utilization: 1 - The response stays focused on the key information provided in the context, addressing the specific question of whether custom models can be run in OpenSearch. It does not introduce any extraneous details or tangents.

Completeness: 1 - The answer thoroughly covers the main points, explaining the different types of custom models supported by OpenSearch, the steps required to register and deploy a model, and how to use the model for tasks like text embedding and question answering. It provides a comprehensive overview of the relevant functionality.

Conciseness: 0 - While the response is informative, it could be more concise. There are some sections, such as the detailed steps for registering and deploying a model, that feel a bit lengthy and could potentially be condensed or summarized more efficiently.

Accuracy: 1 - All the information presented in the answer is accurate and aligned with the details provided in the context. The technical details and steps described match the documentation.

Clarity: 1 - The response is generally clear and easy to follow, with a good balance of technical details and explanations. However, the use of some specialized terms and jargon may make it slightly less accessible to a non-technical audience."
"I have a model and some ML nodes, how do I boost it's performance?","---
layout: default
title: GPU acceleration
parent: Using ML models within OpenSearch
grand_parent: Integrating ML models
nav_order: 150
---


# GPU acceleration 

When running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training. 

## Supported GPUs

Currently, ML nodes support the following GPU instances:

- [NVIDIA instances with CUDA 11.6](https://aws.amazon.com/nvidia/)
- [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)

If you need GPU power, you can provision GPU instances through [Amazon Elastic Compute Cloud (Amazon EC2)](https://aws.amazon.com/ec2/). For more information on how to provision a GPU instance, see [Recommended GPU Instances](https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html).

## Supported images

You can use GPU acceleration with both [Docker images](https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md) with CUDA 11.6 and [Amazon Machine Images (AMIs)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html).

## PyTorch

GPU-accelerated ML nodes require [PyTorch](https://pytorch.org/docs/stable/index.html) 1.12.1 work with ML models.

## Setting up a GPU-accelerated ML node

Depending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts. 

### Preparing an NVIDIA ML node

NVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the `nvidia-uvm` kernel inside the `/dev` directory. To check for the kernel, enter `ls -al /dev | grep nvidia-uvm`.

If the `nvidia-uvm` kernel does not exist, run `nvidia-uvm-init.sh`:

```
#!/bin/bash
## Script to initialize nvidia device nodes.
## https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications
/sbin/modprobe nvidia
if [ ""$?"" -eq 0 ]; then
  # Count the number of NVIDIA controllers found.
  NVDEVS=`lspci | grep -i NVIDIA`
  N3D=`echo ""$NVDEVS"" | grep ""3D controller"" | wc -l`
  NVGA=`echo ""$NVDEVS"" | grep ""VGA compatible controller"" | wc -l`
  N=`expr $N3D + $NVGA - 1`
  for i in `seq 0 $N`; do
    mknod -m 666 /dev/nvidia$i c 195 $i
  done
  mknod -m 666 /dev/nvidiactl c 195 255
else
  exit 1
fi
/sbin/modprobe nvidia-uvm
if [ ""$?"" -eq 0 ]; then
  # Find out the major device number used by the nvidia-uvm driver
  D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`
  mknod -m 666 /dev/nvidia-uvm c $D 0
  mknod -m 666 /dev/nvidia-uvm-tools c $D 0
else
  exit 1
fi
```

After verifying that `nvidia-uvm` exists under `/dev`, you can start OpenSearch inside your cluster. 

### Preparing AWS Inferentia ML node

Depending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster. 

To start, [download and install OpenSearch]({{site.url}}{{site.baseurl}}/install-and-configure/index/) on your cluster.

Then export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory `opensearch-2.5.0`, so `OPENSEARCH_HOME` = `opensearch-2.5.0`:

```
echo ""export OPENSEARCH_HOME=~/opensearch-2.5.0"" | tee -a ~/.bash_profile
echo ""export PYTORCH_VERSION=1.12.1"" | tee -a ~/.bash_profile
source ~/.bash_profile
```

Next, create a shell script file called `prepare_torch_neuron.sh`. You can copy and customize one of the following examples based on your Linux operating system:

- [Ubuntu 20.04](#ubuntu-2004)
- [Amazon Linux 2](#amazon-linux-2)

After you've run the scripts, exit your current terminal and open a new terminal to start OpenSearch.

GPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems.
{: .note}

#### Ubuntu 20.04

```
. /etc/os-release
sudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF
deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main
EOF
wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -

# Update OS packages
sudo apt-get update -y

################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################

# Install OS headers
sudo apt-get install linux-headers-$(uname -r) -y

# Install Neuron Driver
sudo apt-get install aws-neuronx-dkms -y

####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################

# Install Neuron Tools
sudo apt-get install aws-neuronx-tools -y

######################################################
#   Only for Ubuntu 20 - Install Python3.7
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt-get install python3.7
######################################################
# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo apt-get install -y python3.7-venv g++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set pip repository to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

#Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf==3.20.1"" torchvision

# If you need to trace neuron model, install the transformers for tracing the Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile

# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

#### Amazon Linux 2

```
# Configure Linux for Neuron repository updates
sudo tee /etc/yum.repos.d/neuron.repo > /dev/null <<EOF
[neuron]
name=Neuron YUM Repository
baseurl=https://yum.repos.neuron.amazonaws.com
enabled=1
metadata_expire=0
EOF
sudo rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB
# Update OS packages
sudo yum update -y
################################################################################################################
# To install or update to Neuron versions 1.19.1 and newer from previous releases:
# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver
################################################################################################################
# Install OS headers
sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y
# Install Neuron Driver
####################################################################################
# Warning: If Linux kernel is updated as a result of OS package update
#          Neuron driver (aws-neuron-dkms) should be re-installed after reboot
####################################################################################
sudo yum install aws-neuronx-dkms -y
# Install Neuron Tools
sudo yum install aws-neuronx-tools -y

# Install Python venv and activate Python virtual environment to install    
# Neuron pip packages.
cd ~
sudo yum install -y python3.7-venv gcc-c++
python3.7 -m venv pytorch_venv
source pytorch_venv/bin/activate
pip install -U pip

# Set Pip repository  to point to the Neuron repository
pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com

# Install Neuron PyTorch
pip install torch-neuron torchvision
# If you need to trace the neuron model, install torch neuron with this command
# pip install torch-neuron neuron-cc[tensorflow] ""protobuf<4"" torchvision

# If you need to run the trace neuron model, install transformers for tracing Huggingface model.
# pip install transformers

# Copy torch neuron lib to OpenSearch
PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/
mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
echo ""export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so"" | tee -a ~/.bash_profile
# Increase JVm stack size to >=2MB
echo ""-Xss2m"" | tee -a $OPENSEARCH_HOME/config/jvm.options
# Increase max file descriptors to 65535
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
# max virtual memory areas vm.max_map_count to 262144
sudo sysctl -w vm.max_map_count=262144
```

When the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch.

OpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually.

#### Prepare ML node manually

If the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually:

1. Deploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see [Deploy on AWS accelerator instance](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/setup/pytorch-install.html#deploy-on-aws-ml-accelerator-instance).

2. Copy the Neuron library into OpenSearch. The following command uses a directory named `opensearch-2.5.0`:

   ```
   OPENSEARCH_HOME=~/opensearch-2.5.0
   ```

3. Set the `PYTORCH_EXTRA_LIBRARY_PATH` path. In this example, we create a `pytorch` virtual environment in the OPENSEARCH_HOME folder:

   ```
   PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/


   mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r  $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron
   export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so
  ```

4. (Optional) To monitor the GPU usage of your accelerator instance, install [Neuron tools](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/index.html), which allows models to be used inside your instance:

   ```
   # Install Neuron Tools
   sudo apt-get install aws-neuronx-tools -y
   ```

   ```
   # Add Neuron tools your PATH
   export PATH=/opt/aws/neuron/bin:$PATH
   ```
  
   ```
   # Test Neuron tools
   neuron-top
   ```


5. To make sure you have enough memory to upload a model, increase the JVM stack size to `>+2MB`:

   ```
   echo ""-Xss2m"" | sudo tee -a $OPENSEARCH_HOME/config/jvm.options
   ```

6. Start OpenSearch. 

## Troubleshooting

Due to the amount of data required to work with ML models, you might encounter the following `max file descriptors` or `vm.max_map_count` errors when trying to run OpenSearch in a your cluster: 

```
[1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535]
[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
```

To troubleshoot the max file descriptors error, run the following command:

```
echo ""$(whoami) - nofile 65535"" | sudo tee -a /etc/security/limits.conf
```

To fix the `vm.max_map_count` error, run this command to increase the count to `262114`:

```
sudo sysctl -w vm.max_map_count=262144
```

## Next steps

If you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see [Compiling and Deploying HuggingFace Pretrained BERT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html).

","Based on the context provided, to boost the performance of your machine learning (ML) model when running on OpenSearch ML nodes, you can leverage GPU acceleration. The key points are:

1. OpenSearch supports GPU acceleration using NVIDIA GPUs with CUDA 11.6 or AWS Inferentia instances.

2. To use GPU acceleration, you need to provision GPU instances through Amazon EC2 and set up the ML nodes on those GPU instances.

3. For NVIDIA GPUs, you need to ensure the nvidia-uvm kernel is present and initialize it if not.

4. For AWS Inferentia instances, you can use the provided scripts (for Ubuntu 20.04 or Amazon Linux 2) to set up the environment, install required packages like PyTorch 1.12.1, copy required libraries, and configure system settings.

5. If the provided scripts don't work, you can manually deploy on an AWS Inferentia instance, copy the Neuron library to OpenSearch, set the required environment variables, and optionally install Neuron tools.

6. You may need to increase the JVM stack size, max file descriptors, and max virtual memory areas to accommodate the resource requirements of running ML models.

By setting up GPU-accelerated ML nodes, you can leverage the parallel processing capabilities of GPUs to speed up model upload, inference, and training tasks, thereby boosting the overall performance of your ML workloads on OpenSearch.",3.0,"Context Utilization: 1 - The response stays firmly within the context provided, drawing upon the key details and recommendations outlined in the OpenSearch documentation. It does not introduce any extraneous information or speculate beyond the given details.

Completeness: 1 - The answer comprehensively covers the main steps and considerations for boosting the performance of an ML model running on OpenSearch ML nodes through GPU acceleration. It addresses the supported GPU instances, setup requirements, and potential troubleshooting steps.

Conciseness: 0 - While the response provides a thorough overview, it could be more concise in its delivery. The level of technical detail and step-by-step instructions, while helpful, may be more than necessary to directly address the original question.

Accuracy: 1 - All the information presented in the answer aligns accurately with the context provided. The steps and recommendations are consistent with the OpenSearch documentation.

Clarity: 1 - The response is generally clear and easy to follow, though the density of technical information may make it challenging for a non-expert user to fully digest. The structure and flow of the answer help guide the reader through the key points."
Can you show me an example of how to use lat/long coordinates?,"---
layout: default
title: Geopoint
nav_order: 56
has_children: false
parent: Geographic field types
grand_parent: Supported field types
redirect_from:
  - /opensearch/supported-field-types/geo-point/
  - /field-types/geo-point/
---

# Geopoint field type

A geopoint field type contains a geographic point specified by latitude and longitude. 

## Example

Create a mapping with a geopoint field type:

```json
PUT testindex1
{
  ""mappings"": {
    ""properties"": {
      ""point"": {
        ""type"": ""geo_point""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Formats

Geopoints can be indexed in the following formats:

- An object with a latitude and longitude

```json
PUT testindex1/_doc/1
{
  ""point"": { 
    ""lat"": 40.71,
    ""lon"": 74.00
  }
}
```
{% include copy-curl.html %}

- A string in the ""`latitude`,`longitude`"" format

```json
PUT testindex1/_doc/2
{
  ""point"": ""40.71,74.00"" 
}
```
{% include copy-curl.html %}

- A geohash

```json
PUT testindex1/_doc/3
{
  ""point"": ""txhxegj0uyp3""
}
```
{% include copy-curl.html %}

- An array in the [`longitude`, `latitude`] format

```json
PUT testindex1/_doc/4
{
  ""point"": [74.00, 40.71] 
}
```
{% include copy-curl.html %}

- A [Well-Known Text](https://docs.opengeospatial.org/is/12-063r5/12-063r5.html) POINT in the ""POINT(`longitude` `latitude`)"" format

```json
PUT testindex1/_doc/5
{
  ""point"": ""POINT (74.00 40.71)""
}
```
{% include copy-curl.html %}

- GeoJSON format, where the `coordinates` are in the [`longitude`, `latitude`] format

```json
PUT testindex1/_doc/6
{
  ""point"": {
    ""type"": ""Point"",
    ""coordinates"": [74.00, 40.71]
  }
}
```
{% include copy-curl.html %}

## Parameters

The following table lists the parameters accepted by geopoint field types. All parameters are optional.

Parameter | Description 
:--- | :--- 
`ignore_malformed` | A Boolean value that specifies to ignore malformed values and not to throw an exception. Valid values for latitude are [-90, 90]. Valid values for longitude are [-180, 180]. Default is `false`.
`ignore_z_value` | Specific to points with three coordinates. If `ignore_z_value` is `true`, the third coordinate is not indexed but is still stored in the _source field. If `ignore_z_value` is `false`, an exception is thrown.
[`null_value`]({{site.url}}{{site.baseurl}}/opensearch/supported-field-types/index#null-value) | A  value to be used in place of `null`. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is `null`. Default is `null`.","Yes, the context provides several examples of how to use latitude and longitude coordinates to index a geopoint field. Here are a few examples from the context:

1. An object with a latitude and longitude:

```json
PUT testindex1/_doc/1
{
  ""point"": { 
    ""lat"": 40.71,
    ""lon"": 74.00
  }
}
```

2. A string in the ""`latitude`,`longitude`"" format:

```json
PUT testindex1/_doc/2
{
  ""point"": ""40.71,74.00"" 
}
```

3. An array in the [`longitude`, `latitude`] format:

```json
PUT testindex1/_doc/4
{
  ""point"": [74.00, 40.71] 
}
```

4. A [Well-Known Text](https://docs.opengeospatial.org/is/12-063r5/12-063r5.html) POINT in the ""POINT(`longitude` `latitude`)"" format:

```json
PUT testindex1/_doc/5
{
  ""point"": ""POINT (74.00 40.71)""
}
```

5. GeoJSON format, where the `coordinates` are in the [`longitude`, `latitude`] format:

```json
PUT testindex1/_doc/6
{
  ""point"": {
    ""type"": ""Point"",
    ""coordinates"": [74.00, 40.71]
  }
}
```

So the context provides multiple ways to specify latitude and longitude coordinates when indexing a geopoint field.",4.0,"Context Utilization: 1 - The response stays firmly within the provided context, drawing directly from the examples and information given. It does not venture into any external information or speculative elements, which aligns well with the LLM evaluation.

Completeness: 1 - The answer covers the key aspects of the question by providing a comprehensive set of examples demonstrating how to use latitude and longitude coordinates in various formats. The response covers the main use cases and formats outlined in the context, ensuring the question is thoroughly addressed.

Conciseness: 0 - While the level of detail provided is relevant and helpful, the response could be more concise in its explanations. Some of the examples and descriptions, while informative, add a bit of verbosity that could be streamlined without sacrificing the core content.

Accuracy: 1 - All the information presented in the response is factually correct and aligned with the context. The examples and formats described match the details provided in the given information, ensuring the accuracy of the response.

Clarity: 1 - The response is well-structured and easy to follow, with the examples helping to illustrate the different approaches to specifying latitude and longitude coordinates. The language used is clear and straightforward, making the content accessible and understandable."
How do I use vector search?,"---
layout: default
title: Vector search
nav_order: 22
has_children: false
has_toc: false
---

# Vector search

OpenSearch is a comprehensive search platform that supports a variety of data types, including vectors. OpenSearch vector database functionality is seamlessly integrated with its generic database function.

In OpenSearch, you can generate vector embeddings, store those embeddings in an index, and use them for vector search. Choose one of the following options:

- Generate embeddings using a library of your choice before ingesting them into OpenSearch. Once you ingest vectors into an index, you can perform a vector similarity search on the vector space. For more information, see [Working with embeddings generated outside of OpenSearch](#working-with-embeddings-generated-outside-of-opensearch). 
- Automatically generate embeddings within OpenSearch. To use embeddings for semantic search, the ingested text (the corpus) and the query need to be embedded using the same model. [Neural search]({{site.url}}{{site.baseurl}}/search-plugins/neural-search/) packages this functionality, eliminating the need to manage the internal details. For more information, see [Generating vector embeddings within OpenSearch](#generating-vector-embeddings-in-opensearch).

## Working with embeddings generated outside of OpenSearch

After you generate vector embeddings, upload them to an OpenSearch index and search the index using vector search. For a complete example, see [Example](#example).

### k-NN index

To build a vector database and use vector search, you must specify your index as a [k-NN index]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/) when creating it by setting `index.knn` to `true`:

```json
PUT test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 1024,
        ""method"": {
          ""name"": ""hnsw"",
          ""space_type"": ""l2"",
          ""engine"": ""nmslib"",
          ""parameters"": {
            ""ef_construction"": 128,
            ""m"": 24
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

### k-NN vector

You must designate the field that will store vectors as a [`knn_vector`]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector/) field type. OpenSearch supports vectors of up to 16,000 dimensions, each of which is represented as a 32-bit or 16-bit float. 

To save storage space, you can use `byte` or `binary` vectors. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector) and [Binary k-NN vectors]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#binary-k-nn-vectors).

### k-NN vector search

Vector search finds the vectors in your database that are most similar to the query vector. OpenSearch supports the following search methods:

- [Approximate search](#approximate-search) (approximate k-NN, or ANN): Returns approximate nearest neighbors to the query vector. Usually, approximate search algorithms sacrifice indexing speed and search accuracy in exchange for performance benefits such as lower latency, smaller memory footprints, and more scalable search. For most use cases, approximate search is the best option.

- Exact search (exact k-NN): A brute-force, exact k-NN search of vector fields. OpenSearch supports the following types of exact search: 
  - [Exact k-NN with scoring script]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-score-script/): Using the k-NN scoring script, you can apply a filter to an index before executing the nearest neighbor search. 
  - [Painless extensions]({{site.url}}{{site.baseurl}}/search-plugins/knn/painless-functions/): Adds the distance functions as Painless extensions that you can use in more complex combinations. You can use this method to perform a brute-force, exact k-NN search of an index, which also supports pre-filtering. 

### Approximate search

OpenSearch supports several algorithms for approximate vector search, each with its own advantages. For complete documentation, see [Approximate search]({{site.url}}{{site.baseurl}}/search-plugins/knn/approximate-knn/). For more information about the search methods and engines, see [Method definitions]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#method-definitions). For method recommendations, see [Choosing the right method]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#choosing-the-right-method).

To use approximate vector search, specify one of the following search methods (algorithms) in the `method` parameter:

- Hierarchical Navigable Small World (HNSW)
- Inverted File System (IVF)

Additionally, specify the engine (library) that implements this method in the `engine` parameter:

- [Non-Metric Space Library (NMSLIB)](https://github.com/nmslib/nmslib)
- [Facebook AI Similarity Search (Faiss)](https://github.com/facebookresearch/faiss)
- Lucene

The following table lists the combinations of search methods and libraries supported by the k-NN engine for approximate vector search.

Method | Engine
:--- | :---
HNSW | NMSLIB, Faiss, Lucene
IVF | Faiss 

### Engine recommendations

In general, select NMSLIB or Faiss for large-scale use cases. Lucene is a good option for smaller deployments and offers benefits like smart filtering, where the optimal filtering strategy—pre-filtering, post-filtering, or exact k-NN—is automatically applied depending on the situation. The following table summarizes the differences between each option.

| |  NMSLIB/HNSW |  Faiss/HNSW |  Faiss/IVF |  Lucene/HNSW |
|:---|:---|:---|:---|:---|
|  Max dimensions |  16,000  |  16,000 |  16,000 |  16,000 |
|  Filter |  Post-filter |  Post-filter |  Post-filter |  Filter during search |
|  Training required |  No |  No |  Yes |  No |
|  Similarity metrics |  `l2`, `innerproduct`, `cosinesimil`, `l1`, `linf`  |  `l2`, `innerproduct` |  `l2`, `innerproduct` |  `l2`, `cosinesimil` |
|  Number of vectors   |  Tens of billions |  Tens of billions |  Tens of billions |  Less than 10 million |
|  Indexing latency |  Low |  Low  |  Lowest  |  Low  |
|  Query latency and quality  |  Low latency and high quality |  Low latency and high quality  |  Low latency and low quality  |  High latency and high quality  |
|  Vector compression  |  Flat |  Flat <br>Product quantization |  Flat <br>Product quantization |  Flat  |
|  Memory consumption |  High  |  High <br> Low with PQ |  Medium <br> Low with PQ |  High  |

### Example

In this example, you'll create a k-NN index, add data to the index, and search the data.

#### Step 1: Create a k-NN index

First, create an index that will store sample hotel data. Set `index.knn` to `true` and specify the `location` field as a `knn_vector`:

```json
PUT /hotels-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100,
      ""number_of_shards"": 1,
      ""number_of_replicas"": 0
    }
  },
  ""mappings"": {
    ""properties"": {
      ""location"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""space_type"": ""l2"",
          ""engine"": ""lucene"",
          ""parameters"": {
            ""ef_construction"": 100,
            ""m"": 16
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

#### Step 2: Add data to your index

Next, add data to your index. Each document represents a hotel. The `location` field in each document contains a vector specifying the hotel's location:

```json
POST /_bulk
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""1"" } }
{ ""location"": [5.2, 4.4] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""2"" } }
{ ""location"": [5.2, 3.9] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""3"" } }
{ ""location"": [4.9, 3.4] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""4"" } }
{ ""location"": [4.2, 4.6] }
{ ""index"": { ""_index"": ""hotels-index"", ""_id"": ""5"" } }
{ ""location"": [3.3, 4.5] }
```
{% include copy-curl.html %}

#### Step 3: Search your data

Now search for hotels closest to the pin location `[5, 4]`. This location is labeled `Pin` in the following image. Each hotel is labeled with its document number.

![Hotels on a coordinate plane]({{site.url}}{{site.baseurl}}/images/k-nn-search-hotels.png/)

To search for the top three closest hotels, set `k` to `3`:

```json
POST /hotels-index/_search
{
  ""size"": 3,
  ""query"": {
    ""knn"": {
      ""location"": {
        ""vector"": [
          5,
          4
        ],
        ""k"": 3
      }
    }
  }
}
```
{% include copy-curl.html %}

The response contains the hotels closest to the specified pin location:

```json
{
  ""took"": 1093,
  ""timed_out"": false,
  ""_shards"": {
    ""total"": 1,
    ""successful"": 1,
    ""skipped"": 0,
    ""failed"": 0
  },
  ""hits"": {
    ""total"": {
      ""value"": 3,
      ""relation"": ""eq""
    },
    ""max_score"": 0.952381,
    ""hits"": [
      {
        ""_index"": ""hotels-index"",
        ""_id"": ""2"",
        ""_score"": 0.952381,
        ""_source"": {
          ""location"": [
            5.2,
            3.9
          ]
        }
      },
      {
        ""_index"": ""hotels-index"",
        ""_id"": ""1"",
        ""_score"": 0.8333333,
        ""_source"": {
          ""location"": [
            5.2,
            4.4
          ]
        }
      },
      {
        ""_index"": ""hotels-index"",
        ""_id"": ""3"",
        ""_score"": 0.72992706,
        ""_source"": {
          ""location"": [
            4.9,
            3.4
          ]
        }
      }
    ]
  }
}
```

### Vector search with filtering

For information about vector search with filtering, see [k-NN search with filters]({{site.url}}{{site.baseurl}}/search-plugins/knn/filter-search-knn/).

## Generating vector embeddings in OpenSearch

[Neural search]({{site.url}}{{site.baseurl}}/search-plugins/neural-search/) encapsulates the infrastructure needed to perform semantic vector searches. After you integrate an inference (embedding) service, neural search functions like lexical search, accepting a textual query and returning relevant documents.

When you index your data, neural search transforms text into vector embeddings and indexes both the text and its vector embeddings in a vector index. When you use a neural query during search, neural search converts the query text into vector embeddings and uses vector search to return the results.

### Choosing a model

The first step in setting up neural search is choosing a model. You can upload a model to your OpenSearch cluster, use one of the pretrained models provided by OpenSearch, or connect to an externally hosted model. For more information, see [Integrating ML models]({{site.url}}{{site.baseurl}}/ml-commons-plugin/integrating-ml-models/).

### Neural search tutorial

For a step-by-step tutorial, see [Neural search tutorial]({{site.url}}{{site.baseurl}}/search-plugins/neural-search-tutorial/).

### Search methods

Choose one of the following search methods to use your model for neural search:

- [Semantic search]({{site.url}}{{site.baseurl}}/search-plugins/semantic-search/): Uses dense retrieval based on text embedding models to search text data. 

- [Hybrid search]({{site.url}}{{site.baseurl}}/search-plugins/hybrid-search/): Combines lexical and neural search to improve search relevance. 

- [Multimodal search]({{site.url}}{{site.baseurl}}/search-plugins/multimodal-search/): Uses neural search with multimodal embedding models to search text and image data.

- [Neural sparse search]({{site.url}}{{site.baseurl}}/search-plugins/neural-sparse-search/): Uses neural search with sparse retrieval based on sparse embedding models to search text data.

- [Conversational search]({{site.url}}{{site.baseurl}}/search-plugins/conversational-search/): With conversational search, you can ask questions in natural language, receive a text response, and ask additional clarifying questions.
","According to the provided context, there are two main ways to use vector search in OpenSearch:

1. **Working with embeddings generated outside of OpenSearch**:
   - Generate vector embeddings using a library of your choice
   - Create a k-NN index in OpenSearch by setting `index.knn` to `true`
   - Specify the vector field as `knn_vector` type in the index mapping
   - Index the vector embeddings into the k-NN index
   - Use approximate k-NN search methods like HNSW or IVF to find nearest vector neighbors

2. **Generating vector embeddings within OpenSearch**:
   - Use the Neural Search plugin which encapsulates the infrastructure for semantic vector search
   - Choose a pre-trained model or upload your own model for text/image embeddings
   - Index your data, and Neural Search will automatically generate embeddings and index them
   - Use search methods like semantic search, hybrid search, multimodal search, etc. to query the indexed embeddings

The context provides detailed steps and examples for both approaches to help you get started with vector search in OpenSearch.",4.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, demonstrating a clear understanding of the topic and avoiding any extraneous details or assumptions. The steps and approaches described match the content of the context, which is essential for a comprehensive and relevant answer.

Completeness: 1 - The answer covers the key aspects of using vector search in OpenSearch, including the two main approaches (working with external embeddings and generating embeddings within OpenSearch), as well as the details on k-NN indexes, vector field types, and the various search methods. The response provides a thorough overview of the topic, addressing the core elements necessary to understand how to use vector search.

Conciseness: 0 - While the information is well-organized and easy to follow, the answer could be a bit more concise in its explanations. Some of the technical details, such as the specific algorithm parameters, may not be essential for a high-level understanding of the topic and could be omitted or summarized more briefly.

Accuracy: 1 - All the details provided in the response align accurately with the information presented in the context. The steps, concepts, and terminology used are consistent with the source material, demonstrating a strong grasp of the subject matter.

Clarity: 1 - The response is well-structured and easy to understand, with clear explanations and a logical flow. The different approaches and search methods are explained in a way that is accessible and informative, making it easy for the reader to follow along and comprehend the key points."
How do I understand the memory requirements for using hnsw?,"---
layout: default
title: k-NN vector quantization
nav_order: 27
parent: k-NN search
has_children: false
has_math: true
---

# k-NN vector quantization

By default, the k-NN plugin supports the indexing and querying of vectors of type `float`, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping `float` vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native `nmslib` and `faiss` engines). To reduce the memory footprint, you can use vector quantization.

OpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, 16-bit scalar quantization, and product quantization (PQ).

## Lucene byte vector

Starting with k-NN plugin version 2.9, you can use `byte` vectors with the Lucene engine in order to reduce the amount of required memory. This requires quantizing the vectors outside of OpenSearch before ingesting them into an OpenSearch index. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector).

## Lucene scalar quantization

Starting with version 2.16, the k-NN plugin supports built-in scalar quantization for the Lucene engine. Unlike the [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector), which requires you to quantize vectors before ingesting the documents, the Lucene scalar quantizer quantizes input vectors in OpenSearch during ingestion. The Lucene scalar quantizer converts 32-bit floating-point input vectors into 7-bit integer vectors in each segment using the minimum and maximum quantiles computed based on the [`confidence_interval`](#confidence-interval) parameter. During search, the query vector is quantized in each segment using the segment's minimum and maximum quantiles in order to compute the distance between the query vector and the segment's quantized input vectors. 

Quantization can decrease the memory footprint by a factor of 4 in exchange for some loss in recall. Additionally, quantization slightly increases disk usage because it requires storing both the raw input vectors and the quantized vectors.

### Using Lucene scalar quantization

To use the Lucene scalar quantizer, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

### Confidence interval

Optionally, you can specify the `confidence_interval` parameter in the `method.parameters.encoder` object.
The `confidence_interval` is used to compute the minimum and maximum quantiles in order to quantize the vectors:
- If you set the `confidence_interval` to a value in the `0.9` to `1.0` range, inclusive, then the quantiles are calculated statically. For example, setting the `confidence_interval` to `0.9` specifies to compute the minimum and maximum quantiles based on the middle 90% of the vector values, excluding the minimum 5% and maximum 5% of the values. 
- Setting `confidence_interval` to `0` specifies to compute the quantiles dynamically, which involves oversampling and additional computations performed on the input data.
- When `confidence_interval` is not set, it is computed based on the vector dimension $$d$$ using the formula $$max(0.9, 1 - \\frac{1}{1 + d})$$.

Lucene scalar quantization is applied only to `float` vectors. If you change the default value of the `data_type` parameter from `float` to `byte` or any other type when mapping a [k-NN vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector/), then the request is rejected.
{: .warning}

The following example method definition specifies the Lucene `sq` encoder with the `confidence_interval` set to `1.0`. This `confidence_interval` specifies to consider all the input vectors when computing the minimum and maximum quantiles. Vectors are quantized to 7 bits by default:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""confidence_interval"": 1.0
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

There are no changes to ingestion or query mapping and no range limitations for the input vectors. 

### Memory estimation

In the ideal scenario, 7-bit vectors created by the Lucene scalar quantizer use only 25% of the memory required by 32-bit vectors.

#### HNSW memory estimation

The memory required for the Hierarchical Navigable Small World (HNSW) graph can be estimated as `1.1 * (dimension + 8 * M)` bytes/vector, where `M` is the maximum number of bidirectional links created for each element during the construction of the graph.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (256 + 8 * 16) * 1,000,000 ~= 0.4 GB
```

## Faiss 16-bit scalar quantization 
 
Starting with version 2.13, the k-NN plugin supports performing scalar quantization for the Faiss engine within OpenSearch. Within the Faiss engine, a scalar quantizer (SQfp16) performs the conversion between 32-bit and 16-bit vectors. At ingestion time, when you upload 32-bit floating-point vectors to OpenSearch, SQfp16 quantizes them into 16-bit floating-point vectors and stores the quantized vectors in a k-NN index. 

At search time, SQfp16 decodes the vector values back into 32-bit floating-point values for distance computation. The SQfp16 quantization can decrease the memory footprint by a factor of 2. Additionally, it leads to a minimal loss in recall when differences between vector values are large compared to the error introduced by eliminating their two least significant bits. When used with [SIMD optimization]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index#simd-optimization-for-the-faiss-engine), SQfp16 quantization can also significantly reduce search latencies and improve indexing throughput. 

SIMD optimization is not supported on Windows. Using Faiss scalar quantization on Windows can lead to a significant drop in performance, including decreased indexing throughput and increased search latencies.
{: .warning} 

### Using Faiss scalar quantization

To use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

Optionally, you can specify the parameters in `method.parameters.encoder`. For more information about `encoder` object parameters, see [SQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#sq-parameters).

The `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the [-65504.0, 65504.0] range. To define how to handle out-of-range values, the preceding request specifies the `clip` parameter. By default, this parameter is `false`, and any vectors containing out-of-range values are rejected. 

When `clip` is set to `true` (as in the preceding request), out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is `[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.

We recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.
{: .note}

The following example method definition specifies the Faiss SQfp16 encoder, which rejects any indexing request that contains out-of-range vector values (because the `clip` parameter is `false` by default):

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""type"": ""fp16""
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

During ingestion, make sure each vector dimension is in the supported range ([-65504.0, 65504.0]).

```json
PUT test-index/_doc/1
{
  ""my_vector1"": [-65504.0, 65503.845, 55.82]
}
```
{% include copy-curl.html %}

During querying, the query vector has no range limitation:

```json
GET test-index/_search
{
  ""size"": 2,
  ""query"": {
    ""knn"": {
      ""my_vector1"": {
        ""vector"": [265436.876, -120906.256, 99.84],
        ""k"": 2
      }
    }
  }
}
```
{% include copy-curl.html %}

### Memory estimation

In the best-case scenario, 16-bit vectors produced by the Faiss SQfp16 quantizer require 50% of the memory that 32-bit vectors require. 

#### HNSW memory estimation

The memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB
```

#### IVF memory estimation

The memory required for IVF is estimated to be `1.1 * (((2 * dimension) * num_vectors) + (4 * nlist * d))` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and `nlist` of 128. The memory requirement can be estimated as follows:

```r
1.1 * (((2 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 0.525 GB
```

## Faiss product quantization

PQ is a technique used to represent a vector in a configurable amount of bits. In general, it can be used to achieve a higher level of compression as compared to byte or scalar quantization. PQ works by separating vectors into _m_ subvectors and encoding each subvector with _code_size_ bits. Thus, the total amount of memory for the vector is `m*code_size` bits, plus overhead. For details about the parameters, see [PQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#pq-parameters). PQ is only supported for the _Faiss_ engine and can be used with either the _HNSW_ or _IVF_ approximate nearest neighbor (ANN) algorithms.

### Using Faiss product quantization

To minimize loss in accuracy, PQ requires a _training_ step that builds a model based on the distribution of the data that will be searched.

The product quantizer is trained by running k-means clustering on a set of training vectors for each subvector space and extracts the centroids to be used for encoding. The training vectors can be either a subset of the vectors to be ingested or vectors that have the same distribution and dimension as the vectors to be ingested.

In OpenSearch, the training vectors need to be present in an index. In general, the amount of training data will depend on which ANN algorithm is used and how much data will be stored in the index. For IVF-based indexes, a recommended number of training vectors is `max(1000*nlist, 2^code_size * 1000)`. For HNSW-based indexes, a recommended number is `2^code_size*1000`. See the [Faiss documentation](https://github.com/facebookresearch/faiss/wiki/FAQ#how-many-training-points-do-i-need-for-k-means) for more information about the methodology used to calculate these figures.

For PQ, both _m_ and _code_size_ need to be selected. _m_ determines the number of subvectors into which vectors should be split for separate encoding. Consequently, the _dimension_ needs to be divisible by _m_. _code_size_ determines the number of bits used to encode each subvector. In general, we recommend a setting of `code_size = 8` and then tuning _m_ to get the desired trade-off between memory footprint and recall.

For an example of setting up an index with PQ, see the [Building a k-NN index from a model]({{site.url}}{{site.baseurl}}/search-plugins/knn/approximate-knn/#building-a-k-nn-index-from-a-model) tutorial.

### Memory estimation

While PQ is meant to represent individual vectors with `m*code_size` bits, in reality, the indexes consume more space. This is mainly due to the overhead of storing certain code tables and auxiliary data structures.

Some of the memory formulas depend on the number of segments present. This is not typically known beforehand, but a recommended default value is 300.
{: .note}

#### HNSW memory estimation

The memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.

As an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1 * ((8 / 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB
```

#### IVF memory estimation

The memory required for IVF with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24) * num_vectors  + num_segments * (2^code_size * 4 * d + 4 * ivf_nlist * d))` bytes.

For example, assume that you have 1 million vectors with a dimension of 256, `ivf_nlist` of 512, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1*((8 / 8 * 64 + 24) * 1000000  + 100 * (2^8 * 4 * 256 + 4 * 512 * 256))  ~= 0.171 GB
```
","The memory requirements for using HNSW (Hierarchical Navigable Small World) with k-NN vector indexing in OpenSearch depend on several factors:

1. **Vector Quantization Method**:
   - For unquantized float vectors, the memory estimation formula is: `1.1 * (dimension + 8 * M) * num_vectors` bytes
   - For Lucene scalar quantized vectors, it's: `1.1 * (dimension + 8 * M) * num_vectors` bytes
   - For Faiss 16-bit scalar quantized vectors, it's: `1.1 * (2 * dimension + 8 * M) * num_vectors` bytes
   - For Faiss product quantization, it's: `1.1 * (((pq_code_size / 8) * pq_m + 24 + 8 * M) * num_vectors + num_segments * (2^pq_code_size * 4 * dimension))` bytes

2. **Dimension (d)**: The dimensionality of the vectors. Higher dimensions require more memory.

3. **Number of Vectors (num_vectors)**: More vectors require more memory.

4. **Maximum Connections (M)**: The maximum number of bi-directional links for each vector in the HNSW graph. Higher M typically leads to better accuracy but requires more memory.

5. **Product Quantization Parameters (pq_m, pq_code_size)**: For product quantization, the memory depends on the number of sub-vectors (pq_m) and the number of bits used to encode each sub-vector (pq_code_size).

6. **Number of Segments (num_segments)**: For product quantization, the memory also depends on the number of segments in the index.

So in summary, the memory requirements grow with the number of vectors, their dimensionality, the value of M, and the specific quantization method used. Quantization can significantly reduce memory needs compared to unquantized float vectors.",3.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, demonstrating a thorough understanding of the relevant details. It does not introduce any extraneous or speculative elements, which aligns well with the LLM evaluation.

Completeness: 1 - The answer comprehensively addresses the key factors that impact the memory requirements for using HNSW, such as the vector quantization method, vector dimensionality, number of vectors, maximum connections, and quantization parameters. The level of detail provided ensures a complete understanding of the topic.

Conciseness: 0 - While the response covers the topic thoroughly, there are instances where the explanations could be more concise. Some of the descriptions, such as the memory estimation formulas, are slightly verbose and could be streamlined without losing clarity.

Accuracy: 1 - All the information presented in the answer is factually correct and aligns with the context. The response demonstrates a strong grasp of the technical details and accurately conveys the relevant concepts.

Clarity: 1 - The response is generally clear and easy to follow, with a logical flow and appropriate use of examples to illustrate the key points. The explanations are well-structured and should enable a reader to develop a solid understanding of the topic."
Can you show me some different examples of using different quantization methods for vectors?,"---
layout: default
title: k-NN vector quantization
nav_order: 27
parent: k-NN search
has_children: false
has_math: true
---

# k-NN vector quantization

By default, the k-NN plugin supports the indexing and querying of vectors of type `float`, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping `float` vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native `nmslib` and `faiss` engines). To reduce the memory footprint, you can use vector quantization.

OpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, 16-bit scalar quantization, and product quantization (PQ).

## Lucene byte vector

Starting with k-NN plugin version 2.9, you can use `byte` vectors with the Lucene engine in order to reduce the amount of required memory. This requires quantizing the vectors outside of OpenSearch before ingesting them into an OpenSearch index. For more information, see [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector).

## Lucene scalar quantization

Starting with version 2.16, the k-NN plugin supports built-in scalar quantization for the Lucene engine. Unlike the [Lucene byte vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector#lucene-byte-vector), which requires you to quantize vectors before ingesting the documents, the Lucene scalar quantizer quantizes input vectors in OpenSearch during ingestion. The Lucene scalar quantizer converts 32-bit floating-point input vectors into 7-bit integer vectors in each segment using the minimum and maximum quantiles computed based on the [`confidence_interval`](#confidence-interval) parameter. During search, the query vector is quantized in each segment using the segment's minimum and maximum quantiles in order to compute the distance between the query vector and the segment's quantized input vectors. 

Quantization can decrease the memory footprint by a factor of 4 in exchange for some loss in recall. Additionally, quantization slightly increases disk usage because it requires storing both the raw input vectors and the quantized vectors.

### Using Lucene scalar quantization

To use the Lucene scalar quantizer, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

### Confidence interval

Optionally, you can specify the `confidence_interval` parameter in the `method.parameters.encoder` object.
The `confidence_interval` is used to compute the minimum and maximum quantiles in order to quantize the vectors:
- If you set the `confidence_interval` to a value in the `0.9` to `1.0` range, inclusive, then the quantiles are calculated statically. For example, setting the `confidence_interval` to `0.9` specifies to compute the minimum and maximum quantiles based on the middle 90% of the vector values, excluding the minimum 5% and maximum 5% of the values. 
- Setting `confidence_interval` to `0` specifies to compute the quantiles dynamically, which involves oversampling and additional computations performed on the input data.
- When `confidence_interval` is not set, it is computed based on the vector dimension $$d$$ using the formula $$max(0.9, 1 - \\frac{1}{1 + d})$$.

Lucene scalar quantization is applied only to `float` vectors. If you change the default value of the `data_type` parameter from `float` to `byte` or any other type when mapping a [k-NN vector]({{site.url}}{{site.baseurl}}/field-types/supported-field-types/knn-vector/), then the request is rejected.
{: .warning}

The following example method definition specifies the Lucene `sq` encoder with the `confidence_interval` set to `1.0`. This `confidence_interval` specifies to consider all the input vectors when computing the minimum and maximum quantiles. Vectors are quantized to 7 bits by default:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 2,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""confidence_interval"": 1.0
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

There are no changes to ingestion or query mapping and no range limitations for the input vectors. 

### Memory estimation

In the ideal scenario, 7-bit vectors created by the Lucene scalar quantizer use only 25% of the memory required by 32-bit vectors.

#### HNSW memory estimation

The memory required for the Hierarchical Navigable Small World (HNSW) graph can be estimated as `1.1 * (dimension + 8 * M)` bytes/vector, where `M` is the maximum number of bidirectional links created for each element during the construction of the graph.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (256 + 8 * 16) * 1,000,000 ~= 0.4 GB
```

## Faiss 16-bit scalar quantization 
 
Starting with version 2.13, the k-NN plugin supports performing scalar quantization for the Faiss engine within OpenSearch. Within the Faiss engine, a scalar quantizer (SQfp16) performs the conversion between 32-bit and 16-bit vectors. At ingestion time, when you upload 32-bit floating-point vectors to OpenSearch, SQfp16 quantizes them into 16-bit floating-point vectors and stores the quantized vectors in a k-NN index. 

At search time, SQfp16 decodes the vector values back into 32-bit floating-point values for distance computation. The SQfp16 quantization can decrease the memory footprint by a factor of 2. Additionally, it leads to a minimal loss in recall when differences between vector values are large compared to the error introduced by eliminating their two least significant bits. When used with [SIMD optimization]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index#simd-optimization-for-the-faiss-engine), SQfp16 quantization can also significantly reduce search latencies and improve indexing throughput. 

SIMD optimization is not supported on Windows. Using Faiss scalar quantization on Windows can lead to a significant drop in performance, including decreased indexing throughput and increased search latencies.
{: .warning} 

### Using Faiss scalar quantization

To use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq""
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

Optionally, you can specify the parameters in `method.parameters.encoder`. For more information about `encoder` object parameters, see [SQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#sq-parameters).

The `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the [-65504.0, 65504.0] range. To define how to handle out-of-range values, the preceding request specifies the `clip` parameter. By default, this parameter is `false`, and any vectors containing out-of-range values are rejected. 

When `clip` is set to `true` (as in the preceding request), out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is `[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.

We recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.
{: .note}

The following example method definition specifies the Faiss SQfp16 encoder, which rejects any indexing request that contains out-of-range vector values (because the `clip` parameter is `false` by default):

```json
PUT /test-index
{
  ""settings"": {
    ""index"": {
      ""knn"": true,
      ""knn.algo_param.ef_search"": 100
    }
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector1"": {
        ""type"": ""knn_vector"",
        ""dimension"": 3,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""type"": ""fp16""
              }
            },
            ""ef_construction"": 256,
            ""m"": 8
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

During ingestion, make sure each vector dimension is in the supported range ([-65504.0, 65504.0]).

```json
PUT test-index/_doc/1
{
  ""my_vector1"": [-65504.0, 65503.845, 55.82]
}
```
{% include copy-curl.html %}

During querying, the query vector has no range limitation:

```json
GET test-index/_search
{
  ""size"": 2,
  ""query"": {
    ""knn"": {
      ""my_vector1"": {
        ""vector"": [265436.876, -120906.256, 99.84],
        ""k"": 2
      }
    }
  }
}
```
{% include copy-curl.html %}

### Memory estimation

In the best-case scenario, 16-bit vectors produced by the Faiss SQfp16 quantizer require 50% of the memory that 32-bit vectors require. 

#### HNSW memory estimation

The memory required for Hierarchical Navigable Small Worlds (HNSW) is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

```r
1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB
```

#### IVF memory estimation

The memory required for IVF is estimated to be `1.1 * (((2 * dimension) * num_vectors) + (4 * nlist * d))` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and `nlist` of 128. The memory requirement can be estimated as follows:

```r
1.1 * (((2 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 0.525 GB
```

## Faiss product quantization

PQ is a technique used to represent a vector in a configurable amount of bits. In general, it can be used to achieve a higher level of compression as compared to byte or scalar quantization. PQ works by separating vectors into _m_ subvectors and encoding each subvector with _code_size_ bits. Thus, the total amount of memory for the vector is `m*code_size` bits, plus overhead. For details about the parameters, see [PQ parameters]({{site.url}}{{site.baseurl}}/search-plugins/knn/knn-index/#pq-parameters). PQ is only supported for the _Faiss_ engine and can be used with either the _HNSW_ or _IVF_ approximate nearest neighbor (ANN) algorithms.

### Using Faiss product quantization

To minimize loss in accuracy, PQ requires a _training_ step that builds a model based on the distribution of the data that will be searched.

The product quantizer is trained by running k-means clustering on a set of training vectors for each subvector space and extracts the centroids to be used for encoding. The training vectors can be either a subset of the vectors to be ingested or vectors that have the same distribution and dimension as the vectors to be ingested.

In OpenSearch, the training vectors need to be present in an index. In general, the amount of training data will depend on which ANN algorithm is used and how much data will be stored in the index. For IVF-based indexes, a recommended number of training vectors is `max(1000*nlist, 2^code_size * 1000)`. For HNSW-based indexes, a recommended number is `2^code_size*1000`. See the [Faiss documentation](https://github.com/facebookresearch/faiss/wiki/FAQ#how-many-training-points-do-i-need-for-k-means) for more information about the methodology used to calculate these figures.

For PQ, both _m_ and _code_size_ need to be selected. _m_ determines the number of subvectors into which vectors should be split for separate encoding. Consequently, the _dimension_ needs to be divisible by _m_. _code_size_ determines the number of bits used to encode each subvector. In general, we recommend a setting of `code_size = 8` and then tuning _m_ to get the desired trade-off between memory footprint and recall.

For an example of setting up an index with PQ, see the [Building a k-NN index from a model]({{site.url}}{{site.baseurl}}/search-plugins/knn/approximate-knn/#building-a-k-nn-index-from-a-model) tutorial.

### Memory estimation

While PQ is meant to represent individual vectors with `m*code_size` bits, in reality, the indexes consume more space. This is mainly due to the overhead of storing certain code tables and auxiliary data structures.

Some of the memory formulas depend on the number of segments present. This is not typically known beforehand, but a recommended default value is 300.
{: .note}

#### HNSW memory estimation

The memory required for HNSW with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24 + 8 * hnsw_m) * num_vectors + num_segments * (2^pq_code_size * 4 * d))` bytes.

As an example, assume that you have 1 million vectors with a dimension of 256, `hnsw_m` of 16, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1 * ((8 / 8 * 32 + 24 + 8 * 16) * 1000000 + 100 * (2^8 * 4 * 256)) ~= 0.215 GB
```

#### IVF memory estimation

The memory required for IVF with PQ is estimated to be `1.1*(((pq_code_size / 8) * pq_m + 24) * num_vectors  + num_segments * (2^code_size * 4 * d + 4 * ivf_nlist * d))` bytes.

For example, assume that you have 1 million vectors with a dimension of 256, `ivf_nlist` of 512, `pq_m` of 32, `pq_code_size` of 8, and 100 segments. The memory requirement can be estimated as follows:

```r
1.1*((8 / 8 * 64 + 24) * 1000000  + 100 * (2^8 * 4 * 256 + 4 * 512 * 256))  ~= 0.171 GB
```
","Sure, here are some examples of using different quantization methods for vectors with the k-NN plugin in OpenSearch:

1. **Lucene byte vector**:
```json
PUT /index
{
  ""mappings"": {
    ""properties"": {
      ""my_vector"": {
        ""type"": ""knn_vector"",
        ""dimension"": 4,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""data_type"": ""byte""
          }
        }
      }
    }
  }
}
```
This maps a knn_vector field `my_vector` of dimension 4 using the Lucene engine and byte vector quantization.

2. **Lucene scalar quantization**:
```json
PUT /index
{
  ""mappings"": {
    ""properties"": {
      ""my_vector"": {
        ""type"": ""knn_vector"", 
        ""dimension"": 128,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""lucene"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""confidence_interval"": 0.9
              }
            },
            ""m"": 16
          }
        }
      }
    }
  }
}
```
This uses Lucene scalar quantization with a confidence interval of 0.9 for a vector field `my_vector` of dimension 128.

3. **Faiss scalar quantization**:
```json
PUT /index
{
  ""settings"": {
    ""index.knn.algo_param.ef_search"": 100  
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector"": {
        ""type"": ""knn_vector"",
        ""dimension"": 512,
        ""method"": {
          ""name"": ""hnsw"", 
          ""engine"": ""faiss"",
          ""space_type"": ""l2"",
          ""parameters"": {
            ""encoder"": {
              ""name"": ""sq"",
              ""parameters"": {
                ""type"": ""fp16"",
                ""clip"": true
              }
            },
            ""m"": 32
          }
        }
      }
    }
  }
}
```
This uses the Faiss SQfp16 scalar quantizer which quantizes 32-bit vectors to 16-bits, with clipping enabled for out-of-range values. It is for a `my_vector` field of dimension 512.

4. **Faiss product quantization**:
```json
PUT /index
{
  ""settings"": {
    ""index.knn.algo_param.ef_search"": 100
  },
  ""mappings"": {
    ""properties"": {
      ""my_vector"": {
        ""type"": ""knn_vector"",
        ""dimension"": 128,
        ""method"": {
          ""name"": ""hnsw"",
          ""engine"": ""faiss"",
          ""space_type"": ""l2"", 
          ""parameters"": {
            ""encoder"": {
              ""name"": ""pq"",
              ""parameters"": {
                ""m"": 16,
                ""code_size"": 8
              }
            },
            ""m"": 16
          }
        }
      }
    }
  }
}
```
This uses Faiss product quantization with m=16 and code_size=8 for a vector field `my_vector` of dimension 128 using the HNSW index.

Let me know if you need any clarification or have additional examples to show!",3.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, demonstrating a solid understanding of the different quantization methods for vectors. The examples given are directly relevant to the question and do not introduce any external details.

Completeness: 1 - The answer covers a good range of quantization techniques, including Lucene byte vectors, Lucene scalar quantization, Faiss scalar quantization, and Faiss product quantization. The explanations for each method are thorough and provide the key technical details that a user would need to understand the differences between the approaches.

Conciseness: 0 - While the response is comprehensive, it could be more concise in its delivery. Some of the explanations, particularly around the Lucene and Faiss quantization methods, contain more detail than necessary to answer the original question. A more streamlined approach would improve the overall flow and readability of the answer.

Accuracy: 1 - The information presented in the response is accurate and aligns well with the context provided. The technical details around the different quantization techniques are correct, and the examples demonstrate a clear understanding of how to implement these methods in OpenSearch.

Clarity: 0 - The answer is quite technical in nature, which may make it challenging for a non-expert audience to fully comprehend. While the explanations are clear, the use of specialized terminology and the depth of the technical details could be simplified to improve the overall clarity and accessibility of the response."
"Would you recommend I use ReRank? If so, what type of model would you recommend for ReRank? ","---
layout: default
title: Reranking search results
parent: Search relevance
has_children: false
nav_order: 60
---

# Reranking search results
Introduced 2.12
{: .label .label-purple }

You can rerank search results using a cross-encoder reranker in order to improve search relevance. To implement reranking, you need to configure a [search pipeline]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/index/) that runs at search time. The search pipeline intercepts search results and applies the [`rerank` processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/) to them. The `rerank` processor evaluates the search results and sorts them based on the new scores provided by the cross-encoder model. 

**PREREQUISITE**<br>
Before configuring a reranking pipeline, you must set up a cross-encoder model. For information about using an OpenSearch-provided model, see [Cross-encoder models]({{site.url}}{{site.baseurl}}/ml-commons-plugin/pretrained-models/#cross-encoder-models). For information about using a custom model, see [Custom local models]({{site.url}}{{site.baseurl}}/ml-commons-plugin/custom-local-models/).
{: .note}

## Running a search with reranking

To run a search with reranking, follow these steps:

1. [Configure a search pipeline](#step-1-configure-a-search-pipeline).
1. [Create an index for ingestion](#step-2-create-an-index-for-ingestion).
1. [Ingest documents into the index](#step-3-ingest-documents-into-the-index).
1. [Search using reranking](#step-4-search-using-reranking).

## Step 1: Configure a search pipeline

Next, configure a search pipeline with a [`rerank` processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/).

The following example request creates a search pipeline with an `ml_opensearch` rerank processor. In the request, provide a model ID for the cross-encoder model and the document fields to use as context:

```json
PUT /_search/pipeline/my_pipeline
{
  ""description"": ""Pipeline for reranking with a cross-encoder"",
  ""response_processors"": [
    {
      ""rerank"": {
        ""ml_opensearch"": {
          ""model_id"": ""gnDIbI0BfUsSoeNT_jAw""
        },
        ""context"": {
          ""document_fields"": [
            ""passage_text""
          ]
        }
      }
    }
  ]
}
```
{% include copy-curl.html %}

For more information about the request fields, see [Request fields]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/#request-fields).

## Step 2: Create an index for ingestion

In order to use the rerank processor defined in your pipeline, create an OpenSearch index and add the pipeline created in the previous step as the default pipeline:

```json
PUT /my-index
{
  ""settings"": {
    ""index.search.default_pipeline"" : ""my_pipeline""
  },
  ""mappings"": {
    ""properties"": {
      ""passage_text"": {
        ""type"": ""text""
      }
    }
  }
}
```
{% include copy-curl.html %}

## Step 3: Ingest documents into the index

To ingest documents into the index created in the previous step, send the following bulk request:

```json
POST /_bulk
{ ""index"": { ""_index"": ""my-index"" } }
{ ""passage_text"" : ""I said welcome to them and we entered the house"" }
{ ""index"": { ""_index"": ""my-index"" } }
{ ""passage_text"" : ""I feel welcomed in their family"" }
{ ""index"": { ""_index"": ""my-index"" } }
{ ""passage_text"" : ""Welcoming gifts are great"" }

```
{% include copy-curl.html %}

## Step 4: Search using reranking

To perform reranking search on your index, use any OpenSearch query and provide an additional `ext.rerank` field:

```json
POST /my-index/_search
{
  ""query"": {
    ""match"": {
      ""passage_text"": ""how to welcome in family""
    }
  },
  ""ext"": {
    ""rerank"": {
      ""query_context"": {
         ""query_text"": ""how to welcome in family""
      }
    }
  }
}
```
{% include copy-curl.html %}

Alternatively, you can provide the full path to the field containing the context. For more information, see [Rerank processor example]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/rerank-processor/#example).

## Using rerank and normalization processors together

When you use a rerank processor in conjunction with a [normalization processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/normalization-processor/) and a hybrid query, the rerank processor alters the final document scores. This is because the rerank processor operates after the normalization processor in the search pipeline.
{: .note}

The processing order is as follows: 

- Normalization processor: This processor normalizes the document scores based on the configured normalization method. For more information, see [Normalization processor]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/normalization-processor/).
- Rerank processor: Following normalization, the rerank processor further adjusts the document scores. This adjustment can significantly impact the final ordering of search results.

This processing order has the following implications:

- Score modification: The rerank processor modifies the scores that were initially adjusted by the normalization processor, potentially leading to different ranking results than initially expected.
- Hybrid queries: In the context of hybrid queries, where multiple types of queries and scoring mechanisms are combined, this behavior is particularly noteworthy. The combined scores from the initial query are normalized first and then reranked, resulting in a two-stage scoring modification.","Based on the context provided, I don't have enough information to definitively recommend whether you should use reranking or what specific model to use. The decision depends on your specific use case, data, and requirements.

However, here are some general points to consider regarding using the reranking feature:

1. Reranking can potentially improve search relevance by taking into account more complex relationships between the query and documents using a cross-encoder model. This can be beneficial if your initial search ranking is not optimal.

2. Implementing reranking requires additional setup and resources, as you need to provision and host a cross-encoder model. This adds complexity compared to just using the default search ranking.

3. The effectiveness of reranking depends on the quality and suitability of the cross-encoder model for your data and use case. You may need to experiment with different pre-trained models or train a custom model.

4. Reranking can impact query performance, as it adds an additional computation step during searching. The impact depends on factors like the model size, number of documents to rerank, etc.

If you decide to use reranking, the documentation suggests considering pre-trained cross-encoder models provided by OpenSearch, or training a custom model if those don't meet your needs. The choice depends on your data domain, available training data, and performance requirements. Without more specifics about your use case, it's difficult to recommend a particular model.

Ultimately, you'll need to evaluate the potential relevance gains from reranking against the added complexity and resource requirements for your specific scenario. Setting up a test environment to experiment with reranking and different models could help make that determination.",3.0,"Context Utilization: 1 - The response stays firmly within the bounds of the provided context, demonstrating a strong understanding of the relevant information. It does not venture into any external details or speculation beyond what is available.

Completeness: 1 - The answer addresses all the key elements of the question, including a thoughtful discussion of the pros and cons of using reranking and the potential models to consider. It covers the core considerations a user would need to make an informed decision.

Conciseness: 0 - While the response provides a comprehensive overview, it could be more concise in its phrasing and delivery. Some of the explanations, though informative, feel a bit lengthy and could be streamlined without losing the essential points.

Accuracy: 1 - The information presented aligns closely with the details and recommendations outlined in the context. The response accurately captures the nuances and considerations around implementing reranking in OpenSearch.

Clarity: 0 - The length and technical nature of the response may make it challenging for a non-technical user to fully grasp all the implications. While the language is clear, the overall density of the answer could benefit from a more user-friendly structure and tone."
How do I set up a tiered cache with an on-heap and disk tier for our index request cache? And what settings should I tweak to make sure it doesn't eat all our resources?,"---
layout: default
title: Tiered cache
parent: Caching
grand_parent: Improving search performance
nav_order: 10
---

# Tiered cache

This is an experimental feature and is not recommended for use in a production environment. For updates on the progress of the feature or if you want to leave feedback, see the associated [GitHub issue](https://github.com/opensearch-project/OpenSearch/issues/10024).    
{: .warning}

A tiered cache is a multi-level cache in which each tier has its own characteristics and performance levels. By combining different tiers, you can achieve a balance between cache performance and size.

## Types of tiered caches

OpenSearch provides an implementation of a `_tiered` spillover `cache_`. This implementation spills any items removed from the upper tiers to the lower tiers of cache. The upper tier, such as the on-heap tier, is smaller in size but offers better latency. The lower tier, such as the disk cache, is larger in size but slower in terms of latency. OpenSearch offers both on-heap and disk tiers. 

## Enabling a tiered cache

To enable a tiered cache, configure the following setting in `opensearch.yml`:

```yaml
opensearch.experimental.feature.pluggable.caching.enabled: true
```
{% include copy.html %}

For more information about ways to enable experimental features, see [Experimental feature flags]({{site.url}}{{site.baseurl}}/install-and-configure/configuring-opensearch/experimental/).

## Installing required plugins

To use tiered caching, install a tiered cache plugin. As of OpenSearch 2.13, the only available cache plugin is the `cache-ehcache` plugin. This plugin provides a disk cache implementation that can be used as a disk tier within a tiered cache. For more information about installing non-bundled plugins, see [Additional plugins]({{site.url}}{{site.baseurl}}/install-and-configure/plugins/#additional-plugins).

A tiered cache will fail to initialize if the `cache-ehcache` plugin is not installed or if disk cache properties are not set. 
{: .warning}

## Tiered cache settings

In OpenSearch 2.14, a request cache can be used in a tiered cache. To begin, configure the following settings in the `opensearch.yml` file.

### Cache store name

To use the OpenSearch-provided tiered spillover cache implementation, set the cache store name to `tiered_spillover`, as shown in the following example:

```yaml
indices.request.cache.store.name: tiered_spillover
```
{% include copy.html %}

### Setting on-heap and disk store tiers

Set the on-heap and disk store tiers to `opensearch_onheap` and `ehcache_disk`, as shown in the following example:

```yaml
indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap
indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk
```
The `opensearch_onheap` setting uses the built-in on-heap cache available in OpenSearch. 

The `ehcache_disk` setting is the disk cache implementation from [Ehcache](https://www.ehcache.org/) and requires installing the `cache-ehcache` plugin.

{% include copy.html %}

### Configuring on-heap and disk stores

The following table lists the cache store settings for the `opensearch_onheap` store.

Setting | Data type | Default | Description
:--- | :--- | :--- | :---
`indices.request.cache.opensearch_onheap.size` | Percentage | 1% of the heap size | The size of the on-heap cache. Optional.
`indices.request.cache.opensearch_onheap.expire` | Time unit | `MAX_VALUE` (disabled) | Specifies a time-to-live (TTL) for the cached results. Optional.

The following table lists the disk cache store settings for the `ehcache_disk` store.

Setting | Data type | Default | Description
:--- | :--- | :--- | :---
`indices.request.cache.ehcache_disk.max_size_in_bytes` | Long | `1073741824` (1 GB)  | Defines the size of the disk cache. Optional.
`indices.request.cache.ehcache_disk.storage.path` | String | `""""` | Defines the storage path for the disk cache. Required.
`indices.request.cache.ehcache_disk.expire_after_access` | Time unit | `MAX_VALUE` (disabled) | Specifies a TTL for the cached results. Optional.
`indices.request.cache.ehcache_disk.alias` | String | `ehcacheDiskCache#INDICES_REQUEST_CACHE` | Specifies an alias for the disk cache. Optional.
`indices.request.cache.ehcache_disk.segments` | Integer | `16` | Defines the number of segments into which the disk cache is separated. Used for concurrency. Optional.
`indices.request.cache.ehcache_disk.concurrency` | Integer | `1` | Defines the number of distinct write queues created for the disk store, where a group of segments shares a write queue. Optional.

### Additional settings for the `tiered_spillover` store

The following table lists additional settings for the `tiered_spillover` store setting.

Setting | Data type | Default | Description
:--- | :--- | :--- | :---
`indices.request.cache.tiered_spillover.disk.store.policies.took_time.threshold` | Time unit | `10ms` | A policy used to determine whether to cache a query into a disk cache based on its took time. This is a dynamic setting. Optional.
`indices.request.cache.tiered_spillover.disk.store.enabled` | Boolean | `True` | Enables or disables the disk cache dynamically within a tiered spillover cache. Note: After disabling a disk cache, entries are not removed automatically and requires the cache to be manually cleared. Optional.

### Delete stale entries settings

The following table lists the settings related to the deletion of stale entries from the cache.

Setting | Data type | Default | Description
:--- | :--- |:--------| :---
`indices.requests.cache.cleanup.staleness_threshold` | String | `0%`    | Defines the percentage of stale keys in the cache post. After identification, all stale cache entries are deleted. Optional.
`indices.requests.cache.cleanup.interval` | Time unit | `1m`  | Defines the frequency at which the request cache's stale entries are deleted. Optional.

## Getting statistics for the `tiered_spillover` store 

To assess the impact of using the tiered spillover cache, use the [Node Stats API]({{site.url}}{{site.baseurl}}/api-reference/nodes-apis/nodes-stats/#caches), as shown in the following example: 

```json
GET /_nodes/stats/caches/request_cache?level=tier
```

","To set up a tiered cache with an on-heap and disk tier for your index request cache in OpenSearch, follow these steps:

1. **Enable the experimental pluggable caching feature**:
   ```yaml
   opensearch.experimental.feature.pluggable.caching.enabled: true
   ```

2. **Install the `cache-ehcache` plugin** to get the disk cache implementation.

3. **Configure the cache store name**:
   ```yaml
   indices.request.cache.store.name: tiered_spillover
   ```

4. **Set the on-heap and disk store tiers**:
   ```yaml
   indices.request.cache.tiered_spillover.onheap.store.name: opensearch_onheap
   indices.request.cache.tiered_spillover.disk.store.name: ehcache_disk
   ```

5. **Configure the on-heap store settings** (optional):
   - `indices.request.cache.opensearch_onheap.size`: Set the size of the on-heap cache (e.g., 1% of the heap size).
   - `indices.request.cache.opensearch_onheap.expire`: Set a time-to-live (TTL) for cached results.

6. **Configure the disk store settings**:
   - `indices.request.cache.ehcache_disk.max_size_in_bytes`: Set the size of the disk cache (e.g., 1GB).
   - `indices.request.cache.ehcache_disk.storage.path`: Set the storage path for the disk cache (required).
   - `indices.request.cache.ehcache_disk.expire_after_access`: Set a TTL for cached results (optional).

7. **Tweak additional settings to manage resources**:
   - `indices.request.cache.tiered_spillover.disk.store.policies.took_time.threshold`: Set a threshold for caching queries based on their execution time.
   - `indices.request.cache.tiered_spillover.disk.store.enabled`: Enable or disable the disk cache dynamically.
   - `indices.requests.cache.cleanup.staleness_threshold`: Set the percentage of stale keys to trigger cache cleanup.
   - `indices.requests.cache.cleanup.interval`: Set the frequency for cleaning up stale cache entries.

By configuring these settings, you can control the size and behavior of the on-heap and disk tiers, as well as manage the cache's resource usage. It's recommended to monitor the cache's performance and adjust the settings as needed for your specific use case.",3.0,"Context Utilization: 1 - The response stays firmly grounded in the context provided, referencing the relevant settings and configurations without introducing any extraneous information. This demonstrates a strong understanding of the task at hand and the ability to extract the necessary details from the given materials.

Completeness: 1 - The answer covers all the key elements required to set up a tiered cache with an on-heap and disk tier, including the necessary steps, configuration settings, and suggestions for managing resource usage. The level of detail provided ensures that the reader would have a comprehensive understanding of the process.

Conciseness: 0 - While the response is thorough, it could be more concise in its presentation. Some of the information, such as the detailed descriptions of each setting, could be condensed or presented in a more streamlined manner without sacrificing the overall quality of the answer.

Accuracy: 1 - The information provided in the response is accurate and aligns closely with the context. The steps and settings described are consistent with the documentation, and there are no factual errors or misleading statements.

Clarity: 1 - The response is well-organized and easy to follow, with clear explanations of the various configuration options and their purposes. The step-by-step approach helps guide the reader through the process, and the additional suggestions for managing resource usage further enhance the overall clarity of the answer."
I need to combine relevance scores from different types of searches.,"---
layout: default
title: Hybrid
parent: Compound queries
nav_order: 70
---

# Hybrid query

You can use a hybrid query to combine relevance scores from multiple queries into one score for a given document. A hybrid query contains a list of one or more queries and independently calculates document scores at the shard level for each subquery. The subquery rewriting is performed at the coordinating node level in order to avoid duplicate computations.

## Example

Learn how to use the `hybrid` query by following the steps in [Using hybrid search]({{site.url}}{{site.baseurl}}/search-plugins/hybrid-search/#using-hybrid-search).

For a comprehensive example, follow the [Neural search tutorial]({{site.url}}{{site.baseurl}}/ml-commons-plugin/semantic-search#tutorial).

## Parameters

The following table lists all top-level parameters supported by `hybrid` queries.

Parameter | Description
:--- | :---
`queries` | An array of one or more query clauses that are used to match documents. A document must match at least one query clause in order to be returned in the results. The documents' relevance scores from all query clauses are combined into one score by applying a [search pipeline]({{site.url}}{{site.baseurl}}/search-plugins/search-pipelines/index/). The maximum number of query clauses is 5. Required.

## Disabling hybrid queries

By default, hybrid queries are enabled. To disable hybrid queries in your cluster, set the `plugins.neural_search.hybrid_search_disabled` setting to `true` in `opensearch.yml`. ","Based on the context provided, it seems that you can use a hybrid query in OpenSearch to combine relevance scores from multiple queries into one score for a given document. A hybrid query contains a list of one or more queries, and it independently calculates document scores at the shard level for each subquery. The subquery rewriting is performed at the coordinating node level to avoid duplicate computations.

The context mentions that a hybrid query is useful when you need to combine relevance scores from different types of searches. By specifying multiple queries in the `queries` parameter of the hybrid query, you can combine the relevance scores from those queries into a single score for each document.",4.0,"Context Utilization: 1 - The response stays firmly grounded within the provided context, drawing upon the key details and examples to address the question. The answer does not venture into any unsubstantiated territory, demonstrating a strong grasp of the relevant information.

Completeness: 1 - The response comprehensively covers the main aspects of using a hybrid query to combine relevance scores from different types of searches. It explains the purpose and functionality of the hybrid query, as well as the key parameters involved, providing a thorough explanation.

Conciseness: 0 - While the response is generally clear and well-structured, there are a few instances where the phrasing could be more concise. For example, the sentence ""The subquery rewriting is performed at the coordinating node level in order to avoid duplicate computations"" could be shortened to ""The subquery rewriting is performed at the coordinating node level to avoid duplicate computations."" Some minor streamlining of the language could improve the overall conciseness.

Accuracy: 1 - All the information provided in the answer is accurate and aligns closely with the details presented in the given context. The response demonstrates a solid understanding of the hybrid query functionality and its application.

Clarity: 1 - The response is well-organized and easy to follow, with a logical flow that guides the reader through the key points. The language used is clear and accessible, making the explanation easy to comprehend."
What metrics does query metrics give me out of the box? ,"---
layout: default
title: Query metrics
parent: Query insights
nav_order: 20
---

# Query metrics

Key query [metrics](#metrics), such as aggregation types, query types, latency, and resource usage per query type, are captured along the search path by using the OpenTelemetry (OTel) instrumentation framework. The telemetry data can be consumed using OTel metrics [exporters]({{site.url}}{{site.baseurl}}/observing-your-data/trace/distributed-tracing/#exporters).

## Configuring query metric generation

To configure query metric generation, use the following steps.

### Step 1: Install the Query Insights plugin

For information about installing the Query Insights plugin, see [Installing the Query Insights plugin]({{site.url}}{{site.baseurl}}/observing-your-data/query-insights/index/#installing-the-query-insights-plugin).

### Step 2: Install the OpenTelemetry plugin

For information about installing the OpenTelemetry plugin, see [Distributed tracing]({{site.url}}{{site.baseurl}}/observing-your-data/trace/distributed-tracing/).

### Step 3: Enable query metrics

Enable query metrics by configuring the following `opensearch.yml` settings:

```yaml
telemetry.feature.metrics.enabled: true
search.query.metrics.enabled: true
```
{% include copy.html %}

The following is a complete sample configuration that includes a telemetry configuration:

```yaml
# Enable query metrics feature
search.query.metrics.enabled: true
telemetry.feature.metrics.enabled: true

# OTel-related configuration
opensearch.experimental.feature.telemetry.enabled: true
telemetry.tracer.sampler.probability: 1.0
telemetry.feature.tracer.enabled: true
```
{% include copy.html %}

Alternatively, you can configure query metric generation using the API:

```json
PUT _cluster/settings
{
  ""persistent"" : {
    ""search.query.metrics.enabled"" : true
  }
}
```
{% include copy-curl.html %}

Configure the export of metrics and traces using a gRPC exporter. For more information, see [Exporters]({{site.url}}{{site.baseurl}}/observing-your-data/trace/distributed-tracing/#exporters). You can skip this step if you use the [default logging exporter](#default-logging-exporter):

```yaml
telemetry.otel.tracer.span.exporter.class: io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter
telemetry.otel.metrics.exporter.class: io.opentelemetry.exporter.otlp.metrics.OtlpGrpcMetricExporter
```
{% include copy.html %}

## Metrics

Query metrics provide the following measurements:

- The number of queries per query type (for example, the number of `match` or `regex` queries)
- The number of queries per aggregation type (for example, the number of `terms` aggregation queries)
- The number of queries per sort order (for example, the number of ascending and descending `sort` queries)
- Histograms of `latency` for each query type, aggregation type, and sort order
- Histograms of `cpu` for each query type, aggregation type, and sort order
- Histograms of `memory` for each query type, aggregation type, and sort order

## Default logging exporter

By default, if no gRPC exporters are configured, then the metrics and traces are exported to log files. The data is saved in the `opensearch/logs` directory in the following files:

- `opensearch_otel_metrics.log`
- `opensearch_otel_traces.log`
","According to the documentation, query metrics provide the following measurements out of the box:

- The number of queries per query type (e.g., number of `match` or `regex` queries)
- The number of queries per aggregation type (e.g., number of `terms` aggregation queries)
- The number of queries per sort order (e.g., number of ascending and descending `sort` queries)
- Histograms of latency for each query type, aggregation type, and sort order
- Histograms of CPU usage for each query type, aggregation type, and sort order
- Histograms of memory usage for each query type, aggregation type, and sort order",4.0,"Context Utilization: 1 - The response stays firmly grounded in the provided context, drawing all its information directly from the documentation. It does not introduce any external or speculative details, which aligns well with the LLM evaluation.

Completeness: 1 - The answer thoroughly covers the key metrics that query metrics provide out of the box, as outlined in the context. It comprehensively addresses the question, leaving no significant gaps in the information presented.

Conciseness: 0 - While the response is clear and informative, it is a bit lengthy and could be streamlined to provide the same level of detail in a more concise manner. The LLM evaluation correctly identifies that the answer could be more concise.

Accuracy: 1 - All the information provided in the response is factually accurate and consistent with the details given in the context. The answer demonstrates a strong understanding of the subject matter.

Clarity: 1 - The response is well-structured and easy to follow, with a logical flow that makes the information readily accessible. The language used is clear and straightforward, aligning with the LLM evaluation's assessment of clarity."
Does opensearch work with open telemetry?,"---
layout: default
title: Metric analytics
nav_order: 40
---

# Metric analytics
Introduced 2.4
{: .label .label-purple }

With the release of OpenSearch 2.4, you can now ingest and visualize metric data stored directly in OpenSearch using the **Metrics** tool. This equips you with tools to analyze and correlate data across logs, traces, and metrics.

Before the introduction of this feature, you could only ingest and visualize logs and traces from your monitored environments. With the **Metrics** tool, you can now observe your digital assets with more granularity, gain deeper insight into the health of your infrastructure, and better inform your root cause analysis.

The **Metrics** tool offers federated visualization capabilities in addition to the following:

 - An OpenSearch cluster containing an [OpenTelemetry (OTel)-compatible metrics index](https://github.com/opensearch-project/opensearch-catalog/tree/main/docs/schema/observability/metrics) with OTel-based signals. See [What is OpenTelemetry?](https://opentelemetry.io/docs/what-is-opentelemetry/) for an overview of OTel.
 - An OpenSearch cluster containing a [Prometheus data source](https://github.com/opensearch-project/sql/blob/main/docs/dev/datasource-prometheus.md) connected to a Prometheus server. 

The following image displays the flow for retrieving metrics from Prometheus and displaying them on a visualization dashboard.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/prom-metrics.png"" alt=""Prometheus data source"" width=""700""/>

The following image displays an observability dashboard that visualizes metric data from the OpenSearch index using OTel queries.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/otel-metrics.png"" alt=""OTel data source"" width=""700""/>

---

## Configuring Prometheus to send metric data to OpenSearch

You must first create a connection from [Prometheus](https://prometheus.io/) to OpenSearch using the [SQL plugin](https://github.com/opensearch-project/sql). You can then configure a connection to Prometheus by using the `_datasources` API endpoint. 

The following example shows a request that configures a Prometheus data source without any authentication:

```json
POST _plugins/_query/_datasources 
{
    ""name"" : ""my_prometheus"",
    ""connector"": ""prometheus"",
    ""properties"" : {
        ""prometheus.uri"" : ""http://localhost:9090""
    }
}
```
{% include copy-curl.html %}

The following example shows how to configure a Prometheus data source using AWS Signature Version 4 authentication:

```json
POST _plugins/_query/_datasources
{
    ""name"" : ""my_prometheus"",
    ""connector"": ""prometheus"",
    ""properties"" : {
        ""prometheus.uri"" : ""http://localhost:8080"",
        ""prometheus.auth.type"" : ""awssigv4"",
        ""prometheus.auth.region"" : ""us-east-1"",
        ""prometheus.auth.access_key"" : ""{{accessKey}}""
        ""prometheus.auth.secret_key"" : ""{{secretKey}}""
    }
}
```
{% include copy-curl.html %}

After configuring the connection, you can view Prometheus metrics in OpenSearch Dashboards by going to the **Observability** > **Metrics** page, as shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/metrics1.png"" alt=""Prometheus metrics displayed on a dashboard"" width=""700""/>

### Developer resources

See the following developer resources for sample code, articles, tutorials, and API references:

* [Datasource Settings](https://github.com/opensearch-project/sql/blob/main/docs/user/ppl/admin/datasources.rst), which contains information about authentication and authorization of data source APIs.
* [Prometheus Connector](https://github.com/opensearch-project/sql/blob/main/docs/user/ppl/admin/connectors/prometheus_connector.rst), which contains configuration information.
* [Simple Schema for Observability](https://github.com/opensearch-project/opensearch-catalog/tree/main/docs/schema/observability), which contains information about the OTel schema and ingest pipeline.
* [OTel Metrics Source](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/otel-metrics-source), which contains information about the Data Prepper metrics pipeline and ingestion.

---

## Experimenting with OpenTelemetry Metrics in the OpenSearch demo environment 

The OpenSearch [`opentelemetry-demo` repository](https://github.com/opensearch-project/opentelemetry-demo) provides a practical demonstration of collecting, processing, and visualizing metric data through **OpenTelemetry Metrics** from OpenTelemetry and using the **Metrics** tool in OpenSearch Dashboards.

### Visualizing OTel metrics in OpenSearch

To visualize OTel metric data in OpenSearch, follow these steps: 

1. Install the [`opentelemetry-demo` repository](https://github.com/opensearch-project/opentelemetry-demo). See the [Getting Started](https://github.com/opensearch-project/opentelemetry-demo/blob/main/tutorial/GettingStarted.md) guide for instructions.
2. Collect the OTel signals, including metric signals. See the [OTel Collector](https://opentelemetry.io/docs/collector/) guide for instructions.  
3. Configure the OTel pipeline to emit metric signals. See the [OTel Collector Pipeline](https://github.com/opensearch-project/opentelemetry-demo/tree/main/src/otelcollector) guide for instructions.

#### Example YAML config file

```yaml
    service:
      extensions: [basicauth/client]
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp, debug, spanmetrics, otlp/traces, opensearch/traces]
        metrics:
          receivers: [otlp, spanmetrics]
          processors: [filter/ottl, transform, batch]
          exporters: [otlphttp/prometheus, otlp/metrics, debug]
        logs:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp/logs,  opensearch/logs, debug]
```
{% include copy-curl.html %}
    
4. Configure the [Data Prepper pipeline](https://github.com/opensearch-project/opentelemetry-demo/blob/main/src/dataprepper/pipelines.yaml) to emit the collected metric signals into the OpenSearch metrics index.

#### Example YAML config file

```yaml
    otel-metrics-pipeline:
      workers: 8
      delay: 3000
      source:
        otel_metrics_source:
          health_check_service: true
          ssl: false
      buffer:
        bounded_blocking:
          buffer_size: 1024 # max number of records the buffer accepts
          batch_size: 1024 # max number of records the buffer drains after each read
      processor:
        - otel_metrics:
            calculate_histogram_buckets: true
            calculate_exponential_histogram_buckets: true
            exponential_histogram_max_allowed_scale: 10
            flatten_attributes: false
      sink:
        - opensearch:
            hosts: [""https://opensearch-node1:9200""]
            username: ""admin""
            password: ""my_%New%_passW0rd!@#""
            insecure: true
            index_type: custom
            template_file: ""templates/ss4o_metrics.json""
            index: ss4o_metrics-otel-%{yyyy.MM.dd}
            bulk_size: 4
```
{% include copy-curl.html %}

5. Ingest metric data into OpenSearch. As the demo starts generating data, the metric signals will be added to the OpenSearch index that supports the OpenTelemetry Metrics schema format.
6. On the **Metrics** page, choose `Otel-Index` from the **Data sources** dropdown menu and `Simple Schema for Observability Index` from the **OTel index** dropdown menu. A visualization is displayed, as shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/otel-metrics.png"" alt=""OTel metrics dashboard"" width=""700""/>

---

## Visualizing metrics in remote clusters
Introduced 2.14
{: .label .label-purple }

You can view metrics from remote OpenSearch clusters by using the **Metrics** tool. Select the database icon on the upper-right toolbar and choose a cluster from the **DATA SOURCES** dropdown menu, as shown in the following image. You can switch from a local cluster to a remote cluster.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/remote-cluster-selection.png"" alt=""Switching clusters using the Metrics analytics tool"" width=""700""/>

You can also view metric visualizations from other sources alongside local metric visualizations. From the **DATA SOURCES** dropdown menu, choose the remote metric visualization to add it to the group of visualizations already shown on the dashboard. An example dashboard is shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/otel-metrics-remote-cluster-selection.png"" alt=""Metrics dashboard"" width=""700""/>

To learn about multi-cluster support for data sources, see [Enable OpenSearch Dashboards to support multiple OpenSearch clusters](https://github.com/opensearch-project/OpenSearch-Dashboards/issues/1388).

## Creating visualizations based on custom metrics

You can create visualizations using the metric data collected by your OpenSearch cluster, including Prometheus metrics and custom metrics.

To create these visualizations, follow these steps:

1. From the OpenSearch Dashboards main menu, navigate to **Observability** > **Metrics** > **Available Metrics**.
2. Choose the metrics to add to your visualization and then select **Save**.
3. When prompted for a **Custom operational dashboards/application**, choose one of the listed options. You can edit the predefined name values in the **Metric Name** field.
4. Select **Save** to save your visualization. An example visualization is shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/metrics2.png"" alt=""Metrics analytics dashboard with visualizations"" width=""700""/>

## Defining PPL queries for Prometheus metrics

You can define [Piped Processing Language (PPL)]({{site.url}}{{site.baseurl}}/search-plugins/sql/ppl/index/) queries to interact with metrics collected by Prometheus. The following is an example PPL query for a Prometheus metric:

```
source = my_prometheus.prometheus_http_requests_total | stats avg(@value) by span(@timestamp,15s), handler, code
```
{% include copy-curl.html %}

### Creating a custom visualization based on the PPL query

To create a custom visualization based on the PPL query, follow these steps:

1. From the **Logs** page, select > **Event Explorer**.
2. On the **Explorer** page,  enter your PPL query and select **Run**. Then select **Save**.
3. When prompted to choose a **Custom Operational Dashboards/Application**, select one of the listed options. Optionally, you can edit the predefined name values in the **Metric Name** fields and can choose to save the visualization as a metric.
5. Select **Save** to save your custom visualization. 

Only queries that include a time-series visualization and statistics or span information can be saved as a metric, as shown in the following image.

<img src=""{{site.url}}{{site.baseurl}}/images/metrics/metrics3.png"" alt=""Saving queries as metrics"" width=""700""/>
","Yes, OpenSearch does work with OpenTelemetry (OTel). The documentation you provided mentions that with the release of OpenSearch 2.4, you can ingest and visualize metric data stored directly in OpenSearch using the Metrics tool. This includes support for an OpenSearch cluster containing an OTel-compatible metrics index with OTel-based signals.

The documentation explains how you can set up an OpenSearch demo environment using the `opentelemetry-demo` repository to collect, process, and visualize metric data through OpenTelemetry Metrics. It involves configuring the OTel Collector pipeline to emit metric signals, and the Data Prepper pipeline to ingest those metric signals into an OpenSearch metrics index that supports the OpenTelemetry Metrics schema format. Once the data is ingested, you can visualize the OTel metrics on the Metrics page in OpenSearch Dashboards.

So in summary, OpenSearch integrates with OpenTelemetry to allow ingesting, storing, and visualizing metrics collected through the OpenTelemetry instrumentation and collection system.",5.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, demonstrating a solid understanding of the topic at hand. It does not introduce any extraneous details or speculate beyond the given information.

Completeness: 1 - The answer thoroughly addresses the key aspects of the question, explaining how OpenSearch integrates with OpenTelemetry to ingest, store, and visualize metric data. It covers the necessary steps and configurations, providing a comprehensive overview of the integration process.

Conciseness: 1 - The response is concise and efficient in its delivery, conveying the relevant information without unnecessary verbosity. The pacing and structure of the answer allow the reader to follow the explanation easily.

Accuracy: 1 - All the details provided in the answer are factually correct and align with the information presented in the context. The response accurately reflects the capabilities and integration points between OpenSearch and OpenTelemetry.

Clarity: 1 - The answer is well-structured and easy to understand, with clear explanations and logical flow. The technical concepts are explained in a manner that is accessible to the reader, making the integration process between the two systems clear and understandable."
How do I update an existing Slack channel configuration to change its webhook URL and description using the Notifications API?,"---
layout: default
title: API
nav_order: 50
parent: Notifications
redirect_from:
  - /notifications-plugin/api/
---

# Notifications API

If you want to programmatically define your notification channels and sources for versioning and reuse, you can use the Notifications REST API to define, configure, and delete notification channels and send test messages.

---

#### Table of contents
1. TOC
{:toc}

---

## List supported channel configurations

To retrieve a list of all supported notification configuration types, send a GET request to the `features` resource.

#### Example request

```json
GET /_plugins/_notifications/features
```

#### Example response

```json
{
  ""allowed_config_type_list"" : [
    ""slack"",
    ""chime"",
    ""webhook"",
    ""email"",
    ""sns"",
    ""ses_account"",
    ""smtp_account"",
    ""email_group""
  ],
  ""plugin_features"" : {
    ""tooltip_support"" : ""true""
  }
}
```

## List all notification channels

To retrieve a list of all notification channels, send a GET request to the `channels` resource.

#### Example request

```json
GET /_plugins/_notifications/channels
```

#### Example response

```json
{
  ""start_index"" : 0,
  ""total_hits"" : 2,
  ""total_hit_relation"" : ""eq"",
  ""channel_list"" : [
    {
      ""config_id"" : ""sample-id"",
      ""name"" : ""Sample Slack Channel"",
      ""description"" : ""This is a Slack channel"",
      ""config_type"" : ""slack"",
      ""is_enabled"" : true
    },
    {
      ""config_id"" : ""sample-id2"",
      ""name"" : ""Test chime channel"",
      ""description"" : ""A test chime channel"",
      ""config_type"" : ""chime"",
      ""is_enabled"" : true
    }
  ]
}
```

## List all notification configurations

To retrieve a list of all notification configurations, send a GET request to the `configs` resource.

#### Example request

```json
GET _plugins/_notifications/configs
```

#### Example response

```json
{
  ""start_index"" : 0,
  ""total_hits"" : 2,
  ""total_hit_relation"" : ""eq"",
  ""config_list"" : [
    {
      ""config_id"" : ""sample-id"",
      ""last_updated_time_ms"" : 1652760532774,
      ""created_time_ms"" : 1652760532774,
      ""config"" : {
        ""name"" : ""Sample Slack Channel"",
        ""description"" : ""This is a Slack channel"",
        ""config_type"" : ""slack"",
        ""is_enabled"" : true,
        ""slack"" : {
          ""url"" : ""https://sample-slack-webhook""
        }
      }
    },
    {
      ""config_id"" : ""sample-id2"",
      ""last_updated_time_ms"" : 1652760735380,
      ""created_time_ms"" : 1652760735380,
      ""config"" : {
        ""name"" : ""Test chime channel"",
        ""description"" : ""A test chime channel"",
        ""config_type"" : ""chime"",
        ""is_enabled"" : true,
        ""chime"" : {
          ""url"" : ""https://sample-chime-webhook""
        }
      }
    }
  ]
}
```

To filter the notification configuration types this request returns, you can refine your query with the following optional path parameters.

Parameter	| Description
:--- | :---
config_id | Specifies the channel identifier.
config_id_list | Specifies a comma-separated list of channel IDs.
from_index | The starting index to search from.
max_items | The maximum amount of items to return in your request.
sort_order | Specifies the direction to sort results in. Valid options are `asc` and `desc`.
sort_field | Field to sort results with.
last_updated_time_ms | The Unix time in milliseconds of when the channel was last updated.
created_time_ms | The Unix time in milliseconds of when the channel was created.
is_enabled | Indicates whether the channel is enabled.
config_type | The channel type. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`.
name | The channel name.
description	| The channel description.
email.email_account_id | The sender email addresses the channel uses.
email.email_group_id_list | The email groups the channel uses.
email.recipient_list | The channel recipient list.
email_group.recipient_list | The channel list of email recipient groups.
smtp_account.method | The email encryption method.
slack.url	| The Slack channel URL.
chime.url	| The Amazon Chime connection URL.
webhook.url	| The webhook URL.
smtp_account.host	| The domain of the SMTP account.
smtp_account.from_address	| The email account's sender address.
smtp_account.method | The SMTP account's encryption method.
sns.topic_arn	| The Amazon Simple Notification Service (SNS) topic's ARN.
sns.role_arn | The Amazon SNS topic's role ARN.
ses_account.region | The Amazon Simple Email Service (SES) account's AWS Region.
ses_account.role_arn | The Amazon SES account's role ARN.
ses_account.from_address | The Amazon SES account's sender email address.

## Create channel configuration

To create a notification channel configuration, send a POST request to the `configs` resource.

#### Example request

```json
POST /_plugins/_notifications/configs/
{
  ""config_id"": ""sample-id"",
  ""name"": ""sample-name"",
  ""config"": {
    ""name"": ""Sample Slack Channel"",
    ""description"": ""This is a Slack channel"",
    ""config_type"": ""slack"",
    ""is_enabled"": true,
    ""slack"": {
      ""url"": ""https://sample-slack-webhook""
    }
  }
}
```

The create channel API operation accepts the following fields in its request body:

Field |	Data type |	Description |	Required
:--- | :--- | :--- | :---
config_id | String | The configuration's custom ID. | No
config | Object |	Contains all relevant information, such as channel name, configuration type, and plugin source. |	Yes
name | String |	Name of the channel. | Yes
description |	String | The channel's description. | No
config_type |	String | The destination of your notification. Valid options are `sns`, `slack`, `chime`, `webhook`, `smtp_account`, `ses_account`, `email_group`, and `email`. | Yes
is_enabled | Boolean | Indicates whether the channel is enabled for sending and receiving notifications. Default is `true`.	| No

The create channel operation accepts multiple `config_types` as possible notification destinations, so follow the format for your preferred `config_type`.

```json
""sns"": {
  ""topic_arn"": ""<arn>"",
  ""role_arn"": ""<arn>"" //optional
}
""slack"": {
  ""url"": ""https://sample-chime-webhoook""
}
""chime"": {
  ""url"": ""https://sample-amazon-chime-webhoook""
}
""webhook"": {
      ""url"": ""https://custom-webhook-test-url.com:8888/test-path?params1=value1&params2=value2""
}
""smtp_account"": {
  ""host"": ""test-host.com"",
  ""port"": 123,
  ""method"": ""start_tls"",
  ""from_address"": ""test@email.com""
}
""ses_account"": {
  ""region"": ""us-east-1"",
  ""role_arn"": ""arn:aws:iam::012345678912:role/NotificationsSESRole"",
  ""from_address"": ""test@email.com""
}
""email_group"": { //Email recipient group
  ""recipient_list"": [
    {
      ""recipient"": ""test-email1@test.com""
    },
    {
      ""recipient"": ""test-email2@test.com""
    }
  ]
}
""email"": { //The channel that sends emails
  ""email_account_id"": ""<smtp or ses account config id>"",
  ""recipient_list"": [
    {
      ""recipient"": ""custom.email@test.com""
    }
  ],
  ""email_group_id_list"": []
}
```

The following example demonstrates how to create a channel using email as a `config_type`:

```json
POST /_plugins/_notifications/configs/
{
  ""id"": ""sample-email-id"",
  ""name"": ""sample-name"",
  ""config"": {
    ""name"": ""Sample Email Channel"",
    ""description"": ""Sample email description"",
    ""config_type"": ""email"",
    ""is_enabled"": true,
    ""email"": {
      ""email_account_id"": ""<email_account_id>"",
      ""recipient_list"": [
        ""sample@email.com""
      ]
    }
  }
}
```

#### Example response

```json
{
  ""config_id"" : ""<config_id>""
}
```


## Get channel configuration

To get a channel configuration by `config_id`, send a GET request and specify the `config_id` as a path parameter.

#### Example request

```json
GET _plugins/_notifications/configs/<config_id>
```

#### Example response

```json
{
  ""start_index"" : 0,
  ""total_hits"" : 1,
  ""total_hit_relation"" : ""eq"",
  ""config_list"" : [
    {
      ""config_id"" : ""sample-id"",
      ""last_updated_time_ms"" : 1652760532774,
      ""created_time_ms"" : 1652760532774,
      ""config"" : {
        ""name"" : ""Sample Slack Channel"",
        ""description"" : ""This is a Slack channel"",
        ""config_type"" : ""slack"",
        ""is_enabled"" : true,
        ""slack"" : {
          ""url"" : ""https://sample-slack-webhook""
        }
      }
    }
  ]
}
```


## Update channel configuration

To update a channel configuration, send a POST request to the `configs` resource and specify the channel's `config_id` as a path parameter. Specify the new configuration details in the request body.

#### Example request

```json
PUT _plugins/_notifications/configs/<config_id>
{
  ""config"": {
    ""name"": ""Slack Channel"",
    ""description"": ""This is an updated channel configuration"",
    ""config_type"": ""slack"",
    ""is_enabled"": true,
    ""slack"": {
      ""url"": ""https://hooks.slack.com/sample-url""
    }
  }
}
```

#### Example response

```json
{
  ""config_id"" : ""<config_id>""
}
```


## Delete channel configuration

To delete a channel configuration, send a DELETE request to the `configs` resource and specify the `config_id` as a path parameter.

#### Example request

```json
DELETE /_plugins/_notifications/configs/<config_id>
```

#### Example response

```json
{
  ""delete_response_list"" : {
  ""<config_id>"" : ""OK""
  }
}
```

You can also submit a comma-separated list of channel IDs you want to delete, and OpenSearch deletes all of the specified notification channels.

#### Example request

```json
DELETE /_plugins/_notifications/configs/?config_id_list=<config_id1>,<config_id2>,<config_id3>...
```

#### Example response

```json
{
  ""delete_response_list"" : {
  ""<config_id1>"" : ""OK"",
  ""<config_id2>"" : ""OK"",
  ""<config_id3>"" : ""OK""
  }
}
```


## Send test notification

To send a test notification, send a GET request to `/feature/test/` and specify the channel configuration's `config_id` as a path parameter.

#### Example request

```json
GET _plugins/_notifications/feature/test/<config_id>
```

#### Example response

```json
{
  ""event_source"" : {
    ""title"" : ""Test Message Title-0Jnlh4ABa4TCWn5C5H2G"",
    ""reference_id"" : ""0Jnlh4ABa4TCWn5C5H2G"",
    ""severity"" : ""info"",
    ""tags"" : [ ]
  },
  ""status_list"" : [
    {
      ""config_id"" : ""0Jnlh4ABa4TCWn5C5H2G"",
      ""config_type"" : ""slack"",
      ""config_name"" : ""sample-id"",
      ""email_recipient_status"" : [ ],
      ""delivery_status"" : {
        ""status_code"" : ""200"",
        ""status_text"" : """"""<!doctype html>
<html>
<head>
</head>
<body>
<div>
    <h1>Example Domain</h1>
    <p>Sample paragraph.</p>
    <p><a href=""sample.example.com"">TO BE OR NOT TO BE, THAT IS THE QUESTION</a></p>
</div>
</body>
</html>
""""""
      }
    }
  ]
}

```
","To update an existing Slack channel configuration to change its webhook URL and description using the Notifications API, you can send a PUT request to the `/_plugins/_notifications/configs/<config_id>` endpoint with the updated configuration details in the request body.

Here's an example request:

```json
PUT /_plugins/_notifications/configs/<config_id>
{
  ""config"": {
    ""name"": ""Updated Slack Channel"",
    ""description"": ""This is the updated description for the Slack channel"",
    ""config_type"": ""slack"",
    ""is_enabled"": true,
    ""slack"": {
      ""url"": ""https://hooks.slack.com/services/new-webhook-url""
    }
  }
}
```

Replace `<config_id>` with the actual configuration ID of the Slack channel you want to update.

In the request body, update the following fields:

- `name`: The new name for the Slack channel.
- `description`: The new description for the Slack channel.
- `slack.url`: The new webhook URL for the Slack channel.

Make sure to keep the `config_type` as `""slack""` and set `is_enabled` to `true` if you want the channel to be enabled.

The response will contain the `config_id` of the updated configuration:

```json
{
  ""config_id"" : ""<config_id>""
}
```

After sending this request, the Slack channel configuration will be updated with the new webhook URL and description.",3.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, demonstrating a strong understanding of the Notifications API and how to update a Slack channel configuration. The answer does not introduce any extraneous details or speculate beyond the scope of the given context.

Completeness: 1 - The response thoroughly addresses all the key elements of the question, including the specific steps required to update the Slack channel's webhook URL and description using the Notifications API. The step-by-step instructions and example request/response details ensure the user has a comprehensive understanding of the process.

Conciseness: 0 - While the response is detailed and comprehensive, it could be slightly more concise. The lengthy example request and response sections, though informative, could be condensed to focus more on the core steps required to update the channel configuration.

Accuracy: 1 - The information provided in the answer is accurate and aligns perfectly with the context. The example request and response formats, as well as the specific fields that need to be updated, are all correct based on the Notifications API documentation.

Clarity: 1 - The response is well-structured and easy to follow, with clear explanations and a logical flow. The use of code examples and JSON formatting helps to illustrate the process and make the instructions easy to understand."
"What specific Java permission is required to register and unregister MBeans when installing the anomaly detection plugin, and what exact object does this permission apply to?","---
layout: default
title: Installing plugins
nav_order: 90
has_children: true
redirect_from:
   - /opensearch/install/plugins/
   - /install-and-configure/install-opensearch/plugins/
---

# Installing plugins

OpenSearch comprises of a number of plugins that add features and capabilities to the core platform. The plugins available to you are dependent on how OpenSearch was installed and which plugins were subsequently added or removed. For example, the minimal distribution of OpenSearch enables only core functionality, such as indexing and search. Using the minimal distribution of OpenSearch is beneficial when you are working in a testing environment, have custom plugins, or are intending to integrate OpenSearch with other services.

The standard distribution of OpenSearch has much more functionality included. You can choose to add additional plugins or remove any of the plugins you don't need. 

For a list of the available plugins, see [Available plugins](#available-plugins).

For a plugin to work properly with OpenSearch, it may request certain permissions as part of the installation process. Review the requested permissions and proceed accordingly. It is important that you understand a plugin's functionality before installation. When opting for a community-provided plugin, ensure that the source is trustworthy and reliable.
{: .warning}

## Managing plugins

To manage plugins in OpenSearch, you can use a command line tool called `opensearch-plugin`. This tool allows you to perform the following actions:

- [List](#list) installed plugins.
- [Install](#install) plugins.
- [Remove](#remove) an installed plugin.

You can print help text by passing `-h` or `--help`. Depending on your host configuration, you might also need to run the command with `sudo` privileges.

If you're running OpenSearch in a Docker container, plugins must be installed, removed, and configured by modifying the Docker image. For more information, see [Working with plugins]({{site.url}}{{site.baseurl}}/install-and-configure/install-opensearch/docker#working-with-plugins).
{: .note}

## List

Use `list` to see a list of plugins that have already been installed.

#### Usage
```bash
bin/opensearch-plugin list
```

#### Example
```bash
$ ./opensearch-plugin list
opensearch-alerting
opensearch-anomaly-detection
opensearch-asynchronous-search
opensearch-cross-cluster-replication
opensearch-geospatial
opensearch-index-management
opensearch-job-scheduler
opensearch-knn
opensearch-ml
opensearch-notifications
opensearch-notifications-core
opensearch-observability
opensearch-performance-analyzer
opensearch-reports-scheduler
opensearch-security
opensearch-sql
```

## List (with CAT API)
You can also list installed plugins by using the [CAT API]({{site.url}}{{site.baseurl}}/api-reference/cat/cat-plugins/).

#### Usage

```bash
GET _cat/plugins
```

#### Example response

```bash
opensearch-node1 opensearch-alerting                  2.0.1.0
opensearch-node1 opensearch-anomaly-detection         2.0.1.0
opensearch-node1 opensearch-asynchronous-search       2.0.1.0
opensearch-node1 opensearch-cross-cluster-replication 2.0.1.0
opensearch-node1 opensearch-index-management          2.0.1.0
opensearch-node1 opensearch-job-scheduler             2.0.1.0
opensearch-node1 opensearch-knn                       2.0.1.0
opensearch-node1 opensearch-ml                        2.0.1.0
opensearch-node1 opensearch-notifications             2.0.1.0
opensearch-node1 opensearch-notifications-core        2.0.1.0
```

## Install

There are three ways to install plugins using the `opensearch-plugin` tool:

- [Install a plugin by name](#install-a-plugin-by-name).
- [Install a plugin from a zip file](#install-a-plugin-from-a-zip-file).
- [Install a plugin using Maven coordinates](#install-a-plugin-using-maven-coordinates).

### Install a plugin by name

You can install plugins that aren't already preinstalled in your installation by using the plugin name. For a list of plugins that may not be preinstalled, see [Additional plugins](#additional-plugins).

#### Usage
```bash
bin/opensearch-plugin install <plugin-name>
```

#### Example
```bash
$ sudo ./opensearch-plugin install analysis-icu
-> Installing analysis-icu
-> Downloading analysis-icu from opensearch
[=================================================] 100%   
-> Installed analysis-icu with folder name analysis-icu
```

### Install a plugin from a zip file

You can install remote zip files by replacing `<zip-file>` with the URL of the hosted file. The tool supports downloading over HTTP/HTTPS protocols only. For local zip files, replace `<zip-file>` with `file:` followed by the absolute or relative path to the plugin zip file, as shown in the second example that follows.

#### Usage
```bash
bin/opensearch-plugin install <zip-file>
```

#### Example
<details markdown=""block"">
  <summary>
    Select to expand the example 
  </summary>
  {: .text-delta}
  
```bash
# Zip file is hosted on a remote server - in this case, Maven central repository.
$ sudo ./opensearch-plugin install https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip
-> Installing https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip
-> Downloading https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip
[=================================================] 100%   
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.misc
* java.lang.RuntimePermission accessDeclaredMembers
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.reflect.ReflectPermission suppressAccessChecks
* java.net.SocketPermission * connect,resolve
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean
* javax.management.MBeanServerPermission createMBeanServer
* javax.management.MBeanTrustPermission register
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection

# Zip file in a local directory.
$ sudo ./opensearch-plugin install file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip
-> Installing file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip
-> Downloading file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip
[=================================================] 100%   
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.misc
* java.lang.RuntimePermission accessDeclaredMembers
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.reflect.ReflectPermission suppressAccessChecks
* java.net.SocketPermission * connect,resolve
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean
* javax.management.MBeanServerPermission createMBeanServer
* javax.management.MBeanTrustPermission register
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection
```
</details>

### Install a plugin using Maven coordinates

The `opensearch-plugin install` tool also allows you to specify Maven coordinates for available artifacts and versions hosted on [Maven Central](https://search.maven.org/search?q=org.opensearch.plugin). The tool parses the Maven coordinates you provide and constructs a URL. As a result, the host must be able to connect directly to the Maven Central site. The plugin installation fails if you pass coordinates to a proxy or local repository.

#### Usage
```bash
bin/opensearch-plugin install <groupId>:<artifactId>:<version>
```

#### Example

<details markdown=""block"">
  <summary>
    Select to expand the example 
  </summary>
  {: .text-delta}

```console
$ sudo ./opensearch-plugin install org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0
-> Installing org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0
-> Downloading org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 from maven central
[=================================================] 100%   
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.misc
* java.lang.RuntimePermission accessDeclaredMembers
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.reflect.ReflectPermission suppressAccessChecks
* java.net.SocketPermission * connect,resolve
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean
* javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean
* javax.management.MBeanServerPermission createMBeanServer
* javax.management.MBeanTrustPermission register
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection
```
</details>

Restart your OpenSearch node after installing a plugin.
{: .note}

## Installing multiple plugins

Multiple plugins can be installed in a single invocation.

#### Usage
```bash
bin/opensearch-plugin install <plugin-name> <plugin-name> ... <plugin-name>
```

#### Example
```console
$ sudo ./opensearch-plugin install analysis-nori repository-s3
```

## Remove

You can remove a plugin that has already been installed with the `remove` option. 

#### Usage
```bash
bin/opensearch-plugin remove <plugin-name>
```

#### Example
```console
$ sudo ./opensearch-plugin remove opensearch-anomaly-detection
-> removing [opensearch-anomaly-detection]...
```

Restart your OpenSearch node after removing a plugin.
{: .note}

## Batch mode

When installing a plugin that requires additional privileges that are not included by default, the plugin will prompt you for confirmation of the required privileges. To grant all requested privileges, use batch mode to skip the confirmation prompt.

To force batch mode when installing plugins, add the `-b` or `--batch` option:
```bash
bin/opensearch-plugin install --batch <plugin-name>
```

## Available plugins

OpenSearch provides several bundled plugins that are available for immediate use with all OpenSearch distributions except for the minimal distribution. Additional plugins are available but must be installed separately using one of the installation options.  

### Bundled plugins

The following plugins are bundled with all OpenSearch distributions except for the minimal distribution. If you are using the minimal distribution, you can add these plugins by using one of the installation methods.

| Plugin name | Repository | Earliest available version |
| :--- | :--- | :--- |
| Alerting | [opensearch-alerting](https://github.com/opensearch-project/alerting) | 1.0.0 |
| Anomaly Detection | [opensearch-anomaly-detection](https://github.com/opensearch-project/anomaly-detection) | 1.0.0 |
| Asynchronous Search | [opensearch-asynchronous-search](https://github.com/opensearch-project/asynchronous-search) | 1.0.0 |
| Cross Cluster Replication | [opensearch-cross-cluster-replication](https://github.com/opensearch-project/cross-cluster-replication) | 1.1.0 |
| Custom Codecs | [opensearch-custom-codecs](https://github.com/opensearch-project/custom-codecs) | 2.10.0 |
| Flow Framework | [flow-framework](https://github.com/opensearch-project/flow-framework) | 2.12.0 |
| Notebooks<sup>1</sup> | [opensearch-notebooks](https://github.com/opensearch-project/dashboards-notebooks) | 1.0.0 to 1.1.0 |
| Notifications | [notifications](https://github.com/opensearch-project/notifications) | 2.0.0
| Reports Scheduler | [opensearch-reports-scheduler](https://github.com/opensearch-project/dashboards-reports) | 1.0.0 |
| Geospatial | [opensearch-geospatial](https://github.com/opensearch-project/geospatial) | 2.2.0 |
| Index Management | [opensearch-index-management](https://github.com/opensearch-project/index-management) | 1.0.0 |
| Job Scheduler | [opensearch-job-scheduler](https://github.com/opensearch-project/job-scheduler) | 1.0.0 |
| k-NN | [opensearch-knn](https://github.com/opensearch-project/k-NN) | 1.0.0 |
| ML Commons | [opensearch-ml](https://github.com/opensearch-project/ml-commons) | 1.3.0 |
| Skills | [opensearch-skills](https://github.com/opensearch-project/skills) | 2.12.0 |
| Neural Search | [neural-search](https://github.com/opensearch-project/neural-search) | 2.4.0 |
| Observability | [opensearch-observability](https://github.com/opensearch-project/observability) | 1.2.0 |
| Performance Analyzer<sup>2</sup> | [opensearch-performance-analyzer](https://github.com/opensearch-project/performance-analyzer) | 1.0.0 |
| Security | [opensearch-security](https://github.com/opensearch-project/security) | 1.0.0 |
| Security Analytics | [opensearch-security-analytics](https://github.com/opensearch-project/security-analytics) | 2.4.0 |
| SQL | [opensearch-sql](https://github.com/opensearch-project/sql) | 1.0.0 |

_<sup>1</sup>Dashboard Notebooks was merged in to the Observability plugin with the release of OpenSearch 1.2.0._<br>
_<sup>2</sup>Performance Analyzer is not available on Windows._


### Additional plugins

There are many more plugins available in addition to those provided by the default distribution. These additional plugins have been built by OpenSearch developers or members of the OpenSearch community. For a list of additional plugins you can install, see [Additional plugins]({{site.url}}{{site.baseurl}}/install-and-configure/additional-plugins/index/).

## Plugin compatibility

You can specify plugin compatibility with a particular OpenSearch version in the `plugin-descriptor.properties` file. For example, a plugin with the following property is compatible only with OpenSearch 2.3.0:

```properties
opensearch.version=2.3.0
```
Alternatively, you can specify a range of compatible OpenSearch versions by setting the `dependencies` property in the `plugin-descriptor.properties` file to one of the following notations:
- `dependencies={ opensearch: ""2.3.0"" }`: The plugin is compatible only with OpenSearch version 2.3.0.
- `dependencies={ opensearch: ""=2.3.0"" }`: The plugin is compatible only with OpenSearch version 2.3.0.
- `dependencies={ opensearch: ""~2.3.0"" }`: The plugin is compatible with all versions from 2.3.0 up to the next minor version, in this example, 2.4.0 (exclusive).
- `dependencies={ opensearch: ""^2.3.0"" }`: The plugin is compatible with all versions from 2.3.0 up to the next major version, in this example, 3.0.0 (exclusive).

You can specify only one of the `opensearch.version` or `dependencies` properties.
{: .note}

## Related links

- [Observability]({{site.url}}{{site.baseurl}}/observability-plugin/index/)
- [Security Analytics]({{site.url}}{{site.baseurl}}/security-analytics/index/)
- [Security]({{site.url}}{{site.baseurl}}/security/index/)
- [Alerting]({{site.url}}{{site.baseurl}}/monitoring-plugins/alerting/index/)
- [Anomaly detection]({{site.url}}{{site.baseurl}}/monitoring-plugins/ad/index/)
- [Asynchronous search]({{site.url}}{{site.baseurl}}/search-plugins/async/index/)
- [Cross-cluster replication]({{site.url}}{{site.baseurl}}/replication-plugin/index/)
- [Index State Management]({{site.url}}{{site.baseurl}}/im-plugin/ism/index/)
- [k-NN search]({{site.url}}{{site.baseurl}}/search-plugins/knn/index/)
- [ML Commons]({{site.url}}{{site.baseurl}}/ml-commons-plugin/index/)
- [Neural search]({{site.url}}{{site.baseurl}}/neural-search-plugin/index/)
- [Notifications]({{site.url}}{{site.baseurl}}/notifications-plugin/index/)
- [OpenSearch Dashboards]({{site.url}}{{site.baseurl}}/dashboards/index/)
- [Performance Analyzer]({{site.url}}{{site.baseurl}}/monitoring-plugins/pa/index/)
- [SQL]({{site.url}}{{site.baseurl}}/search-plugins/sql/index/)
","According to the example output when installing the opensearch-anomaly-detection plugin, the following Java permissions are required related to registering and unregistering MBeans:

- javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean
- javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean

These permissions allow the plugin to register and unregister the org.apache.commons.pool2.impl.GenericObjectPool MBean with the object name ""org.apache.commons.pool2:name=pool,type=GenericObjectPool"".",3.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, referencing the specific Java permissions and MBean object required for the anomaly detection plugin. It does not introduce any external details or speculate beyond what is stated.

Completeness: 1 - The answer thoroughly addresses the key elements of the question, clearly identifying the required permissions and the exact object they apply to. It provides a comprehensive overview of the necessary permissions.

Conciseness: 0 - While the response is generally clear and well-structured, there are some instances where the wording could be more concise. For example, the repetition of ""register"" and ""unregister"" in the permissions could be streamlined.

Accuracy: 1 - All the information presented in the answer is accurate and consistent with the context. The specific permissions and MBean object are correctly identified.

Clarity: 1 - The response is easy to understand and follow. The formatting and structure help convey the key details clearly. The language used is straightforward and accessible."
What environment variable and YAML structure must be used to customize the admin password in a helm chart? ,"---
layout: default
title: Helm
parent: Installing OpenSearch
nav_order: 6
redirect_from:
  - /opensearch/install/helm/
---

# Helm

Helm is a package manager that allows you to easily install and manage OpenSearch in a Kubernetes cluster. You can define your OpenSearch configurations in a YAML file and use Helm to deploy your applications in a version-controlled and reproducible way.

The Helm chart contains the resources described in the following table.

Resource | Description
:--- | :---
`Chart.yaml` |  Information about the chart.
`values.yaml` |  Default configuration values for the chart.
`templates` |  Templates that combine with values to generate the Kubernetes manifest files.

The specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC).

For information about the default configuration, steps to configure security, and configurable parameters, see the
[README](https://github.com/opensearch-project/helm-charts/blob/main/README.md).

The instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the [Kubernetes documentation](https://kubernetes.io/docs/setup/) for steps to configure a Kubernetes cluster and the [Helm documentation](https://helm.sh/docs/intro/install/) to install Helm.
{: .note }

## Prerequisites

The default Helm chart deploys a three-node cluster. We recommend that you have at least 8 GiB of memory available for this deployment. You can expect the deployment to fail if, say, you have less than 4 GiB of memory available.

## Install OpenSearch using Helm

1. Add `opensearch` [helm-charts](https://github.com/opensearch-project/helm-charts) repository to Helm:

   ```bash
   helm repo add opensearch https://opensearch-project.github.io/helm-charts/
   ```
   {% include copy.html %}

1. Update the available charts locally from charts repositories:

   ```bash
   helm repo update
   ```
   {% include copy.html %}

1. To search for the OpenSearch-related Helm charts:

   ```bash
   helm search repo opensearch
   ```
   {% include copy.html %}

   ```bash
   NAME                            	CHART VERSION	APP VERSION	DESCRIPTION                           
   opensearch/opensearch           	1.0.7        	1.0.0      	A Helm chart for OpenSearch           
   opensearch/opensearch-dashboards	1.0.4        	1.0.0      	A Helm chart for OpenSearch Dashboards
   ```

1. Deploy OpenSearch:

   ```bash
   helm install my-deployment opensearch/opensearch
   ```
   {% include copy.html %}

You can also build the `opensearch-1.0.0.tgz` file manually:

1. Change to the `opensearch` directory:

   ```bash
   cd charts/opensearch
   ```
   {% include copy.html %}

1. Package the Helm chart:

   ```bash
   helm package .
   ```
   {% include copy.html %}

1. Deploy OpenSearch:

   ```bash
   helm install --generate-name opensearch-1.0.0.tgz
   ```
   {% include copy.html %}

   The output shows you the specifications instantiated from the install.
   To customize the deployment, pass in the values that you want to override with a custom YAML file:

   ```bash
   helm install --values=customvalues.yaml opensearch-1.0.0.tgz
   ```
   {% include copy.html %}

For OpenSearch 2.12 or greater, customize the admin password in `values.yaml` under `extraEnvs`, as shown in the following example:

```yaml
extraEnvs:
  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
    value: <custom-admin-password>
```

#### Sample output

  ```yaml
  NAME: opensearch-1-1629223146
  LAST DEPLOYED: Tue Aug 17 17:59:07 2021
  NAMESPACE: default
  STATUS: deployed
  REVISION: 1
  TEST SUITE: None
  NOTES:
  Watch all cluster members come up.
    $ kubectl get pods --namespace=default -l app=opensearch-cluster-master -w
  ```

To make sure your OpenSearch pod is up and running, run the following command:

```bash
$ kubectl get pods
NAME                                                  READY   STATUS    RESTARTS   AGE
opensearch-cluster-master-0                           1/1     Running   0          3m56s
opensearch-cluster-master-1                           1/1     Running   0          3m56s
opensearch-cluster-master-2                           1/1     Running   0          3m56s
```

To access the OpenSearch shell:

```bash
$ kubectl exec -it opensearch-cluster-master-0 -- /bin/bash
```
{% include copy.html %}

You can send requests to the pod to verify that OpenSearch is up and running:

```json
$ curl -XGET https://localhost:9200 -u 'admin:admin' --insecure
{
  ""name"" : ""opensearch-cluster-master-1"",
  ""cluster_name"" : ""opensearch-cluster"",
  ""cluster_uuid"" : ""hP2gq5bPS3SLp8Z7wXm8YQ"",
  ""version"" : {
    ""distribution"" : ""opensearch"",
    ""number"" : <version>,
    ""build_type"" : <build-type>,
    ""build_hash"" : <build-hash>,
    ""build_date"" : <build-date>,
    ""build_snapshot"" : false,
    ""lucene_version"" : <lucene-version>,
    ""minimum_wire_compatibility_version"" : ""6.8.0"",
    ""minimum_index_compatibility_version"" : ""6.0.0-beta1""
  },
  ""tagline"" : ""The OpenSearch Project: https://opensearch.org/""
}
```

## Uninstall using Helm

To identify the OpenSearch deployment that you want to delete:

```bash
$ helm list
NAME  NAMESPACEREVISIONUPDATED  STATUS  CHART APP VERSION
opensearch-1-1629223146 default 1 2021-08-17 17:59:07.664498239 +0000 UTCdeployedopensearch-1.0.0    1.0.0       
```

To delete or uninstall a deployment, run the following command:

```bash
helm delete opensearch-1-1629223146
```
{% include copy.html %}

For steps to install OpenSearch Dashboards, see [Helm to install OpenSearch Dashboards]({{site.url}}{{site.baseurl}}/dashboards/install/helm/).","According to the context provided, to customize the admin password in a Helm chart for OpenSearch version 2.12 or greater, you need to use the `OPENSEARCH_INITIAL_ADMIN_PASSWORD` environment variable under the `extraEnvs` section in the `values.yaml` file. The YAML structure should look like this:

```yaml
extraEnvs:
  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
    value: <custom-admin-password>
```

Replace `<custom-admin-password>` with your desired admin password.",5.0,"Context Utilization: 1 - The response stays firmly grounded in the provided context, drawing all the necessary details from the information given. It does not introduce any extraneous or speculative elements, demonstrating a strong adherence to the task at hand.

Completeness: 1 - The answer thoroughly addresses the key requirements of the question, clearly outlining the necessary environment variable and YAML structure for customizing the admin password in a Helm chart. It covers all the essential details without omitting any critical information.

Conciseness: 1 - The response is concise and succinct, delivering the required information in a direct and efficient manner. It avoids unnecessary elaboration or tangential details, staying focused on the specific question asked.

Accuracy: 1 - All the details presented in the answer, such as the environment variable name and the YAML structure, are accurate and align perfectly with the context provided. There are no inaccuracies or contradictions in the information given.

Clarity: 1 - The response is written in a clear and easy-to-understand manner, with a logical flow and straightforward explanations. The instructions are presented in a way that is readily accessible and understandable to the reader."
How can I group my data by a nested field but still calculate metrics on a parent-level field in the same query?,"---
layout: default
title: Reverse nested
parent: Bucket aggregations
nav_order: 160
redirect_from:
  - /query-dsl/aggregations/bucket/reverse-nested/
---

# Reverse nested aggregations

You can aggregate values from nested documents to their parent; this aggregation is called `reverse_nested`.
You can use `reverse_nested` to aggregate a field from the parent document after grouping by the field from the nested object. The `reverse_nested` aggregation ""joins back"" the root page and gets the `load_time` for each for your variations.

The `reverse_nested` aggregation is a sub-aggregation inside a nested aggregation. It accepts a single option named `path`. This option defines how many steps backwards in the document hierarchy OpenSearch takes to calculate the aggregations.

```json
GET logs/_search
{
  ""query"": {
    ""match"": { ""response"": ""200"" }
  },
  ""aggs"": {
    ""pages"": {
      ""nested"": {
        ""path"": ""pages""
      },
      ""aggs"": {
        ""top_pages_per_load_time"": {
          ""terms"": {
            ""field"": ""pages.load_time""
          },
          ""aggs"": {
            ""comment_to_logs"": {
              ""reverse_nested"": {},
              ""aggs"": {
                ""min_load_time"": {
                  ""min"": {
                    ""field"": ""pages.load_time""
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

#### Example response

```json
...
""aggregations"" : {
  ""pages"" : {
    ""doc_count"" : 2,
    ""top_pages_per_load_time"" : {
      ""doc_count_error_upper_bound"" : 0,
      ""sum_other_doc_count"" : 0,
      ""buckets"" : [
        {
          ""key"" : 200.0,
          ""doc_count"" : 1,
          ""comment_to_logs"" : {
            ""doc_count"" : 1,
            ""min_load_time"" : {
              ""value"" : null
            }
          }
        },
        {
          ""key"" : 500.0,
          ""doc_count"" : 1,
          ""comment_to_logs"" : {
            ""doc_count"" : 1,
            ""min_load_time"" : {
              ""value"" : null
            }
          }
        }
      ]
    }
  }
 }
}
```

The response shows the logs index has one page with a `load_time` of 200 and one with a `load_time` of 500.","Based on the context provided, you can use the `reverse_nested` aggregation to group your data by a nested field while still calculating metrics on a parent-level field in the same query.

The `reverse_nested` aggregation is a sub-aggregation inside a `nested` aggregation. It allows you to ""join back"" to the root document and calculate aggregations on fields from the parent document, after grouping by fields from the nested object.

The example query demonstrates this by first using a `nested` aggregation on the `pages` field to group the data by the nested `load_time` field. Then, inside the `terms` aggregation on `pages.load_time`, it uses a `reverse_nested` sub-aggregation to calculate the `min_load_time` from the parent document's `load_time` field.

So, to group your data by a nested field but still calculate metrics on a parent-level field, you can follow a similar pattern:

1. Use a `nested` aggregation to group by the nested field.
2. Inside the `nested` aggregation, add a `terms` (or other bucket) aggregation on the nested field you want to group by.
3. Inside the `terms` aggregation, add a `reverse_nested` sub-aggregation.
4. Inside the `reverse_nested` sub-aggregation, add metric aggregations (e.g., `min`, `max`, `avg`) on the parent-level fields you want to calculate.

This way, you can effectively join the nested data back to the parent document and calculate metrics on parent-level fields while still grouping by the nested field.",4.0,"Context Utilization: 1 - The response stays closely aligned with the information provided in the context, demonstrating a clear understanding of the problem and the relevant aggregation techniques. It does not introduce any extraneous details or assumptions beyond what is presented.

Completeness: 1 - The answer thoroughly addresses the key elements of the question, providing a step-by-step explanation of how to use the `reverse_nested` aggregation to group data by a nested field while calculating metrics on a parent-level field. The response covers all the necessary components to solve the problem.

Conciseness: 0 - While the explanation is comprehensive, it could be more concise in its delivery. The response contains some redundant phrasing and could be streamlined to convey the same information in a more efficient manner.

Accuracy: 1 - The details provided in the answer accurately reflect the information given in the context. The explanation of the `reverse_nested` aggregation and its usage aligns with the example query and response shown.

Clarity: 1 - The response is clear and easy to follow, with a logical flow that guides the reader through the steps to achieve the desired outcome. The language used is straightforward and the concepts are well-explained."
Make it simpler like what happens when I have missing values in an agg?,"---
layout: default
title: Missing
parent: Bucket aggregations
nav_order: 120
redirect_from:
  - /query-dsl/aggregations/bucket/missing/
---

# Missing aggregations

If you have documents in your index that don’t contain the aggregating field at all or the aggregating field has a value of NULL, use the `missing` parameter to specify the name of the bucket such documents should be placed in.

The following example adds any missing values to a bucket named ""N/A"":

```json
GET opensearch_dashboards_sample_data_logs/_search
{
  ""size"": 0,
  ""aggs"": {
    ""response_codes"": {
      ""terms"": {
        ""field"": ""response.keyword"",
        ""size"": 10,
        ""missing"": ""N/A""
      }
    }
  }
}
```
{% include copy-curl.html %}

Because the default value for the `min_doc_count` parameter is 1, the `missing` parameter doesn't return any buckets in its response. Set `min_doc_count` parameter to 0 to see the ""N/A"" bucket in the response:

```json
GET opensearch_dashboards_sample_data_logs/_search
{
  ""size"": 0,
  ""aggs"": {
    ""response_codes"": {
      ""terms"": {
        ""field"": ""response.keyword"",
        ""size"": 10,
        ""missing"": ""N/A"",
        ""min_doc_count"": 0
      }
    }
  }
}
```

#### Example response

```json
...
""aggregations"" : {
  ""response_codes"" : {
    ""doc_count_error_upper_bound"" : 0,
    ""sum_other_doc_count"" : 0,
    ""buckets"" : [
      {
        ""key"" : ""200"",
        ""doc_count"" : 12832
      },
      {
        ""key"" : ""404"",
        ""doc_count"" : 801
      },
      {
        ""key"" : ""503"",
        ""doc_count"" : 441
      },
      {
        ""key"" : ""N/A"",
        ""doc_count"" : 0
      }
    ]
  }
 }
}
```","When you have missing values in an aggregation, the `missing` parameter in the aggregation allows you to specify a name for a bucket that will contain the documents that are missing the aggregating field or have a null value for that field.

If you don't set the `missing` parameter, the documents with missing values for the aggregating field will not be included in any of the buckets generated by the aggregation.

By setting the `missing` parameter and providing a name (e.g., ""N/A"" in the example), a separate bucket will be created for those missing values, allowing you to account for them in your analysis.

Additionally, to see the bucket for missing values in the response, you may need to set the `min_doc_count` parameter to 0, as the default value of 1 will exclude buckets with 0 documents.",4.0,"Context Utilization: 1 - The response stays firmly grounded within the context provided, drawing upon the example and explanation to address the question. It does not venture beyond the scope of the given information, which is appropriate for the task.

Completeness: 1 - The answer thoroughly covers the key aspects of handling missing values in an aggregation, as requested in the question. It explains the purpose of the ""missing"" parameter, the default behavior when it is not set, and the need to adjust the ""min_doc_count"" parameter to see the bucket for missing values. This comprehensive coverage addresses the core elements of the query.

Conciseness: 0 - While the response provides a clear and detailed explanation, it could be streamlined in certain areas. Some of the elaboration, such as the specific example query and response, could be condensed or removed without compromising the overall understanding. A more concise answer would maintain the informative nature while being more succinct.

Accuracy: 1 - The information presented in the response aligns accurately with the context provided. The explanations and details accurately reflect the behavior of the ""missing"" parameter and the associated aggregation functionality. There are no inaccuracies or contradictions with the given context.

Clarity: 1 - The response is written in a straightforward and easy-to-understand manner. The language used is clear, and the explanations are logically structured, making the information accessible to a non-technical audience. The flow of the response guides the reader through the key concepts effectively."
Can you explain compound queries to me? ,"---
layout: default
title: Compound queries
has_children: true
has_toc: false
nav_order: 40
redirect_from: 
  - /opensearch/query-dsl/compound/index/
  - /query-dsl/compound/index/
  - /query-dsl/query-dsl/compound/
  - /query-dsl/compound/
---

# Compound queries

Compound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. 

The following table lists all compound query types.

Query type | Description
:--- | :---
[`bool`]({{site.url}}{{site.baseurl}}/query-dsl/compound/bool/) (Boolean)| Combines multiple query clauses with Boolean logic. 
[`boosting`]({{site.url}}{{site.baseurl}}/query-dsl/compound/boosting/) | Changes the relevance score of documents without removing them from the search results. Returns documents that match a `positive` query, but downgrades the relevance of documents in the results that match a `negative` query.
[`constant_score`]({{site.url}}{{site.baseurl}}/query-dsl/compound/constant-score/) | Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the `boost` value.
[`dis_max`]({{site.url}}{{site.baseurl}}/query-dsl/compound/disjunction-max/) (disjunction max) | Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value.
[`function_score`]({{site.url}}{{site.baseurl}}/query-dsl/compound/function-score/) | Recalculates the relevance score of documents that are returned by a query using a function that you define.
[`hybrid`]({{site.url}}{{site.baseurl}}/query-dsl/compound/hybrid/) | Combines relevance scores from multiple queries into one score for a given document.
","Compound queries in Elasticsearch are used to combine multiple leaf or compound query clauses together, either to combine their results or to modify their behavior. They act as wrappers around other queries, allowing you to apply boolean logic, adjust relevance scores, or combine results in various ways.

Some key compound queries include:

1. `bool` query: Combines multiple queries using boolean logic (AND, OR, NOT). This allows you to create complex queries by combining simpler ones.

2. `boosting` query: Adjusts the relevance scores of documents that match a ""negative"" query, without removing them from the results. This can be useful for demoting certain results.

3. `constant_score` query: Wraps a query or filter and assigns a constant relevance score to all matching documents.

4. `dis_max` (disjunction max) query: Returns documents that match one or more queries, assigning higher relevance scores to documents that match multiple queries.

5. `function_score` query: Recalculates the relevance scores of documents returned by a query, using a custom scoring function.

6. `hybrid` query: Combines relevance scores from multiple queries into a single score for each document.

Compound queries provide a way to build complex search queries by combining simpler ones, applying boolean logic, adjusting relevance scores, and customizing how results are scored and ranked. They offer flexibility in crafting sophisticated search queries tailored to specific needs.",3.0,"Context Utilization: 1 - The response stays firmly within the bounds of the provided context, drawing on the information and examples outlined in the background material. It does not venture into any external information or tangents, which aligns well with the LLM evaluation.

Completeness: 1 - The answer covers the key aspects of compound queries, including the different types (bool, boosting, constant_score, dis_max, function_score, hybrid) and a high-level description of how each one works. This comprehensive overview addresses the original question effectively.

Conciseness: 0 - While the response provides a thorough explanation, it does tend to be a bit wordy and verbose in certain sections. Some of the details and examples could potentially be streamlined or condensed to make the core concepts more concise, as noted in the LLM evaluation.

Accuracy: 1 - The information presented about compound queries is accurate and aligns closely with the details provided in the context. The definitions and explanations match the reference material, demonstrating a solid understanding of the topic.

Clarity: 1 - The response is generally clear and easy to follow, with a logical flow and appropriate use of terminology. However, the occasional verbosity may make it slightly less accessible in parts, as mentioned in the LLM evaluation."
What is the difference between vector search and full text search? ,"---
layout: default
title: Term-level and full-text queries compared
nav_order: 10
redirect_from:
  - /query-dsl/query-dsl/term-vs-full-text/
  - /opensearch/query-dsl/term-vs-full-text/
---

# Term-level and full-text queries compared

You can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries [analyze]({{site.url}}{{site.baseurl}}/analyzers/) the query string. The following table summarizes the differences between term-level and full-text queries.

| | Term-level queries | Full-text queries
:--- | :--- | :---
*Description* | Term-level queries answer which documents match a query. | Full-text queries answer how well the documents match a query.
*Analyzer* | The search term isn't analyzed. This means that the term query searches for your search term as it is.  | The search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document's field.
*Relevance* | Term-level queries simply return documents that match without sorting them based on the relevance score. They still calculate the relevance score, but this score is the same for all the documents that are returned. | Full-text queries calculate a relevance score for each match and sort the results by decreasing order of relevance.
*Use Case* | Use term-level queries when you want to match exact values such as numbers, dates, or tags and don't need the matches to be sorted by relevance. | Use full-text queries to match text fields and sort by relevance after taking into account factors like casing and stemming variants.

OpenSearch uses the BM25 ranking algorithm to calculate relevance scores. To learn more, see [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25).
{: .note }

## Should I use a full-text or a term-level query?

To clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster.

### Example: Phrase search

In this example, you'll search the complete works of Shakespeare for the phrase ""To be, or not to be"" in the `text_entry` field. 

First, use a **term-level query** for this search:

```json
GET shakespeare/_search
{
  ""query"": {
    ""term"": {
      ""text_entry"": ""To be, or not to be""
    }
  }
}
```

The response contains no matches, indicated by zero `hits`:

```json
{
  ""took"" : 3,
  ""timed_out"" : false,
  ""_shards"" : {
    ""total"" : 1,
    ""successful"" : 1,
    ""skipped"" : 0,
    ""failed"" : 0
  },
  ""hits"" : {
    ""total"" : {
      ""value"" : 0,
      ""relation"" : ""eq""
    },
    ""max_score"" : null,
    ""hits"" : [ ]
  }
}
```

This is because the term “To be, or not to be” is searched literally in the inverted index, where only the analyzed values of the text fields are stored. Term-level queries aren’t suited for searching analyzed text fields because they often yield unexpected results. When working with text data, use term-level queries only for fields mapped as `keyword`.

Now search for the same phrase using a **full-text query**:

```json
GET shakespeare/_search
{
  ""query"": {
    ""match"": {
      ""text_entry"": ""To be, or not to be""
    }
  }
}
```

The search query “To be, or not to be” is analyzed and tokenized into an array of tokens just like the `text_entry` field of the documents. The full-text query takes an intersection of tokens between the search query and the `text_entry` fields for all the documents, and then sorts the results by relevance score:

```json
{
  ""took"" : 19,
  ""timed_out"" : false,
  ""_shards"" : {
    ""total"" : 1,
    ""successful"" : 1,
    ""skipped"" : 0,
    ""failed"" : 0
  },
  ""hits"" : {
    ""total"" : {
      ""value"" : 10000,
      ""relation"" : ""gte""
    },
    ""max_score"" : 17.419369,
    ""hits"" : [
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""34229"",
        ""_score"" : 17.419369,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 34230,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 19,
          ""line_number"" : ""3.1.64"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""To be, or not to be: that is the question:""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""109930"",
        ""_score"" : 14.883024,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 109931,
          ""play_name"" : ""A Winters Tale"",
          ""speech_number"" : 23,
          ""line_number"" : ""4.4.153"",
          ""speaker"" : ""PERDITA"",
          ""text_entry"" : ""Not like a corse; or if, not to be buried,""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""103117"",
        ""_score"" : 14.782743,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 103118,
          ""play_name"" : ""Twelfth Night"",
          ""speech_number"" : 53,
          ""line_number"" : ""1.3.95"",
          ""speaker"" : ""SIR ANDREW"",
          ""text_entry"" : ""will not be seen; or if she be, its four to one""
        }
      }
    ]
  }
}
...
```

For a list of all full-text queries, see [Full-text queries]({{site.url}}{{site.baseurl}}/opensearch/query-dsl/full-text/index).

### Example: Exact term search

If you want to search for an exact term like “HAMLET” in the `speaker` field and don't need the results to be sorted by relevance score, a term-level query is more efficient:

```json
GET shakespeare/_search
{
  ""query"": {
    ""term"": {
      ""speaker"": ""HAMLET""
    }
  }
}
```

The response contains document matches:

```json
{
  ""took"" : 5,
  ""timed_out"" : false,
  ""_shards"" : {
    ""total"" : 1,
    ""successful"" : 1,
    ""skipped"" : 0,
    ""failed"" : 0
  },
  ""hits"" : {
    ""total"" : {
      ""value"" : 1582,
      ""relation"" : ""eq""
    },
    ""max_score"" : 4.2540946,
    ""hits"" : [
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""32700"",
        ""_score"" : 4.2540946,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 32701,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 9,
          ""line_number"" : ""1.2.66"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""[Aside]  A little more than kin, and less than kind.""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""32702"",
        ""_score"" : 4.2540946,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 32703,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 11,
          ""line_number"" : ""1.2.68"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""Not so, my lord; I am too much i' the sun.""
        }
      },
      {
        ""_index"" : ""shakespeare"",
        ""_id"" : ""32709"",
        ""_score"" : 4.2540946,
        ""_source"" : {
          ""type"" : ""line"",
          ""line_id"" : 32710,
          ""play_name"" : ""Hamlet"",
          ""speech_number"" : 13,
          ""line_number"" : ""1.2.75"",
          ""speaker"" : ""HAMLET"",
          ""text_entry"" : ""Ay, madam, it is common.""
        }
      }
    ]
  }
}
...
```

The term-level queries provide exact matches. So if you search for “Hamlet”, you don’t receive any matches, because “HAMLET” is a keyword field and is stored in OpenSearch literally and not in an analyzed form.
The search query “HAMLET” is also searched literally. So to get a match for this field, we need to enter the exact same characters.
",The context provided does not mention vector search or explain the differences between vector search and full-text search. It only covers the differences between term-level queries and full-text queries in OpenSearch/Elasticsearch. I don't have enough information to accurately explain the difference between vector search and full-text search.,3.0,"Context Utilization: 1 - The response accurately acknowledges the limitations of the provided context, which does not cover the differences between vector search and full-text search as requested in the question. This shows a clear understanding of the scope and content of the given information.
Completeness: 0 - The answer does not provide any details about the differences between vector search and full-text search, which is the core of the question. While the response explains why it cannot answer the question, it fails to address the actual query.
Conciseness: 1 - The response is succinct and to the point, directly stating that the context does not contain the necessary information to answer the question.
Accuracy: 1 - The statement about the context lacking details on vector search and full-text search is correct based on the provided information.
Clarity: 1 - The response is clear and unambiguous in conveying the limitations of the context in addressing the question."
"Why can't my coworker see my async searches, but I can see and delete theirs? This role thing is messing everything up!","---
layout: default
title: Asynchronous search security
nav_order: 2
parent: Asynchronous search
grand_parent: Improving search performance
has_children: false
redirect_from:
 - /search-plugins/async/security/
---

# Asynchronous search security

You can use the Security plugin with asynchronous searches to limit non-admin users to specific actions. For example, you might want some users to only be able to submit or delete asynchronous searches, while you might want others to only view the results.

All asynchronous search indexes are protected as system indexes. Only a super admin user or an admin user with a Transport Layer Security (TLS) certificate can access system indexes. For more information, see [System indexes]({{site.url}}{{site.baseurl}}/security/configuration/system-indices/).

## Basic permissions

As an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see [Asynchronous search]({{site.url}}{{site.baseurl}}/).

The Security plugin has two built-in roles that cover most asynchronous search use cases: `asynchronous_search_full_access` and `asynchronous_search_read_access`. For descriptions of each, see [Predefined roles]({{site.url}}{{site.baseurl}}/security/access-control/users-roles#predefined-roles).

If these roles don’t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the `cluster:admin/opensearch/asynchronous_search/delete` permission lets you delete a previously submitted asynchronous search.

### A note on Asynchronous Search and fine-grained access control

By design, the Asynchronous Search plugin extracts data from a target index and stores the data in a separate index to make search results available to users with the proper permissions. Although a user with either the `asynchronous_search_read_access` or `cluster:admin/opensearch/asynchronous_search/get` permission cannot submit the asynchronous search request itself, that user can get and view the search results using the associated search ID. [Document-level security]({{site.url}}{{site.baseurl}}/security/access-control/document-level-security) (DLS) and [field-level security]({{site.url}}{{site.baseurl}}/security/access-control/field-level-security) (FLS) access controls are designed to protect the data in the target index. But once the data is stored outside this index, users with these access permissions are able to use search IDs to get and view asynchronous search results, which may include data that is otherwise concealed by DLS and FLS access control in the target index.

To reduce the chances of unintended users viewing search results that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See [Limit access by backend role](#advanced-limit-access-by-backend-role) for details.

## (Advanced) Limit access by backend role

Use backend roles to configure fine-grained access to asynchronous searches based on roles. For example, users of different departments in an organization can view asynchronous searches owned by their own department.

First, make sure your users have the appropriate [backend roles]({{site.url}}{{site.baseurl}}/security/access-control/index/). Backend roles usually come from an [LDAP server]({{site.url}}{{site.baseurl}}/security/configuration/ldap/) or [SAML provider]({{site.url}}{{site.baseurl}}/security/configuration/saml/). However, if you use the internal user database, you can use the REST API to [add them manually]({{site.url}}{{site.baseurl}}/security/access-control/api#create-user).

Now when users view asynchronous search resources in OpenSearch Dashboards (or make REST API calls), they only see asynchronous searches submitted by users who have a subset of the backend role.
For example, consider two users: `judy` and `elon`.

`judy` has an IT backend role:

```json
PUT _plugins/_security/api/internalusers/judy
{
  ""password"": ""judy"",
  ""backend_roles"": [
    ""IT""
  ],
  ""attributes"": {}
}
```

`elon` has an admin backend role:

```json
PUT _plugins/_security/api/internalusers/elon
{
  ""password"": ""elon"",
  ""backend_roles"": [
    ""admin""
  ],
  ""attributes"": {}
}
```

Both `judy` and `elon` have full access to asynchronous search:

```json
PUT _plugins/_security/api/rolesmapping/async_full_access
{
  ""backend_roles"": [],
  ""hosts"": [],
  ""users"": [
    ""judy"",
    ""elon""
  ]
}
```

Because they have different backend roles, an asynchronous search submitted by `judy` will not be visible to `elon` and vice versa.

`judy` needs to have at least the superset of all roles that `elon` has to see `elon`'s asynchronous searches.

For example, if `judy` has five backend roles and `elon` has one of these roles, then `judy` can see asynchronous searches submitted by `elon`, but `elon` can’t see the asynchronous searches submitted by `judy`. This means that `judy` can perform GET and DELETE operations on asynchronous searches submitted by `elon`, but not the reverse.

If none of the users have any backend roles, all three will be able to see the others' searches.

For example, consider three users: `judy`, `elon`, and `jack`.

`judy`, `elon`, and `jack` have no backend roles set up:

```json
PUT _plugins/_security/api/internalusers/judy
{
  ""password"": ""judy"",
  ""backend_roles"": [],
  ""attributes"": {}
}
```

```json
PUT _plugins/_security/api/internalusers/elon
{
  ""password"": ""elon"",
  ""backend_roles"": [],
  ""attributes"": {}
}
```

```json
PUT _plugins/_security/api/internalusers/jack
{
  ""password"": ""jack"",
  ""backend_roles"": [],
  ""attributes"": {}
}
```

Both `judy` and `elon` have full access to asynchronous search:

```json
PUT _plugins/_security/api/rolesmapping/async_full_access
{
  ""backend_roles"": [],
  ""hosts"": [],
  ""users"": [""judy"",""elon""]
}
```

`jack` has read access to asynchronous search results:

```json
PUT _plugins/_security/api/rolesmapping/async_read_access
{
  ""backend_roles"": [],
  ""hosts"": [],
  ""users"": [""jack""]
}
```

Because none of the users have backend roles, they will be able to see each other's asynchronous searches. So, if `judy` submits an asynchronous search, `elon`, who has full access, will be able to see that search. `jack`, who has read access, will also be able to see `judy`'s asynchronous search.","Based on the context provided, it seems that the issue is related to the backend roles assigned to you and your coworker. The backend roles determine the level of access and visibility for asynchronous searches between users.

The key points from the context:

1. Backend roles are typically assigned from an LDAP server, SAML provider, or manually added to the internal user database.
2. When users view or interact with asynchronous search resources, they can only see and access the searches submitted by users who have a subset of their own backend roles.
3. If a user has a superset of another user's backend roles, they can see and perform operations on the other user's asynchronous searches, but not vice versa.
4. If none of the users have any backend roles, they can see and access each other's asynchronous searches.

So, in your case, it seems that you and your coworker have different backend roles assigned, and your backend roles are a superset of your coworker's backend roles. This allows you to see and delete your coworker's asynchronous searches, but not the other way around.

To resolve this issue, you can either:

1. Adjust the backend roles assigned to you and your coworker to ensure you both have the appropriate level of access and visibility.
2. If you both need full access to each other's asynchronous searches, you can remove or leave the backend roles empty for both users.

It's essential to carefully manage backend roles and permissions to ensure the desired level of access and visibility for asynchronous searches within your organization.",4.0,"Context Utilization: 1 - The response stays firmly grounded in the context provided, drawing solely on the information presented and avoiding any external assumptions or speculations. This demonstrates a clear understanding of the scenario and a disciplined approach to addressing the question.

Completeness: 1 - The answer thoroughly covers the key elements of the issue, explaining the underlying reasons for the coworker's inability to see the user's asynchronous searches, as well as the potential solutions to resolve the problem. The response provides a comprehensive overview of the relevant security and access control mechanisms at play.

Conciseness: 0 - While the response is well-structured and logically presented, it could be slightly more concise in its delivery. There are a few instances where the explanation could be streamlined without sacrificing clarity or detail.

Accuracy: 1 - The information provided in the answer is completely accurate and aligned with the context. The technical details and explanations are precise and in line with the documented behavior of the Asynchronous Search plugin and its security features.

Clarity: 1 - The response is highly readable and easy to follow, with a clear and coherent flow. The technical concepts are explained in a manner that is accessible to a general audience, striking a balance between technical depth and user-friendliness."
